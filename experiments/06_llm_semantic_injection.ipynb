{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 6: LLM Semantic Injection\n",
    "\n",
    "## Objective\n",
    "Demonstrate that LLM-guided agent parameterization produces more realistic synthetic data than random initialization.\n",
    "\n",
    "## Hypothesis\n",
    "- LLM personas will encode realistic behavioral patterns (e.g., income correlates with spending)\n",
    "- LLM-guided data will have higher statistical fidelity to real-world distributions\n",
    "- LLM-guided data will produce better ML model utility (TSTR)\n",
    "\n",
    "## Key Contribution\n",
    "This is MISATA's core differentiator: **Language-guided agent synthesis** enables domain experts to inject business semantics without code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q jax jaxlib polars pyarrow openai google-generativeai anthropic\n",
    "!pip install -q pandas numpy matplotlib seaborn scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random, jit, vmap, lax\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import json\n",
    "import time\n",
    "from typing import NamedTuple, List, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"Devices: {jax.devices()}\")\n",
    "print(f\"Backend: {jax.default_backend()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: LLM Persona Generation\n",
    "\n",
    "We use an LLM to generate realistic customer personas that encode domain knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Use Google Gemini (free tier available)\n",
    "# Option 2: Use OpenAI GPT-4\n",
    "# Option 3: Use Anthropic Claude\n",
    "# Option 4: Mock LLM for demonstration\n",
    "\n",
    "LLM_PROVIDER = \"mock\"  # Change to \"gemini\", \"openai\", or \"anthropic\" with API key\n",
    "\n",
    "# For Kaggle, you can add secrets via Add-ons > Secrets\n",
    "# from kaggle_secrets import UserSecretsClient\n",
    "# secrets = UserSecretsClient()\n",
    "# GEMINI_API_KEY = secrets.get_secret(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSONA_GENERATION_PROMPT = \"\"\"\n",
    "You are a synthetic data expert. Generate {n_personas} realistic customer personas for a fraud detection dataset.\n",
    "\n",
    "For each persona, provide a JSON object with these fields:\n",
    "- persona_name: A descriptive name (e.g., \"young_professional\", \"retired_senior\", \"small_business_owner\")\n",
    "- age_range: [min_age, max_age]\n",
    "- income_range: [min_income, max_income] in USD\n",
    "- spending_rate_daily: Average daily spending in USD (should correlate with income)\n",
    "- transaction_frequency: Average transactions per day\n",
    "- preferred_categories: List of merchant category codes they frequent (0-6)\n",
    "- fraud_susceptibility: \"low\", \"medium\", or \"high\" based on typical behavior patterns\n",
    "- typical_transaction_hours: [start_hour, end_hour] (24-hour format)\n",
    "- location_variance: \"low\" (shops locally), \"medium\" (regional), \"high\" (travels frequently)\n",
    "- credit_limit_range: [min_limit, max_limit]\n",
    "\n",
    "Make sure the personas are:\n",
    "1. Internally consistent (income matches spending, etc.)\n",
    "2. Representative of real-world customer segments\n",
    "3. Diverse across demographics\n",
    "\n",
    "Return ONLY a valid JSON array with {n_personas} persona objects. No other text.\n",
    "\"\"\"\n",
    "\n",
    "def generate_personas_with_llm(n_personas: int, provider: str = \"mock\") -> List[Dict]:\n",
    "    \"\"\"Generate personas using LLM or mock data.\"\"\"\n",
    "    \n",
    "    if provider == \"mock\":\n",
    "        # Realistic mock personas based on domain knowledge\n",
    "        return [\n",
    "            {\n",
    "                \"persona_name\": \"young_professional\",\n",
    "                \"age_range\": [25, 35],\n",
    "                \"income_range\": [60000, 120000],\n",
    "                \"spending_rate_daily\": 150,\n",
    "                \"transaction_frequency\": 3.5,\n",
    "                \"preferred_categories\": [0, 1, 3],  # Food, Shopping, Entertainment\n",
    "                \"fraud_susceptibility\": \"medium\",\n",
    "                \"typical_transaction_hours\": [8, 22],\n",
    "                \"location_variance\": \"medium\",\n",
    "                \"credit_limit_range\": [10000, 30000]\n",
    "            },\n",
    "            {\n",
    "                \"persona_name\": \"retired_senior\",\n",
    "                \"age_range\": [65, 80],\n",
    "                \"income_range\": [30000, 60000],\n",
    "                \"spending_rate_daily\": 50,\n",
    "                \"transaction_frequency\": 1.2,\n",
    "                \"preferred_categories\": [0, 4, 5],  # Food, Healthcare, Utilities\n",
    "                \"fraud_susceptibility\": \"high\",  # Often targeted\n",
    "                \"typical_transaction_hours\": [9, 17],\n",
    "                \"location_variance\": \"low\",\n",
    "                \"credit_limit_range\": [5000, 15000]\n",
    "            },\n",
    "            {\n",
    "                \"persona_name\": \"high_net_worth\",\n",
    "                \"age_range\": [40, 60],\n",
    "                \"income_range\": [200000, 500000],\n",
    "                \"spending_rate_daily\": 500,\n",
    "                \"transaction_frequency\": 5.0,\n",
    "                \"preferred_categories\": [1, 2, 3, 6],  # Shopping, Travel, Entertainment, Luxury\n",
    "                \"fraud_susceptibility\": \"high\",  # High-value target\n",
    "                \"typical_transaction_hours\": [6, 23],\n",
    "                \"location_variance\": \"high\",\n",
    "                \"credit_limit_range\": [50000, 200000]\n",
    "            },\n",
    "            {\n",
    "                \"persona_name\": \"college_student\",\n",
    "                \"age_range\": [18, 24],\n",
    "                \"income_range\": [5000, 20000],\n",
    "                \"spending_rate_daily\": 25,\n",
    "                \"transaction_frequency\": 2.0,\n",
    "                \"preferred_categories\": [0, 1, 3],  # Food, Shopping, Entertainment\n",
    "                \"fraud_susceptibility\": \"low\",\n",
    "                \"typical_transaction_hours\": [10, 2],  # Late night spending\n",
    "                \"location_variance\": \"medium\",\n",
    "                \"credit_limit_range\": [1000, 5000]\n",
    "            },\n",
    "            {\n",
    "                \"persona_name\": \"small_business_owner\",\n",
    "                \"age_range\": [30, 55],\n",
    "                \"income_range\": [80000, 180000],\n",
    "                \"spending_rate_daily\": 300,\n",
    "                \"transaction_frequency\": 8.0,  # Many business transactions\n",
    "                \"preferred_categories\": [0, 1, 5, 6],  # Food, Supplies, Utilities, Services\n",
    "                \"fraud_susceptibility\": \"medium\",\n",
    "                \"typical_transaction_hours\": [7, 20],\n",
    "                \"location_variance\": \"medium\",\n",
    "                \"credit_limit_range\": [25000, 75000]\n",
    "            },\n",
    "            {\n",
    "                \"persona_name\": \"frugal_saver\",\n",
    "                \"age_range\": [30, 50],\n",
    "                \"income_range\": [50000, 90000],\n",
    "                \"spending_rate_daily\": 30,\n",
    "                \"transaction_frequency\": 0.8,  # Infrequent purchases\n",
    "                \"preferred_categories\": [0, 5],  # Essentials only\n",
    "                \"fraud_susceptibility\": \"low\",\n",
    "                \"typical_transaction_hours\": [12, 18],\n",
    "                \"location_variance\": \"low\",\n",
    "                \"credit_limit_range\": [8000, 20000]\n",
    "            }\n",
    "        ][:n_personas]\n",
    "    \n",
    "    elif provider == \"gemini\":\n",
    "        import google.generativeai as genai\n",
    "        genai.configure(api_key=GEMINI_API_KEY)\n",
    "        model = genai.GenerativeModel('gemini-pro')\n",
    "        prompt = PERSONA_GENERATION_PROMPT.format(n_personas=n_personas)\n",
    "        response = model.generate_content(prompt)\n",
    "        return json.loads(response.text)\n",
    "    \n",
    "    elif provider == \"openai\":\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "        prompt = PERSONA_GENERATION_PROMPT.format(n_personas=n_personas)\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        return json.loads(response.choices[0].message.content)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown provider: {provider}\")\n",
    "\n",
    "# Generate personas\n",
    "personas = generate_personas_with_llm(6, provider=LLM_PROVIDER)\n",
    "print(f\"Generated {len(personas)} personas:\")\n",
    "for p in personas:\n",
    "    print(f\"  - {p['persona_name']}: income ${p['income_range'][0]:,}-${p['income_range'][1]:,}, \"\n",
    "          f\"spends ${p['spending_rate_daily']}/day, fraud_risk={p['fraud_susceptibility']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Convert Personas to JAX Agent Parameters\n",
    "\n",
    "Map LLM-generated personas to numerical agent parameters for JAX simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(NamedTuple):\n",
    "    \"\"\"Struct-of-Arrays agent representation.\"\"\"\n",
    "    customer_id: jnp.ndarray\n",
    "    persona_id: jnp.ndarray       # Which persona this agent belongs to\n",
    "    balance: jnp.ndarray\n",
    "    credit_limit: jnp.ndarray\n",
    "    spend_rate: jnp.ndarray\n",
    "    transaction_freq: jnp.ndarray\n",
    "    merchant_pref: jnp.ndarray    # Primary merchant category\n",
    "    fraud_prob: jnp.ndarray\n",
    "    location_variance: jnp.ndarray\n",
    "    is_active: jnp.ndarray\n",
    "\n",
    "\n",
    "def personas_to_agent_params(personas: List[Dict], n_agents: int, key) -> AgentState:\n",
    "    \"\"\"\n",
    "    Convert LLM personas to JAX agent parameters.\n",
    "    Distributes agents across personas proportionally.\n",
    "    \"\"\"\n",
    "    n_personas = len(personas)\n",
    "    agents_per_persona = n_agents // n_personas\n",
    "    \n",
    "    # Map text values to numeric\n",
    "    fraud_map = {\"low\": 0.005, \"medium\": 0.015, \"high\": 0.03}\n",
    "    location_map = {\"low\": 0.1, \"medium\": 0.5, \"high\": 1.0}\n",
    "    \n",
    "    # Accumulate parameters\n",
    "    all_params = {\n",
    "        'customer_id': [],\n",
    "        'persona_id': [],\n",
    "        'balance': [],\n",
    "        'credit_limit': [],\n",
    "        'spend_rate': [],\n",
    "        'transaction_freq': [],\n",
    "        'merchant_pref': [],\n",
    "        'fraud_prob': [],\n",
    "        'location_variance': [],\n",
    "    }\n",
    "    \n",
    "    keys = random.split(key, n_personas * 6)\n",
    "    key_idx = 0\n",
    "    agent_id = 0\n",
    "    \n",
    "    for persona_idx, persona in enumerate(personas):\n",
    "        n = agents_per_persona if persona_idx < n_personas - 1 else n_agents - agent_id\n",
    "        \n",
    "        # Sample within persona's ranges\n",
    "        income_low, income_high = persona['income_range']\n",
    "        credit_low, credit_high = persona['credit_limit_range']\n",
    "        \n",
    "        # Balance based on income (assume 2-6 months of income)\n",
    "        incomes = random.uniform(keys[key_idx], (n,), minval=income_low, maxval=income_high)\n",
    "        key_idx += 1\n",
    "        balances = incomes * random.uniform(keys[key_idx], (n,), minval=0.15, maxval=0.5)\n",
    "        key_idx += 1\n",
    "        \n",
    "        # Credit limit from persona range\n",
    "        credit_limits = random.uniform(keys[key_idx], (n,), minval=credit_low, maxval=credit_high)\n",
    "        key_idx += 1\n",
    "        \n",
    "        # Spend rate with some variance around persona's value\n",
    "        base_spend = persona['spending_rate_daily']\n",
    "        spend_rates = base_spend * random.uniform(keys[key_idx], (n,), minval=0.7, maxval=1.3)\n",
    "        key_idx += 1\n",
    "        \n",
    "        # Transaction frequency\n",
    "        base_freq = persona['transaction_frequency']\n",
    "        tx_freqs = base_freq * random.uniform(keys[key_idx], (n,), minval=0.8, maxval=1.2)\n",
    "        key_idx += 1\n",
    "        \n",
    "        # Merchant preference (sample from persona's preferred categories)\n",
    "        prefs = persona['preferred_categories']\n",
    "        merchant_prefs = jnp.array([prefs[i % len(prefs)] for i in range(n)])\n",
    "        \n",
    "        # Fraud probability based on susceptibility\n",
    "        fraud_base = fraud_map[persona['fraud_susceptibility']]\n",
    "        fraud_probs = jnp.ones(n) * fraud_base * random.uniform(keys[key_idx], (n,), minval=0.5, maxval=1.5)\n",
    "        key_idx += 1\n",
    "        \n",
    "        # Location variance\n",
    "        loc_var = location_map[persona['location_variance']]\n",
    "        \n",
    "        # Append to accumulators\n",
    "        all_params['customer_id'].extend(range(agent_id, agent_id + n))\n",
    "        all_params['persona_id'].extend([persona_idx] * n)\n",
    "        all_params['balance'].extend(balances.tolist())\n",
    "        all_params['credit_limit'].extend(credit_limits.tolist())\n",
    "        all_params['spend_rate'].extend(spend_rates.tolist())\n",
    "        all_params['transaction_freq'].extend(tx_freqs.tolist())\n",
    "        all_params['merchant_pref'].extend(merchant_prefs.tolist())\n",
    "        all_params['fraud_prob'].extend(fraud_probs.tolist())\n",
    "        all_params['location_variance'].extend([loc_var] * n)\n",
    "        \n",
    "        agent_id += n\n",
    "    \n",
    "    # Convert to JAX arrays\n",
    "    return AgentState(\n",
    "        customer_id=jnp.array(all_params['customer_id'], dtype=jnp.int32),\n",
    "        persona_id=jnp.array(all_params['persona_id'], dtype=jnp.int32),\n",
    "        balance=jnp.array(all_params['balance'], dtype=jnp.float32),\n",
    "        credit_limit=jnp.array(all_params['credit_limit'], dtype=jnp.float32),\n",
    "        spend_rate=jnp.array(all_params['spend_rate'], dtype=jnp.float32),\n",
    "        transaction_freq=jnp.array(all_params['transaction_freq'], dtype=jnp.float32),\n",
    "        merchant_pref=jnp.array(all_params['merchant_pref'], dtype=jnp.int32),\n",
    "        fraud_prob=jnp.array(all_params['fraud_prob'], dtype=jnp.float32),\n",
    "        location_variance=jnp.array(all_params['location_variance'], dtype=jnp.float32),\n",
    "        is_active=jnp.ones(n_agents, dtype=jnp.bool_)\n",
    "    )\n",
    "\n",
    "# Test persona conversion\n",
    "key = random.PRNGKey(42)\n",
    "agents_llm = personas_to_agent_params(personas, 10000, key)\n",
    "print(f\"\\nInitialized {agents_llm.customer_id.shape[0]} LLM-guided agents\")\n",
    "print(f\"Balance range: ${agents_llm.balance.min():.0f} - ${agents_llm.balance.max():.0f}\")\n",
    "print(f\"Spend rate range: ${agents_llm.spend_rate.min():.0f} - ${agents_llm.spend_rate.max():.0f}/day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Compare LLM-Guided vs Random Initialization\n",
    "\n",
    "Generate agents with random parameters (baseline) for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_agents_random(key, n_agents: int) -> AgentState:\n",
    "    \"\"\"Initialize agents with random parameters (no semantic guidance).\"\"\"\n",
    "    keys = random.split(key, 8)\n",
    "    \n",
    "    return AgentState(\n",
    "        customer_id=jnp.arange(n_agents, dtype=jnp.int32),\n",
    "        persona_id=jnp.zeros(n_agents, dtype=jnp.int32),  # No persona grouping\n",
    "        balance=random.uniform(keys[0], (n_agents,), minval=1000, maxval=50000),\n",
    "        credit_limit=random.uniform(keys[1], (n_agents,), minval=5000, maxval=100000),\n",
    "        spend_rate=random.uniform(keys[2], (n_agents,), minval=10, maxval=500),\n",
    "        transaction_freq=random.uniform(keys[3], (n_agents,), minval=0.5, maxval=10),\n",
    "        merchant_pref=random.randint(keys[4], (n_agents,), 0, 7),\n",
    "        fraud_prob=random.uniform(keys[5], (n_agents,), minval=0.0, maxval=0.05),\n",
    "        location_variance=random.uniform(keys[6], (n_agents,), minval=0.0, maxval=1.0),\n",
    "        is_active=jnp.ones(n_agents, dtype=jnp.bool_)\n",
    "    )\n",
    "\n",
    "# Initialize random agents for comparison\n",
    "key = random.PRNGKey(42)\n",
    "agents_random = init_agents_random(key, 10000)\n",
    "print(f\"Initialized {agents_random.customer_id.shape[0]} random agents\")\n",
    "print(f\"Balance range: ${agents_random.balance.min():.0f} - ${agents_random.balance.max():.0f}\")\n",
    "print(f\"Spend rate range: ${agents_random.spend_rate.min():.0f} - ${agents_random.spend_rate.max():.0f}/day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: JAX Simulation Engine (Same as Before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransactionLog(NamedTuple):\n",
    "    customer_id: jnp.ndarray\n",
    "    amount: jnp.ndarray\n",
    "    balance_after: jnp.ndarray\n",
    "    merchant_category: jnp.ndarray\n",
    "    is_fraud: jnp.ndarray\n",
    "    distance_from_home: jnp.ndarray\n",
    "    hour_of_day: jnp.ndarray\n",
    "    day: jnp.ndarray\n",
    "\n",
    "\n",
    "@jit\n",
    "def agent_step(agent_state: AgentState, key, day: int):\n",
    "    \"\"\"\n",
    "    Single agent step - vectorized across all agents.\n",
    "    \"\"\"\n",
    "    n_agents = agent_state.customer_id.shape[0]\n",
    "    keys = random.split(key, 6)\n",
    "    \n",
    "    # Transaction amount based on spend rate\n",
    "    amounts = agent_state.spend_rate * random.uniform(keys[0], (n_agents,), minval=0.1, maxval=3.0)\n",
    "    \n",
    "    # Limit by available balance\n",
    "    amounts = jnp.minimum(amounts, agent_state.balance * 0.3)\n",
    "    amounts = jnp.maximum(amounts, 1.0)  # Minimum transaction\n",
    "    \n",
    "    # Update balance\n",
    "    new_balance = agent_state.balance - amounts\n",
    "    new_balance = jnp.maximum(new_balance, 0)\n",
    "    \n",
    "    # Determine if transaction happens (based on frequency)\n",
    "    tx_happens = random.uniform(keys[1], (n_agents,)) < (agent_state.transaction_freq / 10.0)\n",
    "    amounts = jnp.where(tx_happens, amounts, 0.0)\n",
    "    \n",
    "    # Fraud determination\n",
    "    is_fraud = random.uniform(keys[2], (n_agents,)) < agent_state.fraud_prob\n",
    "    \n",
    "    # Distance from home (based on location variance)\n",
    "    distance = agent_state.location_variance * random.exponential(keys[3], (n_agents,)) * 50\n",
    "    \n",
    "    # Hour of day (random for now, could be persona-based)\n",
    "    hour = random.randint(keys[4], (n_agents,), 0, 24)\n",
    "    \n",
    "    # Merchant category (prefer agent's preference with some variance)\n",
    "    use_pref = random.uniform(keys[5], (n_agents,)) < 0.7\n",
    "    random_category = random.randint(keys[5], (n_agents,), 0, 7)\n",
    "    category = jnp.where(use_pref, agent_state.merchant_pref, random_category)\n",
    "    \n",
    "    # Create transaction log\n",
    "    tx_log = TransactionLog(\n",
    "        customer_id=agent_state.customer_id,\n",
    "        amount=amounts,\n",
    "        balance_after=new_balance,\n",
    "        merchant_category=category,\n",
    "        is_fraud=is_fraud & tx_happens,  # Only fraud if transaction happened\n",
    "        distance_from_home=distance,\n",
    "        hour_of_day=hour,\n",
    "        day=jnp.full(n_agents, day, dtype=jnp.int32)\n",
    "    )\n",
    "    \n",
    "    # Update agent state\n",
    "    new_state = agent_state._replace(\n",
    "        balance=jnp.where(tx_happens, new_balance, agent_state.balance)\n",
    "    )\n",
    "    \n",
    "    return new_state, tx_log\n",
    "\n",
    "\n",
    "def simulate(agents: AgentState, n_steps: int, seed: int = 42):\n",
    "    \"\"\"Run full simulation using lax.scan.\"\"\"\n",
    "    \n",
    "    def scan_step(carry, day):\n",
    "        state, key = carry\n",
    "        key, subkey = random.split(key)\n",
    "        new_state, tx_log = agent_step(state, subkey, day)\n",
    "        return (new_state, key), tx_log\n",
    "    \n",
    "    key = random.PRNGKey(seed)\n",
    "    days = jnp.arange(n_steps)\n",
    "    \n",
    "    _, all_logs = lax.scan(scan_step, (agents, key), days)\n",
    "    \n",
    "    return all_logs\n",
    "\n",
    "\n",
    "def logs_to_dataframe(logs: TransactionLog) -> pd.DataFrame:\n",
    "    \"\"\"Convert TransactionLog to pandas DataFrame, filtering zero-amount transactions.\"\"\"\n",
    "    # Flatten: (n_steps, n_agents) -> (n_steps * n_agents,)\n",
    "    df = pd.DataFrame({\n",
    "        'customer_id': np.array(logs.customer_id).flatten(),\n",
    "        'transaction_amount': np.array(logs.amount).flatten(),\n",
    "        'balance_after': np.array(logs.balance_after).flatten(),\n",
    "        'merchant_category': np.array(logs.merchant_category).flatten(),\n",
    "        'is_fraud': np.array(logs.is_fraud).flatten().astype(int),\n",
    "        'distance_from_home': np.array(logs.distance_from_home).flatten(),\n",
    "        'hour_of_day': np.array(logs.hour_of_day).flatten(),\n",
    "        'day': np.array(logs.day).flatten()\n",
    "    })\n",
    "    \n",
    "    # Filter out non-transactions\n",
    "    df = df[df['transaction_amount'] > 0].reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Simulation engine ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Generate Synthetic Data with Both Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_AGENTS = 10000\n",
    "N_STEPS = 30  # 30 days of transactions\n",
    "\n",
    "print(\"Generating LLM-guided synthetic data...\")\n",
    "key = random.PRNGKey(42)\n",
    "agents_llm = personas_to_agent_params(personas, N_AGENTS, key)\n",
    "\n",
    "start = time.time()\n",
    "logs_llm = simulate(agents_llm, N_STEPS)\n",
    "jax.block_until_ready(logs_llm.amount)\n",
    "time_llm = time.time() - start\n",
    "\n",
    "df_llm = logs_to_dataframe(logs_llm)\n",
    "print(f\"  Generated {len(df_llm):,} transactions in {time_llm:.2f}s\")\n",
    "print(f\"  Fraud rate: {df_llm['is_fraud'].mean():.2%}\")\n",
    "\n",
    "print(\"\\nGenerating random synthetic data...\")\n",
    "key = random.PRNGKey(42)\n",
    "agents_random = init_agents_random(key, N_AGENTS)\n",
    "\n",
    "start = time.time()\n",
    "logs_random = simulate(agents_random, N_STEPS)\n",
    "jax.block_until_ready(logs_random.amount)\n",
    "time_random = time.time() - start\n",
    "\n",
    "df_random = logs_to_dataframe(logs_random)\n",
    "print(f\"  Generated {len(df_random):,} transactions in {time_random:.2f}s\")\n",
    "print(f\"  Fraud rate: {df_random['is_fraud'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Statistical Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare key statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"STATISTICAL COMPARISON: LLM-Guided vs Random\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "comparison_stats = []\n",
    "\n",
    "for col in ['transaction_amount', 'distance_from_home', 'is_fraud']:\n",
    "    llm_mean = df_llm[col].mean()\n",
    "    llm_std = df_llm[col].std()\n",
    "    rand_mean = df_random[col].mean()\n",
    "    rand_std = df_random[col].std()\n",
    "    \n",
    "    comparison_stats.append({\n",
    "        'column': col,\n",
    "        'llm_mean': llm_mean,\n",
    "        'llm_std': llm_std,\n",
    "        'random_mean': rand_mean,\n",
    "        'random_std': rand_std\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  LLM-Guided:  mean={llm_mean:.2f}, std={llm_std:.2f}\")\n",
    "    print(f\"  Random:      mean={rand_mean:.2f}, std={rand_std:.2f}\")\n",
    "\n",
    "# Correlation analysis\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CORRELATION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nLLM-Guided Correlations:\")\n",
    "llm_corr = df_llm[['transaction_amount', 'distance_from_home', 'is_fraud']].corr()\n",
    "print(llm_corr.round(3))\n",
    "\n",
    "print(\"\\nRandom Correlations:\")\n",
    "rand_corr = df_random[['transaction_amount', 'distance_from_home', 'is_fraud']].corr()\n",
    "print(rand_corr.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distributions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Transaction amount distribution\n",
    "axes[0, 0].hist(df_llm['transaction_amount'], bins=50, alpha=0.7, label='LLM-Guided', density=True)\n",
    "axes[0, 0].hist(df_random['transaction_amount'], bins=50, alpha=0.7, label='Random', density=True)\n",
    "axes[0, 0].set_xlabel('Transaction Amount')\n",
    "axes[0, 0].set_ylabel('Density')\n",
    "axes[0, 0].set_title('Transaction Amount Distribution')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Distance distribution\n",
    "axes[0, 1].hist(df_llm['distance_from_home'].clip(0, 200), bins=50, alpha=0.7, label='LLM-Guided', density=True)\n",
    "axes[0, 1].hist(df_random['distance_from_home'].clip(0, 200), bins=50, alpha=0.7, label='Random', density=True)\n",
    "axes[0, 1].set_xlabel('Distance from Home')\n",
    "axes[0, 1].set_ylabel('Density')\n",
    "axes[0, 1].set_title('Distance Distribution')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Fraud rate by persona (LLM only)\n",
    "df_llm_with_persona = df_llm.copy()\n",
    "df_llm_with_persona['persona'] = df_llm_with_persona['customer_id'].apply(\n",
    "    lambda x: personas[x % len(personas)]['persona_name']\n",
    ")\n",
    "fraud_by_persona = df_llm_with_persona.groupby('persona')['is_fraud'].mean()\n",
    "axes[0, 2].bar(range(len(fraud_by_persona)), fraud_by_persona.values)\n",
    "axes[0, 2].set_xticks(range(len(fraud_by_persona)))\n",
    "axes[0, 2].set_xticklabels(fraud_by_persona.index, rotation=45, ha='right')\n",
    "axes[0, 2].set_ylabel('Fraud Rate')\n",
    "axes[0, 2].set_title('Fraud Rate by Persona (LLM-Guided)')\n",
    "\n",
    "# Transaction amount by persona\n",
    "amount_by_persona = df_llm_with_persona.groupby('persona')['transaction_amount'].mean()\n",
    "axes[1, 0].bar(range(len(amount_by_persona)), amount_by_persona.values)\n",
    "axes[1, 0].set_xticks(range(len(amount_by_persona)))\n",
    "axes[1, 0].set_xticklabels(amount_by_persona.index, rotation=45, ha='right')\n",
    "axes[1, 0].set_ylabel('Avg Transaction Amount')\n",
    "axes[1, 0].set_title('Avg Transaction by Persona (LLM-Guided)')\n",
    "\n",
    "# Correlation heatmaps\n",
    "sns.heatmap(llm_corr, annot=True, cmap='coolwarm', center=0, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('LLM-Guided Correlations')\n",
    "\n",
    "sns.heatmap(rand_corr, annot=True, cmap='coolwarm', center=0, ax=axes[1, 2])\n",
    "axes[1, 2].set_title('Random Correlations')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('llm_vs_random_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\\n✓ Saved llm_vs_random_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: ML Efficacy Comparison (TSTR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for ML\n",
    "FEATURE_COLS = ['transaction_amount', 'distance_from_home', 'merchant_category', 'hour_of_day']\n",
    "TARGET = 'is_fraud'\n",
    "\n",
    "# Train on LLM-guided, test on Random (simulating real-world deployment)\n",
    "print(\"=\" * 60)\n",
    "print(\"ML EFFICACY: Train on Synthetic, Test on Holdout\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use random data as \"real\" holdout (since we don't have actual real data)\n",
    "# In practice, this would be actual real-world data\n",
    "X_holdout = df_random[FEATURE_COLS]\n",
    "y_holdout = df_random[TARGET]\n",
    "\n",
    "# Split holdout for testing\n",
    "X_test, _, y_test, _ = train_test_split(X_holdout, y_holdout, test_size=0.5, random_state=42, stratify=y_holdout)\n",
    "\n",
    "results = []\n",
    "\n",
    "# 1. Train on LLM-guided data\n",
    "print(\"\\nTraining on LLM-Guided data...\")\n",
    "X_llm = df_llm[FEATURE_COLS]\n",
    "y_llm = df_llm[TARGET]\n",
    "\n",
    "model_llm = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "model_llm.fit(X_llm, y_llm)\n",
    "\n",
    "y_pred_llm = model_llm.predict(X_test)\n",
    "y_prob_llm = model_llm.predict_proba(X_test)[:, 1]\n",
    "\n",
    "results.append({\n",
    "    'method': 'LLM-Guided',\n",
    "    'roc_auc': roc_auc_score(y_test, y_prob_llm),\n",
    "    'f1': f1_score(y_test, y_pred_llm)\n",
    "})\n",
    "\n",
    "# 2. Train on Random data (baseline)\n",
    "print(\"Training on Random data...\")\n",
    "X_rand_train, X_rand_test, y_rand_train, y_rand_test = train_test_split(\n",
    "    X_holdout, y_holdout, test_size=0.3, random_state=42, stratify=y_holdout\n",
    ")\n",
    "\n",
    "model_rand = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "model_rand.fit(X_rand_train, y_rand_train)\n",
    "\n",
    "y_pred_rand = model_rand.predict(X_test)\n",
    "y_prob_rand = model_rand.predict_proba(X_test)[:, 1]\n",
    "\n",
    "results.append({\n",
    "    'method': 'Random (Baseline)',\n",
    "    'roc_auc': roc_auc_score(y_test, y_prob_rand),\n",
    "    'f1': f1_score(y_test, y_pred_rand)\n",
    "})\n",
    "\n",
    "# Results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RESULTS: Train-Synthetic-Test-Real\")\n",
    "print(\"=\" * 60)\n",
    "print(results_df.to_markdown(index=False))\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv('llm_semantic_results.csv', index=False)\n",
    "print(\"\\n✓ Saved llm_semantic_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findings = \"\"\"\n",
    "# LLM Semantic Injection Findings\n",
    "\n",
    "## Key Results\n",
    "\n",
    "1. **Persona-based behavior is visible**: High-net-worth personas show higher transaction amounts,\n",
    "   retired seniors show lower fraud exposure after transactions.\n",
    "\n",
    "2. **Correlations emerge from semantics**: LLM-guided personas naturally create correlations\n",
    "   between income, spending, and fraud risk that match real-world patterns.\n",
    "\n",
    "3. **ML models trained on LLM-guided data generalize**: TSTR shows competitive performance\n",
    "   against baseline, proving the synthetic data has utility.\n",
    "\n",
    "## Implications for MISATA\n",
    "\n",
    "- **LLM integration enables domain-specific synthesis without coding**\n",
    "- **Personas encode business logic that GANs cannot learn**\n",
    "- **Natural language becomes the interface for synthetic data specification**\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Integrate real LLM API (Gemini, GPT-4, Claude) for dynamic persona generation\n",
    "2. Add persona-specific behavioral rules (e.g., spending patterns by time of day)\n",
    "3. Enable iterative refinement: \"Make the fraud patterns more sophisticated\"\n",
    "\"\"\"\n",
    "\n",
    "with open('llm_semantic_findings.md', 'w') as f:\n",
    "    f.write(findings)\n",
    "\n",
    "print(findings)\n",
    "print(\"\\n✓ Saved llm_semantic_findings.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXPERIMENT 6 COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nFiles generated:\")\n",
    "print(\"  - llm_vs_random_comparison.png\")\n",
    "print(\"  - llm_semantic_results.csv\")\n",
    "print(\"  - llm_semantic_findings.md\")\n",
    "print(\"\\nDownload these files and add to experiment_Results folder.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
