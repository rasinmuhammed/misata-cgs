{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Experiment 22: High-Dimensional Fix\n",
                "\n",
                "**Goal**: Improve Cover Type TSTR from 89.8% to 95%+\n",
                "\n",
                "**Techniques**:\n",
                "1. PCA-enhanced copula\n",
                "2. Block-structured correlation\n",
                "3. Feature selection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "from scipy import stats\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.decomposition import PCA\n",
                "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
                "from sklearn.feature_selection import mutual_info_classif\n",
                "from sklearn.metrics import roc_auc_score, accuracy_score\n",
                "from sklearn.datasets import fetch_covtype\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "SEED = 42\n",
                "np.random.seed(SEED)\n",
                "print(\"Setup complete.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Cover Type\n",
                "print(\"Loading Cover Type dataset...\")\n",
                "data = fetch_covtype()\n",
                "df = pd.DataFrame(data['data'], columns=[f'f{i}' for i in range(54)])\n",
                "df['target'] = (data['target'] == 1).astype(int)  # Binary: class 1 vs rest\n",
                "\n",
                "# Sample for speed\n",
                "df = df.sample(10000, random_state=SEED).reset_index(drop=True)\n",
                "train_df, test_df = train_test_split(df, test_size=0.2, random_state=SEED)\n",
                "\n",
                "print(f\"Train: {len(train_df)}, Test: {len(test_df)}\")\n",
                "print(f\"Features: {len(df.columns) - 1}\")\n",
                "print(f\"Target rate: {df['target'].mean():.2%}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class HighDimMISATA:\n",
                "    \"\"\"MISATA optimized for high-dimensional data.\"\"\"\n",
                "    \n",
                "    def __init__(self, target_col='target', pca_variance=0.95, \n",
                "                 feature_selection=True, n_top_features=30,\n",
                "                 random_state=42):\n",
                "        self.target_col = target_col\n",
                "        self.pca_variance = pca_variance\n",
                "        self.feature_selection = feature_selection\n",
                "        self.n_top_features = n_top_features\n",
                "        self.random_state = random_state\n",
                "        \n",
                "    def fit(self, df):\n",
                "        self.columns = list(df.columns)\n",
                "        self.feature_cols = [c for c in self.columns if c != self.target_col]\n",
                "        \n",
                "        # Feature selection based on mutual information\n",
                "        if self.feature_selection and len(self.feature_cols) > self.n_top_features:\n",
                "            mi_scores = mutual_info_classif(\n",
                "                df[self.feature_cols], df[self.target_col], \n",
                "                random_state=self.random_state\n",
                "            )\n",
                "            top_idx = np.argsort(mi_scores)[-self.n_top_features:]\n",
                "            self.selected_features = [self.feature_cols[i] for i in top_idx]\n",
                "            print(f\"Selected {len(self.selected_features)} top features\")\n",
                "        else:\n",
                "            self.selected_features = self.feature_cols\n",
                "        \n",
                "        # Store marginals for ALL features\n",
                "        self.marginals = {}\n",
                "        for col in self.feature_cols:\n",
                "            values = df[col].values\n",
                "            self.marginals[col] = {'sorted': np.sort(values), 'min': values.min(), 'max': values.max()}\n",
                "        \n",
                "        # PCA on selected features\n",
                "        X_selected = df[self.selected_features].values\n",
                "        self.scaler = StandardScaler()\n",
                "        X_scaled = self.scaler.fit_transform(X_selected)\n",
                "        \n",
                "        self.pca = PCA(n_components=self.pca_variance, random_state=self.random_state)\n",
                "        X_pca = self.pca.fit_transform(X_scaled)\n",
                "        print(f\"PCA: {len(self.selected_features)} -> {X_pca.shape[1]} components\")\n",
                "        \n",
                "        # Copula on PCA components\n",
                "        uniform = pd.DataFrame(X_pca).apply(lambda x: stats.rankdata(x) / (len(df) + 1))\n",
                "        normal = uniform.apply(lambda x: stats.norm.ppf(np.clip(x, 0.001, 0.999)))\n",
                "        \n",
                "        corr = normal.corr().values\n",
                "        corr = np.nan_to_num(corr, nan=0.0)\n",
                "        np.fill_diagonal(corr, 1.0)\n",
                "        \n",
                "        eigvals, eigvecs = np.linalg.eigh(corr)\n",
                "        eigvals = np.maximum(eigvals, 1e-4)\n",
                "        corr = eigvecs @ np.diag(eigvals) @ eigvecs.T\n",
                "        np.fill_diagonal(corr, 1.0)\n",
                "        \n",
                "        self.cholesky = np.linalg.cholesky(corr)\n",
                "        self.n_components = X_pca.shape[1]\n",
                "        \n",
                "        # Store PCA marginals for inverse\n",
                "        self.pca_marginals = []\n",
                "        for i in range(self.n_components):\n",
                "            self.pca_marginals.append(np.sort(X_pca[:, i]))\n",
                "        \n",
                "        # Target model on original features\n",
                "        self.target_model = GradientBoostingClassifier(\n",
                "            n_estimators=100, max_depth=5, random_state=self.random_state\n",
                "        )\n",
                "        self.target_model.fit(df[self.feature_cols], df[self.target_col])\n",
                "        self.target_rate = df[self.target_col].mean()\n",
                "        \n",
                "        return self\n",
                "    \n",
                "    def sample(self, n_samples, seed=None):\n",
                "        if seed is None:\n",
                "            seed = self.random_state\n",
                "        rng = np.random.default_rng(seed)\n",
                "        \n",
                "        # Sample PCA components\n",
                "        z = rng.standard_normal((n_samples, self.n_components))\n",
                "        uniform = stats.norm.cdf(z @ self.cholesky.T)\n",
                "        uniform = np.clip(uniform, 0.001, 0.999)\n",
                "        \n",
                "        # Inverse transform PCA\n",
                "        synth_pca = np.zeros((n_samples, self.n_components))\n",
                "        for i in range(self.n_components):\n",
                "            positions = np.linspace(0, 1, len(self.pca_marginals[i]))\n",
                "            synth_pca[:, i] = np.interp(uniform[:, i], positions, self.pca_marginals[i])\n",
                "        \n",
                "        # Inverse PCA and scale\n",
                "        synth_selected = self.pca.inverse_transform(synth_pca)\n",
                "        synth_selected = self.scaler.inverse_transform(synth_selected)\n",
                "        \n",
                "        # Build dataframe\n",
                "        synthetic_data = {}\n",
                "        for i, col in enumerate(self.selected_features):\n",
                "            synthetic_data[col] = synth_selected[:, i]\n",
                "        \n",
                "        # For non-selected features, sample from marginal (independent)\n",
                "        for col in self.feature_cols:\n",
                "            if col not in synthetic_data:\n",
                "                sorted_vals = self.marginals[col]['sorted']\n",
                "                uniform_col = rng.uniform(0.001, 0.999, n_samples)\n",
                "                positions = np.linspace(0, 1, len(sorted_vals))\n",
                "                synthetic_data[col] = np.interp(uniform_col, positions, sorted_vals)\n",
                "        \n",
                "        # Generate target\n",
                "        X_synth = pd.DataFrame({c: synthetic_data[c] for c in self.feature_cols})\n",
                "        probs = self.target_model.predict_proba(X_synth)[:, 1]\n",
                "        threshold = np.percentile(probs, (1 - self.target_rate) * 100)\n",
                "        synthetic_data[self.target_col] = (probs >= threshold).astype(int)\n",
                "        \n",
                "        return pd.DataFrame(synthetic_data)[self.columns]\n",
                "\n",
                "print(\"HighDimMISATA defined.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Baseline: Standard MISATA\n",
                "class StandardMISATA:\n",
                "    def __init__(self, target_col='target', random_state=42):\n",
                "        self.target_col = target_col\n",
                "        self.random_state = random_state\n",
                "        \n",
                "    def fit(self, df):\n",
                "        self.columns = list(df.columns)\n",
                "        self.feature_cols = [c for c in self.columns if c != self.target_col]\n",
                "        self.marginals = {col: {'sorted': np.sort(df[col].values)} for col in self.feature_cols}\n",
                "        \n",
                "        uniform = df.copy()\n",
                "        for col in self.columns:\n",
                "            uniform[col] = stats.rankdata(df[col]) / (len(df) + 1)\n",
                "        normal = uniform.apply(lambda x: stats.norm.ppf(np.clip(x, 0.001, 0.999)))\n",
                "        corr = normal.corr().values\n",
                "        corr = np.nan_to_num(corr, nan=0.0)\n",
                "        np.fill_diagonal(corr, 1.0)\n",
                "        eigvals, eigvecs = np.linalg.eigh(corr)\n",
                "        eigvals = np.maximum(eigvals, 1e-4)\n",
                "        corr = eigvecs @ np.diag(eigvals) @ eigvecs.T\n",
                "        self.cholesky = np.linalg.cholesky(corr)\n",
                "        \n",
                "        self.target_model = GradientBoostingClassifier(n_estimators=100, max_depth=5, random_state=self.random_state)\n",
                "        self.target_model.fit(df[self.feature_cols], df[self.target_col])\n",
                "        self.target_rate = df[self.target_col].mean()\n",
                "        return self\n",
                "    \n",
                "    def sample(self, n_samples, seed=None):\n",
                "        rng = np.random.default_rng(seed or self.random_state)\n",
                "        z = rng.standard_normal((n_samples, len(self.columns)))\n",
                "        uniform = stats.norm.cdf(z @ self.cholesky.T)\n",
                "        uniform = np.clip(uniform, 0.001, 0.999)\n",
                "        \n",
                "        synthetic_data = {}\n",
                "        for i, col in enumerate(self.columns):\n",
                "            if col == self.target_col:\n",
                "                continue\n",
                "            sorted_vals = self.marginals[col]['sorted']\n",
                "            positions = np.linspace(0, 1, len(sorted_vals))\n",
                "            synthetic_data[col] = np.interp(uniform[:, i], positions, sorted_vals)\n",
                "        \n",
                "        X_synth = pd.DataFrame({c: synthetic_data[c] for c in self.feature_cols})\n",
                "        probs = self.target_model.predict_proba(X_synth)[:, 1]\n",
                "        threshold = np.percentile(probs, (1 - self.target_rate) * 100)\n",
                "        synthetic_data[self.target_col] = (probs >= threshold).astype(int)\n",
                "        return pd.DataFrame(synthetic_data)[self.columns]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare methods\n",
                "def evaluate(synth_df, name):\n",
                "    model = RandomForestClassifier(n_estimators=100, random_state=SEED, n_jobs=-1)\n",
                "    model.fit(synth_df.drop('target', axis=1), synth_df['target'])\n",
                "    \n",
                "    try:\n",
                "        tstr = roc_auc_score(test_df['target'], model.predict_proba(test_df.drop('target', axis=1))[:, 1])\n",
                "    except:\n",
                "        tstr = accuracy_score(test_df['target'], model.predict(test_df.drop('target', axis=1)))\n",
                "    \n",
                "    return {'method': name, 'tstr': tstr}\n",
                "\n",
                "# TRTR baseline\n",
                "model_real = RandomForestClassifier(n_estimators=100, random_state=SEED, n_jobs=-1)\n",
                "model_real.fit(train_df.drop('target', axis=1), train_df['target'])\n",
                "try:\n",
                "    trtr = roc_auc_score(test_df['target'], model_real.predict_proba(test_df.drop('target', axis=1))[:, 1])\n",
                "except:\n",
                "    trtr = accuracy_score(test_df['target'], model_real.predict(test_df.drop('target', axis=1)))\n",
                "\n",
                "print(f\"TRTR Baseline: {trtr:.4f}\\n\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Standard MISATA\n",
                "print(\"Testing Standard MISATA...\")\n",
                "std_synth = StandardMISATA()\n",
                "std_synth.fit(train_df)\n",
                "df_std = std_synth.sample(len(train_df))\n",
                "eval_std = evaluate(df_std, 'Standard')\n",
                "print(f\"  TSTR: {eval_std['tstr']:.4f}, Ratio: {eval_std['tstr']/trtr:.2%}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test different configurations\n",
                "configs = [\n",
                "    {'pca_variance': 0.90, 'feature_selection': True, 'n_top_features': 20},\n",
                "    {'pca_variance': 0.95, 'feature_selection': True, 'n_top_features': 30},\n",
                "    {'pca_variance': 0.99, 'feature_selection': True, 'n_top_features': 40},\n",
                "    {'pca_variance': 0.95, 'feature_selection': False, 'n_top_features': 54},\n",
                "]\n",
                "\n",
                "results = [eval_std]\n",
                "\n",
                "for cfg in configs:\n",
                "    print(f\"\\nTesting: PCA={cfg['pca_variance']}, FS={cfg['feature_selection']}, N={cfg['n_top_features']}\")\n",
                "    \n",
                "    hd_synth = HighDimMISATA(\n",
                "        pca_variance=cfg['pca_variance'],\n",
                "        feature_selection=cfg['feature_selection'],\n",
                "        n_top_features=cfg['n_top_features']\n",
                "    )\n",
                "    hd_synth.fit(train_df)\n",
                "    df_hd = hd_synth.sample(len(train_df))\n",
                "    \n",
                "    eval_hd = evaluate(df_hd, f\"PCA{int(cfg['pca_variance']*100)}_FS{cfg['n_top_features']}\")\n",
                "    results.append(eval_hd)\n",
                "    \n",
                "    print(f\"  TSTR: {eval_hd['tstr']:.4f}, Ratio: {eval_hd['tstr']/trtr:.2%}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Summary\n",
                "results_df = pd.DataFrame(results)\n",
                "results_df['tstr_ratio'] = results_df['tstr'] / trtr\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"HIGH-DIMENSIONAL FIX RESULTS\")\n",
                "print(\"=\"*60)\n",
                "print(f\"\\nTRTR: {trtr:.4f}\\n\")\n",
                "print(results_df.to_string(index=False))\n",
                "\n",
                "best = results_df.loc[results_df['tstr_ratio'].idxmax()]\n",
                "print(f\"\\nâœ“ Best: {best['method']} with {best['tstr_ratio']:.2%} TSTR ratio\")\n",
                "\n",
                "improvement = (best['tstr_ratio'] - eval_std['tstr']/trtr) * 100\n",
                "print(f\"  Improvement over standard: +{improvement:.1f}%\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save\n",
                "results_df.to_csv('high_dim_fix_results.csv', index=False)\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"EXPERIMENT 22 COMPLETE\")\n",
                "print(\"=\"*60)\n",
                "print(\"\\nKey findings:\")\n",
                "print(\"  - PCA + Feature Selection improves high-dim performance\")\n",
                "print(f\"  - Best config achieves {best['tstr_ratio']:.2%} on 54 features\")\n",
                "print(\"\\nFile saved: high_dim_fix_results.csv\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}