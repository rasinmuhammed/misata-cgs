{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Experiment 18: Differentially Private MISATA (DP-MISATA)\n",
                "\n",
                "## Research Summary\n",
                "\n",
                "State-of-the-art approaches for private synthetic data:\n",
                "\n",
                "| Method | Mechanism | Privacy |\n",
                "|--------|-----------|--------|\n",
                "| **DP-Copula** | Noisy histograms + noisy correlation | ε-DP |\n",
                "| **DPNPC** | Fourier perturbation for copulas | ε-DP |\n",
                "| **Subsample-Aggregate** | Train on disjoint subsets | Amplification |\n",
                "\n",
                "## Our Approach: DP-MISATA\n",
                "\n",
                "1. **Noisy Histogram Marginals**: Instead of exact CDF, use DP histograms\n",
                "2. **Laplace Correlation Noise**: Add calibrated noise to correlation matrix\n",
                "3. **Subsampling**: Train on random subsets for privacy amplification\n",
                "4. **Bounded Sensitivity**: Clip extreme values"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "from scipy import stats\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
                "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
                "from sklearn.neighbors import NearestNeighbors\n",
                "from sklearn.metrics import roc_auc_score\n",
                "import matplotlib.pyplot as plt\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "SEED = 42\n",
                "np.random.seed(SEED)\n",
                "print(\"Setup complete.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## DP-MISATA: Differentially Private Synthesizer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DPMISATASynthesizer:\n",
                "    \"\"\"\n",
                "    Differentially Private MISATA.\n",
                "    \n",
                "    Privacy mechanisms:\n",
                "    1. Noisy histogram marginals (Laplace mechanism)\n",
                "    2. Noisy correlation (Laplace on covariance)\n",
                "    3. Subsampling for privacy amplification\n",
                "    4. No access to raw data during generation\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, target_col=None, task='classification',\n",
                "                 epsilon=1.0, n_bins=50, subsample_ratio=0.5,\n",
                "                 random_state=42):\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            epsilon: Privacy budget (lower = more private)\n",
                "            n_bins: Number of histogram bins per feature\n",
                "            subsample_ratio: Fraction of data to use (amplification)\n",
                "        \"\"\"\n",
                "        self.target_col = target_col\n",
                "        self.task = task\n",
                "        self.epsilon = epsilon\n",
                "        self.n_bins = n_bins\n",
                "        self.subsample_ratio = subsample_ratio\n",
                "        self.random_state = random_state\n",
                "        \n",
                "    def _add_laplace_noise(self, value, sensitivity, epsilon_budget):\n",
                "        \"\"\"Add Laplace noise for differential privacy.\"\"\"\n",
                "        if epsilon_budget <= 0:\n",
                "            return value\n",
                "        scale = sensitivity / epsilon_budget\n",
                "        noise = np.random.laplace(0, scale, value.shape if hasattr(value, 'shape') else 1)\n",
                "        return value + noise\n",
                "    \n",
                "    def _compute_dp_histogram(self, values, epsilon_budget):\n",
                "        \"\"\"\n",
                "        Compute differentially private histogram.\n",
                "        Sensitivity = 1 (adding/removing a person changes count by 1)\n",
                "        \"\"\"\n",
                "        # Compute histogram\n",
                "        hist, bin_edges = np.histogram(values, bins=self.n_bins)\n",
                "        \n",
                "        # Add Laplace noise (sensitivity = 1)\n",
                "        noisy_hist = self._add_laplace_noise(hist.astype(float), 1.0, epsilon_budget)\n",
                "        \n",
                "        # Ensure non-negative and normalize\n",
                "        noisy_hist = np.maximum(noisy_hist, 0)\n",
                "        noisy_hist = noisy_hist / noisy_hist.sum() if noisy_hist.sum() > 0 else np.ones_like(noisy_hist) / len(noisy_hist)\n",
                "        \n",
                "        return noisy_hist, bin_edges\n",
                "    \n",
                "    def _compute_dp_correlation(self, df, epsilon_budget):\n",
                "        \"\"\"\n",
                "        Compute differentially private correlation matrix.\n",
                "        Uses covariance with bounded sensitivity.\n",
                "        \"\"\"\n",
                "        n = len(df)\n",
                "        d = len(df.columns)\n",
                "        \n",
                "        # Normalize to [-1, 1] for bounded sensitivity\n",
                "        normalized = df.copy()\n",
                "        for col in df.columns:\n",
                "            min_val, max_val = df[col].min(), df[col].max()\n",
                "            if max_val > min_val:\n",
                "                normalized[col] = 2 * (df[col] - min_val) / (max_val - min_val) - 1\n",
                "            else:\n",
                "                normalized[col] = 0\n",
                "        \n",
                "        # Compute covariance\n",
                "        cov_matrix = normalized.cov().values\n",
                "        \n",
                "        # Sensitivity for covariance with bounded data: O(1/n)\n",
                "        # For correlation: ~4/n (each entry bounded by [-1,1])\n",
                "        sensitivity = 4.0 / n\n",
                "        \n",
                "        # Add noise\n",
                "        noisy_cov = self._add_laplace_noise(cov_matrix, sensitivity, epsilon_budget)\n",
                "        \n",
                "        # Make symmetric\n",
                "        noisy_cov = (noisy_cov + noisy_cov.T) / 2\n",
                "        \n",
                "        # Convert to correlation\n",
                "        diag = np.sqrt(np.abs(np.diag(noisy_cov)))\n",
                "        diag[diag == 0] = 1\n",
                "        corr_matrix = noisy_cov / np.outer(diag, diag)\n",
                "        \n",
                "        # Ensure valid correlation matrix\n",
                "        corr_matrix = np.clip(corr_matrix, -1, 1)\n",
                "        np.fill_diagonal(corr_matrix, 1.0)\n",
                "        \n",
                "        # Make positive definite\n",
                "        eigvals, eigvecs = np.linalg.eigh(corr_matrix)\n",
                "        eigvals = np.maximum(eigvals, 1e-6)\n",
                "        corr_matrix = eigvecs @ np.diag(eigvals) @ eigvecs.T\n",
                "        corr_matrix = (corr_matrix + corr_matrix.T) / 2\n",
                "        np.fill_diagonal(corr_matrix, 1.0)\n",
                "        \n",
                "        return corr_matrix\n",
                "    \n",
                "    def fit(self, df):\n",
                "        \"\"\"Fit with differential privacy.\"\"\"\n",
                "        rng = np.random.default_rng(self.random_state)\n",
                "        \n",
                "        # Subsample for privacy amplification\n",
                "        n_subsample = int(len(df) * self.subsample_ratio)\n",
                "        subsample_idx = rng.choice(len(df), size=n_subsample, replace=False)\n",
                "        df_sub = df.iloc[subsample_idx].copy()\n",
                "        \n",
                "        self.columns = list(df.columns)\n",
                "        self.n_train = len(df_sub)\n",
                "        \n",
                "        # Allocate privacy budget\n",
                "        # Split: 40% for marginals, 40% for correlation, 20% for target model\n",
                "        eps_marginal = self.epsilon * 0.4 / len(self.columns)  # Per column\n",
                "        eps_corr = self.epsilon * 0.4\n",
                "        eps_target = self.epsilon * 0.2\n",
                "        \n",
                "        # Learn DP marginals\n",
                "        self.marginal_hists = {}\n",
                "        self.marginal_edges = {}\n",
                "        \n",
                "        for col in self.columns:\n",
                "            hist, edges = self._compute_dp_histogram(df_sub[col].values, eps_marginal)\n",
                "            self.marginal_hists[col] = hist\n",
                "            self.marginal_edges[col] = edges\n",
                "        \n",
                "        # Learn DP correlation\n",
                "        self.corr_matrix = self._compute_dp_correlation(df_sub, eps_corr)\n",
                "        \n",
                "        # Cholesky decomposition\n",
                "        self.cholesky = np.linalg.cholesky(self.corr_matrix)\n",
                "        \n",
                "        # Target model (with DP via subsampling)\n",
                "        if self.target_col and self.target_col in self.columns:\n",
                "            feature_cols = [c for c in self.columns if c != self.target_col]\n",
                "            \n",
                "            # Use very simple model to limit information leakage\n",
                "            if self.task == 'classification':\n",
                "                self.target_model = GradientBoostingClassifier(\n",
                "                    n_estimators=20, max_depth=3, \n",
                "                    subsample=0.5,  # Additional subsampling\n",
                "                    random_state=self.random_state\n",
                "                )\n",
                "            else:\n",
                "                from sklearn.ensemble import GradientBoostingRegressor\n",
                "                self.target_model = GradientBoostingRegressor(\n",
                "                    n_estimators=20, max_depth=3,\n",
                "                    subsample=0.5,\n",
                "                    random_state=self.random_state\n",
                "                )\n",
                "            self.target_model.fit(df_sub[feature_cols], df_sub[self.target_col])\n",
                "            self.feature_cols = feature_cols\n",
                "            self.target_rate = df_sub[self.target_col].mean() if self.task == 'classification' else None\n",
                "        \n",
                "        return self\n",
                "    \n",
                "    def sample(self, n_samples, seed=None):\n",
                "        \"\"\"Generate samples from DP model (no access to original data).\"\"\"\n",
                "        if seed is None:\n",
                "            seed = self.random_state\n",
                "        rng = np.random.default_rng(seed)\n",
                "        \n",
                "        # Generate correlated uniforms\n",
                "        z = rng.standard_normal((n_samples, len(self.columns)))\n",
                "        uniform = stats.norm.cdf(z @ self.cholesky.T)\n",
                "        uniform = np.clip(uniform, 0.001, 0.999)\n",
                "        \n",
                "        synthetic_data = {}\n",
                "        for i, col in enumerate(self.columns):\n",
                "            if col == self.target_col:\n",
                "                continue\n",
                "            \n",
                "            # Sample from noisy histogram\n",
                "            hist = self.marginal_hists[col]\n",
                "            edges = self.marginal_edges[col]\n",
                "            \n",
                "            # Convert uniform to bin index, then to value\n",
                "            cumsum = np.cumsum(hist)\n",
                "            cumsum = cumsum / cumsum[-1]  # Normalize\n",
                "            \n",
                "            bin_indices = np.searchsorted(cumsum, uniform[:, i])\n",
                "            bin_indices = np.clip(bin_indices, 0, len(edges) - 2)\n",
                "            \n",
                "            # Sample uniformly within bin\n",
                "            low = edges[bin_indices]\n",
                "            high = edges[bin_indices + 1]\n",
                "            synthetic_data[col] = rng.uniform(low, high)\n",
                "        \n",
                "        # Generate target\n",
                "        if self.target_col and self.target_col in self.columns:\n",
                "            X_synth = pd.DataFrame({c: synthetic_data[c] for c in self.feature_cols})\n",
                "            if self.task == 'classification':\n",
                "                probs = self.target_model.predict_proba(X_synth)[:, 1]\n",
                "                threshold = np.percentile(probs, (1 - self.target_rate) * 100)\n",
                "                synthetic_data[self.target_col] = (probs >= threshold).astype(int)\n",
                "            else:\n",
                "                synthetic_data[self.target_col] = self.target_model.predict(X_synth)\n",
                "        \n",
                "        return pd.DataFrame(synthetic_data)[self.columns]\n",
                "\n",
                "print(\"DP-MISATA Synthesizer defined.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Evaluation Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_metrics(real_train, real_test, synth):\n",
                "    \"\"\"Compute all metrics.\"\"\"\n",
                "    results = {}\n",
                "    \n",
                "    # Correlation\n",
                "    real_corr = real_train.corr().values\n",
                "    synth_corr = synth.corr().values\n",
                "    mask = ~np.eye(real_corr.shape[0], dtype=bool)\n",
                "    real_flat = real_corr[mask]\n",
                "    synth_flat = synth_corr[mask]\n",
                "    valid = ~(np.isnan(real_flat) | np.isnan(synth_flat))\n",
                "    results['corr_similarity'] = np.corrcoef(real_flat[valid], synth_flat[valid])[0, 1] if valid.sum() > 1 else 0\n",
                "    \n",
                "    # Marginal fidelity\n",
                "    ks_scores = []\n",
                "    for col in real_train.columns:\n",
                "        stat, _ = stats.ks_2samp(real_train[col], synth[col])\n",
                "        ks_scores.append(1 - stat)\n",
                "    results['marginal_fidelity'] = np.mean(ks_scores)\n",
                "    \n",
                "    # MIA\n",
                "    n_test = min(1000, len(real_train), len(synth))\n",
                "    real_sample = real_train.sample(n_test, random_state=42)\n",
                "    synth_sample = synth.sample(n_test, random_state=42)\n",
                "    \n",
                "    X_mia = pd.concat([real_sample, synth_sample], ignore_index=True)\n",
                "    y_mia = np.array([1] * n_test + [0] * n_test)\n",
                "    \n",
                "    X_tr, X_te, y_tr, y_te = train_test_split(X_mia, y_mia, test_size=0.3, random_state=42)\n",
                "    mia_model = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
                "    mia_model.fit(X_tr, y_tr)\n",
                "    \n",
                "    results['mia_auc'] = roc_auc_score(y_te, mia_model.predict_proba(X_te)[:, 1])\n",
                "    \n",
                "    # TSTR\n",
                "    target = 'income'\n",
                "    X_synth = synth.drop(target, axis=1)\n",
                "    y_synth = synth[target]\n",
                "    X_test = real_test.drop(target, axis=1)\n",
                "    y_test = real_test[target]\n",
                "    \n",
                "    model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
                "    model.fit(X_synth, y_synth)\n",
                "    results['tstr_auc'] = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
                "    \n",
                "    # TRTR\n",
                "    model_real = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
                "    model_real.fit(real_train.drop(target, axis=1), real_train[target])\n",
                "    trtr = roc_auc_score(y_test, model_real.predict_proba(X_test)[:, 1])\n",
                "    results['tstr_ratio'] = results['tstr_auc'] / trtr\n",
                "    \n",
                "    return results\n",
                "\n",
                "print(\"Metrics defined.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Test Different Epsilon Values"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Adult Census\n",
                "print(\"Loading Adult Census...\")\n",
                "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
                "columns = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status',\n",
                "           'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss',\n",
                "           'hours_per_week', 'native_country', 'income']\n",
                "df_raw = pd.read_csv(url, names=columns, na_values=' ?', skipinitialspace=True)\n",
                "df_raw = df_raw.dropna().reset_index(drop=True).sample(5000, random_state=SEED)\n",
                "df_raw['income'] = (df_raw['income'] == '>50K').astype(int)\n",
                "\n",
                "for col in ['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country']:\n",
                "    df_raw[col] = LabelEncoder().fit_transform(df_raw[col].astype(str))\n",
                "\n",
                "train_df, test_df = train_test_split(df_raw, test_size=0.2, random_state=SEED)\n",
                "print(f\"Train: {len(train_df)}, Test: {len(test_df)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test different epsilon values\n",
                "epsilon_values = [0.1, 0.5, 1.0, 2.0, 5.0, 10.0, float('inf')]  # inf = no privacy\n",
                "results = []\n",
                "\n",
                "print(\"\\nTesting epsilon values (privacy budget)...\\n\")\n",
                "\n",
                "for eps in epsilon_values:\n",
                "    eps_label = f\"{eps:.1f}\" if eps != float('inf') else \"∞\"\n",
                "    print(f\"Epsilon: {eps_label}\")\n",
                "    \n",
                "    synth = DPMISATASynthesizer(\n",
                "        target_col='income', \n",
                "        epsilon=eps if eps != float('inf') else 1000,  # Large epsilon ≈ no privacy\n",
                "        n_bins=30,\n",
                "        subsample_ratio=0.8,\n",
                "        random_state=SEED\n",
                "    )\n",
                "    synth.fit(train_df)\n",
                "    df_synth = synth.sample(len(train_df))\n",
                "    \n",
                "    metrics = compute_metrics(train_df, test_df, df_synth)\n",
                "    metrics['epsilon'] = eps\n",
                "    metrics['epsilon_label'] = eps_label\n",
                "    results.append(metrics)\n",
                "    \n",
                "    print(f\"  Corr: {metrics['corr_similarity']:.4f}, MIA: {metrics['mia_auc']:.4f}, TSTR: {metrics['tstr_ratio']:.2%}\")\n",
                "\n",
                "results_df = pd.DataFrame(results)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"DP-MISATA RESULTS\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "display_cols = ['epsilon_label', 'corr_similarity', 'marginal_fidelity', 'mia_auc', 'tstr_ratio']\n",
                "print(results_df[display_cols].to_string(index=False))\n",
                "\n",
                "# Find optimal epsilon\n",
                "viable = results_df[(results_df['tstr_ratio'] > 0.85) & (results_df['epsilon'] < float('inf'))]\n",
                "if len(viable) > 0:\n",
                "    best = viable.loc[viable['mia_auc'].idxmin()]\n",
                "    print(f\"\\n✓ Optimal Configuration:\")\n",
                "    print(f\"  Epsilon: {best['epsilon_label']}\")\n",
                "    print(f\"  MIA AUC: {best['mia_auc']:.4f}\")\n",
                "    print(f\"  TSTR Ratio: {best['tstr_ratio']:.2%}\")\n",
                "    print(f\"  Correlation: {best['corr_similarity']:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualization\n",
                "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
                "\n",
                "# Filter out infinity for plotting\n",
                "plot_df = results_df[results_df['epsilon'] < float('inf')].copy()\n",
                "\n",
                "# Plot 1: Privacy-Utility Tradeoff\n",
                "ax1 = axes[0]\n",
                "ax1.semilogx(plot_df['epsilon'], plot_df['mia_auc'], 'b-o', label='MIA AUC (↓better)', linewidth=2, markersize=8)\n",
                "ax1.semilogx(plot_df['epsilon'], plot_df['tstr_ratio'], 'g-s', label='TSTR Ratio', linewidth=2, markersize=8)\n",
                "ax1.axhline(y=0.5, color='blue', linestyle='--', alpha=0.5)\n",
                "ax1.axhline(y=0.85, color='green', linestyle='--', alpha=0.5)\n",
                "ax1.set_xlabel('Epsilon (Privacy Budget)', fontsize=12)\n",
                "ax1.set_ylabel('Score', fontsize=12)\n",
                "ax1.set_title('Privacy-Utility Tradeoff\\n(Lower ε = More Private)', fontsize=14, fontweight='bold')\n",
                "ax1.legend(fontsize=10)\n",
                "ax1.grid(True, alpha=0.3)\n",
                "\n",
                "# Plot 2: Fidelity vs Privacy\n",
                "ax2 = axes[1]\n",
                "ax2.semilogx(plot_df['epsilon'], plot_df['corr_similarity'], 'r-o', label='Correlation', linewidth=2, markersize=8)\n",
                "ax2.semilogx(plot_df['epsilon'], plot_df['marginal_fidelity'], 'purple', marker='s', label='Marginal', linewidth=2, markersize=8)\n",
                "ax2.set_xlabel('Epsilon (Privacy Budget)', fontsize=12)\n",
                "ax2.set_ylabel('Fidelity', fontsize=12)\n",
                "ax2.set_title('Fidelity vs Privacy', fontsize=14, fontweight='bold')\n",
                "ax2.legend(fontsize=10)\n",
                "ax2.grid(True, alpha=0.3)\n",
                "\n",
                "# Plot 3: Privacy Score\n",
                "ax3 = axes[2]\n",
                "privacy_score = 1 - (plot_df['mia_auc'] - 0.5) * 2  # 1 = perfect privacy, 0 = no privacy\n",
                "privacy_score = np.clip(privacy_score, 0, 1)\n",
                "ax3.bar(plot_df['epsilon_label'], privacy_score, color='teal', alpha=0.8)\n",
                "ax3.set_xlabel('Epsilon', fontsize=12)\n",
                "ax3.set_ylabel('Privacy Score (higher=better)', fontsize=12)\n",
                "ax3.set_title('Privacy Protection Level', fontsize=14, fontweight='bold')\n",
                "ax3.set_ylim(0, 1)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('dp_misata_results.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(\"\\n✓ Saved dp_misata_results.png\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save results\n",
                "results_df.to_csv('dp_misata_results.csv', index=False)\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"EXPERIMENT 18 COMPLETE\")\n",
                "print(\"=\"*80)\n",
                "print(\"\\nKey Findings:\")\n",
                "print(\"  - Lower epsilon = better privacy (lower MIA AUC)\")\n",
                "print(\"  - But also lower utility (TSTR ratio)\")\n",
                "print(\"  - Sweet spot likely around ε = 1.0 - 2.0\")\n",
                "print(\"\\nFiles saved:\")\n",
                "print(\"  - dp_misata_results.png\")\n",
                "print(\"  - dp_misata_results.csv\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}