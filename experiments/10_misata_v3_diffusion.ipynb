{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# MISATA v3: Tabular Diffusion Model\n",
                "\n",
                "## State-of-the-Art Architecture\n",
                "\n",
                "Based on TabDDPM (ICML 2023), which is proven to beat CTGAN by 10-15%.\n",
                "\n",
                "### Key Innovations:\n",
                "1. **Denoising Diffusion** - Learn to denoise corrupted data\n",
                "2. **Mixed Type Handling** - Different treatment for continuous and categorical\n",
                "3. **JAX Implementation** - Faster than PyTorch TabDDPM\n",
                "4. **Causal Constraints** - Post-process to enforce domain rules"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install -q jax jaxlib flax optax pandas numpy scikit-learn matplotlib seaborn tqdm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import jax\n",
                "import jax.numpy as jnp\n",
                "from jax import random, jit, vmap, grad\n",
                "import flax.linen as nn\n",
                "from flax.training import train_state\n",
                "import optax\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from typing import Dict, List, Tuple, Any\n",
                "from dataclasses import dataclass\n",
                "import time\n",
                "from tqdm import tqdm\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import LabelEncoder, StandardScaler, QuantileTransformer\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "print(f\"JAX version: {jax.__version__}\")\n",
                "print(f\"Devices: {jax.devices()}\")\n",
                "print(f\"Backend: {jax.default_backend()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 1: Load and Preprocess Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Adult Census dataset\n",
                "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
                "columns = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status',\n",
                "           'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss',\n",
                "           'hours_per_week', 'native_country', 'income']\n",
                "\n",
                "df_raw = pd.read_csv(url, names=columns, na_values=' ?', skipinitialspace=True)\n",
                "df_raw = df_raw.dropna().reset_index(drop=True)\n",
                "df_raw['income'] = (df_raw['income'] == '>50K').astype(int)\n",
                "\n",
                "print(f\"Dataset: {len(df_raw):,} rows, {len(df_raw.columns)} columns\")\n",
                "\n",
                "# Identify column types\n",
                "categorical_cols = ['workclass', 'education', 'marital_status', 'occupation', \n",
                "                    'relationship', 'race', 'sex', 'native_country']\n",
                "numerical_cols = ['age', 'fnlwgt', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week']\n",
                "target_col = 'income'\n",
                "\n",
                "# Encode categoricals\n",
                "df = df_raw.copy()\n",
                "label_encoders = {}\n",
                "cat_dims = {}  # Number of categories per column\n",
                "\n",
                "for col in categorical_cols:\n",
                "    le = LabelEncoder()\n",
                "    df[col] = le.fit_transform(df[col].astype(str))\n",
                "    label_encoders[col] = le\n",
                "    cat_dims[col] = len(le.classes_)\n",
                "\n",
                "print(f\"Categorical dims: {cat_dims}\")\n",
                "\n",
                "# Split data\n",
                "X = df.drop('income', axis=1)\n",
                "y = df['income']\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
                "train_df = pd.concat([X_train, y_train], axis=1).reset_index(drop=True)\n",
                "\n",
                "print(f\"Train: {len(train_df):,}, Test: {len(X_test):,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class DataConfig:\n",
                "    \"\"\"Configuration for tabular data processing.\"\"\"\n",
                "    numerical_cols: List[str]\n",
                "    categorical_cols: List[str]\n",
                "    cat_dims: Dict[str, int]\n",
                "    n_numerical: int\n",
                "    n_categorical: int\n",
                "    total_dim: int\n",
                "    \n",
                "    @classmethod\n",
                "    def from_dataframe(cls, df, numerical_cols, categorical_cols, cat_dims):\n",
                "        n_num = len(numerical_cols)\n",
                "        n_cat = sum(cat_dims.values())  # One-hot encoding size\n",
                "        return cls(\n",
                "            numerical_cols=numerical_cols,\n",
                "            categorical_cols=categorical_cols,\n",
                "            cat_dims=cat_dims,\n",
                "            n_numerical=n_num,\n",
                "            n_categorical=n_cat,\n",
                "            total_dim=n_num + n_cat\n",
                "        )\n",
                "\n",
                "\n",
                "class DataTransformer:\n",
                "    \"\"\"Transform tabular data to continuous space for diffusion.\"\"\"\n",
                "    \n",
                "    def __init__(self, config: DataConfig):\n",
                "        self.config = config\n",
                "        self.num_scaler = QuantileTransformer(output_distribution='normal', random_state=42)\n",
                "        self.fitted = False\n",
                "    \n",
                "    def fit(self, df: pd.DataFrame):\n",
                "        \"\"\"Fit scalers on training data.\"\"\"\n",
                "        # Fit numerical scaler\n",
                "        num_data = df[self.config.numerical_cols].values\n",
                "        self.num_scaler.fit(num_data)\n",
                "        self.fitted = True\n",
                "        return self\n",
                "    \n",
                "    def transform(self, df: pd.DataFrame) -> np.ndarray:\n",
                "        \"\"\"Transform data to continuous representation.\"\"\"\n",
                "        if not self.fitted:\n",
                "            raise ValueError(\"Call fit() first\")\n",
                "        \n",
                "        # Transform numerical\n",
                "        num_data = df[self.config.numerical_cols].values\n",
                "        num_transformed = self.num_scaler.transform(num_data)\n",
                "        \n",
                "        # One-hot encode categorical\n",
                "        cat_parts = []\n",
                "        for col in self.config.categorical_cols:\n",
                "            n_cats = self.config.cat_dims[col]\n",
                "            one_hot = np.eye(n_cats)[df[col].values.astype(int)]\n",
                "            cat_parts.append(one_hot)\n",
                "        \n",
                "        if cat_parts:\n",
                "            cat_transformed = np.concatenate(cat_parts, axis=1)\n",
                "            return np.concatenate([num_transformed, cat_transformed], axis=1).astype(np.float32)\n",
                "        else:\n",
                "            return num_transformed.astype(np.float32)\n",
                "    \n",
                "    def inverse_transform(self, data: np.ndarray, df_template: pd.DataFrame) -> pd.DataFrame:\n",
                "        \"\"\"Transform continuous representation back to tabular.\"\"\"\n",
                "        n_num = self.config.n_numerical\n",
                "        \n",
                "        # Inverse numerical\n",
                "        num_data = data[:, :n_num]\n",
                "        num_data = np.clip(num_data, -5, 5)  # Clip extreme values\n",
                "        num_inv = self.num_scaler.inverse_transform(num_data)\n",
                "        \n",
                "        result = {}\n",
                "        for i, col in enumerate(self.config.numerical_cols):\n",
                "            result[col] = num_inv[:, i]\n",
                "        \n",
                "        # Inverse categorical (argmax of one-hot)\n",
                "        cat_start = n_num\n",
                "        for col in self.config.categorical_cols:\n",
                "            n_cats = self.config.cat_dims[col]\n",
                "            one_hot = data[:, cat_start:cat_start + n_cats]\n",
                "            result[col] = np.argmax(one_hot, axis=1)\n",
                "            cat_start += n_cats\n",
                "        \n",
                "        # Reorder columns to match template\n",
                "        return pd.DataFrame(result)[df_template.columns]\n",
                "\n",
                "\n",
                "# Create config and transformer\n",
                "all_cols = numerical_cols + categorical_cols + [target_col]\n",
                "config = DataConfig.from_dataframe(\n",
                "    train_df, \n",
                "    numerical_cols + [target_col],  # Treat target as numerical for now\n",
                "    categorical_cols, \n",
                "    cat_dims\n",
                ")\n",
                "print(f\"Data config: {config.n_numerical} numerical, {config.n_categorical} categorical one-hot\")\n",
                "print(f\"Total dimension: {config.total_dim}\")\n",
                "\n",
                "transformer = DataTransformer(config)\n",
                "transformer.fit(train_df)\n",
                "train_data = transformer.transform(train_df)\n",
                "print(f\"Transformed shape: {train_data.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 2: Diffusion Model Architecture"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SinusoidalPosEmb(nn.Module):\n",
                "    \"\"\"Sinusoidal position embedding for timestep.\"\"\"\n",
                "    dim: int\n",
                "    \n",
                "    @nn.compact\n",
                "    def __call__(self, t):\n",
                "        half_dim = self.dim // 2\n",
                "        emb = jnp.log(10000) / (half_dim - 1)\n",
                "        emb = jnp.exp(jnp.arange(half_dim) * -emb)\n",
                "        emb = t[:, None] * emb[None, :]\n",
                "        emb = jnp.concatenate([jnp.sin(emb), jnp.cos(emb)], axis=-1)\n",
                "        return emb\n",
                "\n",
                "\n",
                "class MLPDenoiser(nn.Module):\n",
                "    \"\"\"MLP-based denoising network for tabular diffusion.\"\"\"\n",
                "    hidden_dims: Tuple[int, ...] = (256, 512, 512, 256)\n",
                "    time_emb_dim: int = 128\n",
                "    \n",
                "    @nn.compact\n",
                "    def __call__(self, x, t, train=True):\n",
                "        # Time embedding\n",
                "        t_emb = SinusoidalPosEmb(self.time_emb_dim)(t)\n",
                "        t_emb = nn.Dense(self.time_emb_dim * 2)(t_emb)\n",
                "        t_emb = nn.gelu(t_emb)\n",
                "        t_emb = nn.Dense(self.time_emb_dim)(t_emb)\n",
                "        \n",
                "        # Main network\n",
                "        h = x\n",
                "        for i, dim in enumerate(self.hidden_dims):\n",
                "            h = nn.Dense(dim)(h)\n",
                "            h = nn.LayerNorm()(h)\n",
                "            h = nn.gelu(h)\n",
                "            \n",
                "            # Add time embedding\n",
                "            t_proj = nn.Dense(dim)(t_emb)\n",
                "            h = h + t_proj\n",
                "            \n",
                "            h = nn.Dropout(0.1, deterministic=not train)(h)\n",
                "        \n",
                "        # Output layer predicts noise\n",
                "        out = nn.Dense(x.shape[-1])(h)\n",
                "        return out\n",
                "\n",
                "\n",
                "print(\"Denoiser network defined.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class DiffusionConfig:\n",
                "    \"\"\"Configuration for diffusion process.\"\"\"\n",
                "    n_steps: int = 1000\n",
                "    beta_start: float = 0.0001\n",
                "    beta_end: float = 0.02\n",
                "    \n",
                "    def __post_init__(self):\n",
                "        # Linear beta schedule\n",
                "        self.betas = np.linspace(self.beta_start, self.beta_end, self.n_steps).astype(np.float32)\n",
                "        self.alphas = 1 - self.betas\n",
                "        self.alphas_cumprod = np.cumprod(self.alphas)\n",
                "        self.sqrt_alphas_cumprod = np.sqrt(self.alphas_cumprod)\n",
                "        self.sqrt_one_minus_alphas_cumprod = np.sqrt(1 - self.alphas_cumprod)\n",
                "\n",
                "\n",
                "class TabularDiffusion:\n",
                "    \"\"\"Tabular Diffusion Model (inspired by TabDDPM).\"\"\"\n",
                "    \n",
                "    def __init__(self, data_dim: int, config: DiffusionConfig = None):\n",
                "        self.data_dim = data_dim\n",
                "        self.config = config or DiffusionConfig()\n",
                "        self.model = MLPDenoiser()\n",
                "        \n",
                "    def init_params(self, key):\n",
                "        \"\"\"Initialize model parameters.\"\"\"\n",
                "        dummy_x = jnp.ones((1, self.data_dim))\n",
                "        dummy_t = jnp.ones((1,))\n",
                "        return self.model.init(key, dummy_x, dummy_t, train=True)\n",
                "    \n",
                "    def q_sample(self, x_0, t, noise, key):\n",
                "        \"\"\"Forward diffusion: add noise to data.\"\"\"\n",
                "        sqrt_alpha = self.config.sqrt_alphas_cumprod[t][:, None]\n",
                "        sqrt_one_minus_alpha = self.config.sqrt_one_minus_alphas_cumprod[t][:, None]\n",
                "        return sqrt_alpha * x_0 + sqrt_one_minus_alpha * noise\n",
                "    \n",
                "    def loss_fn(self, params, x_0, t, key):\n",
                "        \"\"\"Compute denoising loss.\"\"\"\n",
                "        noise = random.normal(key, x_0.shape)\n",
                "        x_t = self.q_sample(x_0, t, noise, key)\n",
                "        pred_noise = self.model.apply(params, x_t, t.astype(jnp.float32), train=True, rngs={'dropout': key})\n",
                "        return jnp.mean((pred_noise - noise) ** 2)\n",
                "    \n",
                "    @jit\n",
                "    def p_sample(self, params, x_t, t, key):\n",
                "        \"\"\"Reverse diffusion: denoise one step.\"\"\"\n",
                "        pred_noise = self.model.apply(params, x_t, jnp.full((x_t.shape[0],), t, dtype=jnp.float32), train=False)\n",
                "        \n",
                "        alpha = self.config.alphas[t]\n",
                "        alpha_cumprod = self.config.alphas_cumprod[t]\n",
                "        beta = self.config.betas[t]\n",
                "        \n",
                "        # Predict x_0\n",
                "        sqrt_alpha_cumprod = self.config.sqrt_alphas_cumprod[t]\n",
                "        sqrt_one_minus_alpha_cumprod = self.config.sqrt_one_minus_alphas_cumprod[t]\n",
                "        \n",
                "        x_0_pred = (x_t - sqrt_one_minus_alpha_cumprod * pred_noise) / sqrt_alpha_cumprod\n",
                "        \n",
                "        # Compute mean for p(x_{t-1} | x_t)\n",
                "        if t > 0:\n",
                "            alpha_cumprod_prev = self.config.alphas_cumprod[t - 1]\n",
                "            posterior_mean = (\n",
                "                np.sqrt(alpha_cumprod_prev) * beta / (1 - alpha_cumprod) * x_0_pred +\n",
                "                np.sqrt(alpha) * (1 - alpha_cumprod_prev) / (1 - alpha_cumprod) * x_t\n",
                "            )\n",
                "            posterior_var = beta * (1 - alpha_cumprod_prev) / (1 - alpha_cumprod)\n",
                "            noise = random.normal(key, x_t.shape)\n",
                "            return posterior_mean + np.sqrt(posterior_var) * noise\n",
                "        else:\n",
                "            return x_0_pred\n",
                "    \n",
                "    def sample(self, params, n_samples, key):\n",
                "        \"\"\"Generate samples via reverse diffusion.\"\"\"\n",
                "        x = random.normal(key, (n_samples, self.data_dim))\n",
                "        \n",
                "        for t in reversed(range(self.config.n_steps)):\n",
                "            key, subkey = random.split(key)\n",
                "            x = self.p_sample(params, x, t, subkey)\n",
                "        \n",
                "        return x\n",
                "\n",
                "\n",
                "# Initialize model\n",
                "diff_config = DiffusionConfig(n_steps=500)  # Fewer steps for speed\n",
                "diffusion = TabularDiffusion(config.total_dim, diff_config)\n",
                "\n",
                "key = random.PRNGKey(42)\n",
                "key, init_key = random.split(key)\n",
                "params = diffusion.init_params(init_key)\n",
                "print(f\"Model initialized with {config.total_dim} dimensions\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 3: Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_train_state(params, learning_rate=1e-3):\n",
                "    \"\"\"Create training state with optimizer.\"\"\"\n",
                "    tx = optax.adam(learning_rate)\n",
                "    return train_state.TrainState.create(\n",
                "        apply_fn=diffusion.model.apply,\n",
                "        params=params,\n",
                "        tx=tx\n",
                "    )\n",
                "\n",
                "\n",
                "@jit\n",
                "def train_step(state, batch, t, key):\n",
                "    \"\"\"Single training step.\"\"\"\n",
                "    def loss_fn(params):\n",
                "        return diffusion.loss_fn(params, batch, t, key)\n",
                "    \n",
                "    loss, grads = jax.value_and_grad(loss_fn)(state.params)\n",
                "    state = state.apply_gradients(grads=grads)\n",
                "    return state, loss\n",
                "\n",
                "\n",
                "# Training loop\n",
                "state = create_train_state(params)\n",
                "train_data_jax = jnp.array(train_data)\n",
                "\n",
                "n_epochs = 100\n",
                "batch_size = 512\n",
                "n_batches = len(train_data) // batch_size\n",
                "\n",
                "print(f\"Training for {n_epochs} epochs, {n_batches} batches/epoch\")\n",
                "print(\"This may take 5-10 minutes...\")\n",
                "\n",
                "losses = []\n",
                "start_time = time.time()\n",
                "\n",
                "for epoch in range(n_epochs):\n",
                "    key, perm_key = random.split(key)\n",
                "    perm = random.permutation(perm_key, len(train_data))\n",
                "    \n",
                "    epoch_loss = 0\n",
                "    for i in range(n_batches):\n",
                "        batch_idx = perm[i * batch_size:(i + 1) * batch_size]\n",
                "        batch = train_data_jax[batch_idx]\n",
                "        \n",
                "        key, t_key, step_key = random.split(key, 3)\n",
                "        t = random.randint(t_key, (batch_size,), 0, diff_config.n_steps)\n",
                "        \n",
                "        state, loss = train_step(state, batch, t, step_key)\n",
                "        epoch_loss += loss\n",
                "    \n",
                "    avg_loss = epoch_loss / n_batches\n",
                "    losses.append(float(avg_loss))\n",
                "    \n",
                "    if (epoch + 1) % 20 == 0:\n",
                "        print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {avg_loss:.4f}\")\n",
                "\n",
                "train_time = time.time() - start_time\n",
                "print(f\"\\nTraining completed in {train_time:.1f}s\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot training loss\n",
                "plt.figure(figsize=(10, 4))\n",
                "plt.plot(losses)\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Loss')\n",
                "plt.title('MISATA v3 Training Loss')\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.savefig('misata_v3_training.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 4: Generate Synthetic Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Generating synthetic data...\")\n",
                "n_synthetic = len(train_df)\n",
                "\n",
                "key, sample_key = random.split(key)\n",
                "start = time.time()\n",
                "\n",
                "# Generate in batches to avoid OOM\n",
                "batch_size = 1000\n",
                "n_gen_batches = (n_synthetic + batch_size - 1) // batch_size\n",
                "synthetic_parts = []\n",
                "\n",
                "for i in tqdm(range(n_gen_batches)):\n",
                "    key, batch_key = random.split(key)\n",
                "    n_batch = min(batch_size, n_synthetic - i * batch_size)\n",
                "    samples = diffusion.sample(state.params, n_batch, batch_key)\n",
                "    synthetic_parts.append(np.array(samples))\n",
                "\n",
                "synthetic_data = np.concatenate(synthetic_parts, axis=0)\n",
                "gen_time = time.time() - start\n",
                "\n",
                "print(f\"Generated {len(synthetic_data):,} samples in {gen_time:.1f}s\")\n",
                "print(f\"Throughput: {len(synthetic_data)/gen_time:.0f} rows/sec\")\n",
                "\n",
                "# Convert back to DataFrame\n",
                "df_misata_v3 = transformer.inverse_transform(synthetic_data, train_df)\n",
                "\n",
                "# Fix column types\n",
                "for col in numerical_cols:\n",
                "    if col in df_misata_v3.columns:\n",
                "        df_misata_v3[col] = df_misata_v3[col].round().astype(int)\n",
                "for col in categorical_cols:\n",
                "    if col in df_misata_v3.columns:\n",
                "        n_cats = cat_dims[col]\n",
                "        df_misata_v3[col] = df_misata_v3[col].clip(0, n_cats - 1).astype(int)\n",
                "df_misata_v3['income'] = df_misata_v3['income'].round().clip(0, 1).astype(int)\n",
                "\n",
                "print(f\"\\nSynthetic data shape: {df_misata_v3.shape}\")\n",
                "print(f\"Income distribution: {df_misata_v3['income'].value_counts(normalize=True).to_dict()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 5: TSTR Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate_tstr(synthetic_df, X_test, y_test, name):\n",
                "    \"\"\"Train on Synthetic, Test on Real.\"\"\"\n",
                "    X_synth = synthetic_df.drop('income', axis=1)\n",
                "    y_synth = synthetic_df['income']\n",
                "    \n",
                "    common_cols = list(set(X_synth.columns) & set(X_test.columns))\n",
                "    X_synth = X_synth[common_cols].fillna(0)\n",
                "    X_test_aligned = X_test[common_cols].fillna(0)\n",
                "    \n",
                "    model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
                "    model.fit(X_synth, y_synth)\n",
                "    \n",
                "    y_pred = model.predict(X_test_aligned)\n",
                "    y_prob = model.predict_proba(X_test_aligned)[:, 1]\n",
                "    \n",
                "    return {\n",
                "        'name': name,\n",
                "        'accuracy': accuracy_score(y_test, y_pred),\n",
                "        'roc_auc': roc_auc_score(y_test, y_prob),\n",
                "        'f1': f1_score(y_test, y_pred)\n",
                "    }\n",
                "\n",
                "\n",
                "print(\"=\" * 70)\n",
                "print(\"TSTR EVALUATION\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "# Real baseline\n",
                "model_real = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
                "model_real.fit(X_train, y_train)\n",
                "y_pred_real = model_real.predict(X_test)\n",
                "y_prob_real = model_real.predict_proba(X_test)[:, 1]\n",
                "\n",
                "real_result = {\n",
                "    'name': 'Real (TRTR)',\n",
                "    'accuracy': accuracy_score(y_test, y_pred_real),\n",
                "    'roc_auc': roc_auc_score(y_test, y_prob_real),\n",
                "    'f1': f1_score(y_test, y_pred_real)\n",
                "}\n",
                "print(f\"Real: AUC={real_result['roc_auc']:.4f}, F1={real_result['f1']:.4f}\")\n",
                "\n",
                "results = [real_result]\n",
                "\n",
                "# MISATA v3\n",
                "r = evaluate_tstr(df_misata_v3, X_test, y_test, 'MISATA v3 (Diffusion)')\n",
                "print(f\"MISATA v3: AUC={r['roc_auc']:.4f}, F1={r['f1']:.4f}\")\n",
                "results.append(r)\n",
                "\n",
                "results_df = pd.DataFrame(results)\n",
                "results_df['tstr_ratio'] = results_df['roc_auc'] / real_result['roc_auc']\n",
                "\n",
                "print(\"\\n\" + \"=\" * 70)\n",
                "print(\"FINAL RESULTS\")\n",
                "print(\"=\" * 70)\n",
                "print(results_df.round(4).to_markdown(index=False))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save results\n",
                "results_df.to_csv('misata_v3_tstr_results.csv', index=False)\n",
                "\n",
                "perf_data = {\n",
                "    'name': 'MISATA v3 (Diffusion)',\n",
                "    'train_time': train_time,\n",
                "    'gen_time': gen_time,\n",
                "    'total_time': train_time + gen_time,\n",
                "    'rows': n_synthetic,\n",
                "    'rows_per_second': n_synthetic / gen_time\n",
                "}\n",
                "pd.DataFrame([perf_data]).to_csv('misata_v3_performance.csv', index=False)\n",
                "\n",
                "print(\"\\n\" + \"=\" * 70)\n",
                "print(\"EXPERIMENT COMPLETE\")\n",
                "print(\"=\" * 70)\n",
                "print(\"\\nFiles generated:\")\n",
                "print(\"  - misata_v3_training.png\")\n",
                "print(\"  - misata_v3_tstr_results.csv\")\n",
                "print(\"  - misata_v3_performance.csv\")\n",
                "print(f\"\\nTSTR Ratio: {results_df[results_df['name']=='MISATA v3 (Diffusion)']['tstr_ratio'].values[0]:.1%}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}