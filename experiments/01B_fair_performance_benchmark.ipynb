{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Experiment 1B: Fair Performance Benchmark\n",
                "\n",
                "## Critical Fix Applied\n",
                "**Issue**: Previous comparison was unfair - comparing CTGAN training+generation vs MISATA generation-only.\n",
                "\n",
                "**Fix**: Separate and report:\n",
                "1. **Fitting/Training Time** - Time to learn from data\n",
                "2. **Generation Time** - Time to generate N samples from fitted model\n",
                "3. **Total Time** - End-to-end time to go from data to synthetic output\n",
                "4. **Generation Throughput** - Rows/second for generation only\n",
                "\n",
                "This is the HONEST, BULLETPROOF benchmark."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q sdv faker numpy pandas scikit-learn matplotlib"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import time\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "# Reproducibility\n",
                "SEED = 42\n",
                "np.random.seed(SEED)\n",
                "\n",
                "print(\"Setup complete.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Adult Census\n",
                "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
                "columns = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status',\n",
                "           'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss',\n",
                "           'hours_per_week', 'native_country', 'income']\n",
                "\n",
                "df_raw = pd.read_csv(url, names=columns, na_values=' ?', skipinitialspace=True)\n",
                "df_raw = df_raw.dropna().reset_index(drop=True)\n",
                "\n",
                "# Use 10K rows for fair comparison\n",
                "df_sample = df_raw.sample(n=10000, random_state=SEED).reset_index(drop=True)\n",
                "\n",
                "print(f\"Dataset: {len(df_sample):,} rows, {len(df_sample.columns)} columns\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Benchmark Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def benchmark_with_timing(name, fit_fn, generate_fn, df, n_generate=10000, n_runs=3):\n",
                "    \"\"\"\n",
                "    Benchmark a synthetic data generator with SEPARATE timing.\n",
                "    \n",
                "    Returns:\n",
                "        dict with fit_time, gen_time, total_time, gen_throughput\n",
                "    \"\"\"\n",
                "    fit_times = []\n",
                "    gen_times = []\n",
                "    \n",
                "    for run in range(n_runs):\n",
                "        # Fit/Train timing\n",
                "        start = time.time()\n",
                "        model = fit_fn(df)\n",
                "        fit_time = time.time() - start\n",
                "        fit_times.append(fit_time)\n",
                "        \n",
                "        # Generate timing\n",
                "        start = time.time()\n",
                "        synthetic = generate_fn(model, n_generate)\n",
                "        gen_time = time.time() - start\n",
                "        gen_times.append(gen_time)\n",
                "    \n",
                "    avg_fit = np.mean(fit_times)\n",
                "    avg_gen = np.mean(gen_times)\n",
                "    std_fit = np.std(fit_times)\n",
                "    std_gen = np.std(gen_times)\n",
                "    \n",
                "    return {\n",
                "        'name': name,\n",
                "        'fit_time_mean': avg_fit,\n",
                "        'fit_time_std': std_fit,\n",
                "        'gen_time_mean': avg_gen,\n",
                "        'gen_time_std': std_gen,\n",
                "        'total_time': avg_fit + avg_gen,\n",
                "        'gen_throughput': n_generate / avg_gen,\n",
                "        'n_runs': n_runs,\n",
                "        'n_generate': n_generate\n",
                "    }\n",
                "\n",
                "print(\"Benchmark function defined.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Method 1: CTGAN"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sdv.single_table import CTGANSynthesizer\n",
                "from sdv.metadata import SingleTableMetadata\n",
                "\n",
                "# Prepare metadata\n",
                "metadata = SingleTableMetadata()\n",
                "metadata.detect_from_dataframe(df_sample)\n",
                "\n",
                "def fit_ctgan(df):\n",
                "    synthesizer = CTGANSynthesizer(metadata, epochs=10, verbose=False)  # Reduced epochs for speed\n",
                "    synthesizer.fit(df)\n",
                "    return synthesizer\n",
                "\n",
                "def generate_ctgan(model, n):\n",
                "    return model.sample(n)\n",
                "\n",
                "print(\"Benchmarking CTGAN (this may take a few minutes)...\")\n",
                "ctgan_results = benchmark_with_timing('CTGAN', fit_ctgan, generate_ctgan, df_sample, n_runs=2)\n",
                "print(f\"CTGAN: Fit={ctgan_results['fit_time_mean']:.1f}s, Gen={ctgan_results['gen_time_mean']:.1f}s\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Method 2: GaussianCopula"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sdv.single_table import GaussianCopulaSynthesizer\n",
                "\n",
                "def fit_copula(df):\n",
                "    synthesizer = GaussianCopulaSynthesizer(metadata)\n",
                "    synthesizer.fit(df)\n",
                "    return synthesizer\n",
                "\n",
                "def generate_copula(model, n):\n",
                "    return model.sample(n)\n",
                "\n",
                "print(\"Benchmarking GaussianCopula...\")\n",
                "copula_results = benchmark_with_timing('GaussianCopula', fit_copula, generate_copula, df_sample, n_runs=3)\n",
                "print(f\"Copula: Fit={copula_results['fit_time_mean']:.1f}s, Gen={copula_results['gen_time_mean']:.1f}s\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Method 3: MISATA-IPF"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from scipy import stats\n",
                "from sklearn.ensemble import GradientBoostingClassifier\n",
                "\n",
                "class MISATAIPFSynthesizer:\n",
                "    \"\"\"MISATA with IPF-guided synthesis.\"\"\"\n",
                "    \n",
                "    def __init__(self, target_col='income', random_state=42):\n",
                "        self.target_col = target_col\n",
                "        self.random_state = random_state\n",
                "        \n",
                "    def fit(self, df):\n",
                "        self.columns = list(df.columns)\n",
                "        self.train_data = df.copy()\n",
                "        \n",
                "        # Learn marginals\n",
                "        self.marginals = {}\n",
                "        for col in self.columns:\n",
                "            self.marginals[col] = {'all_values': df[col].values}\n",
                "        \n",
                "        # Learn correlation via copula\n",
                "        uniform_df = df.copy()\n",
                "        for col in self.columns:\n",
                "            uniform_df[col] = stats.rankdata(df[col]) / (len(df) + 1)\n",
                "        \n",
                "        normal_df = uniform_df.apply(lambda x: stats.norm.ppf(np.clip(x, 0.001, 0.999)))\n",
                "        corr_matrix = normal_df.corr().values\n",
                "        corr_matrix = np.nan_to_num(corr_matrix, nan=0.0)\n",
                "        np.fill_diagonal(corr_matrix, 1.0)\n",
                "        \n",
                "        eigvals, eigvecs = np.linalg.eigh(corr_matrix)\n",
                "        eigvals = np.maximum(eigvals, 1e-6)\n",
                "        corr_matrix = eigvecs @ np.diag(eigvals) @ eigvecs.T\n",
                "        \n",
                "        self.cholesky = np.linalg.cholesky(corr_matrix)\n",
                "        \n",
                "        # Causal model for target\n",
                "        if self.target_col in self.columns:\n",
                "            feature_cols = [c for c in self.columns if c != self.target_col]\n",
                "            # Encode categoricals\n",
                "            X = df[feature_cols].copy()\n",
                "            for col in X.columns:\n",
                "                if X[col].dtype == 'object':\n",
                "                    X[col] = LabelEncoder().fit_transform(X[col].astype(str))\n",
                "            \n",
                "            y = df[self.target_col]\n",
                "            if y.dtype == 'object':\n",
                "                y = LabelEncoder().fit_transform(y.astype(str))\n",
                "            \n",
                "            self.causal_model = GradientBoostingClassifier(\n",
                "                n_estimators=50, max_depth=4, random_state=self.random_state\n",
                "            )\n",
                "            self.causal_model.fit(X, y)\n",
                "            self.feature_cols = feature_cols\n",
                "            self.target_rate = y.mean()\n",
                "        \n",
                "        return self\n",
                "    \n",
                "    def sample(self, n_samples):\n",
                "        rng = np.random.default_rng(self.random_state)\n",
                "        \n",
                "        z = rng.standard_normal((n_samples, len(self.columns)))\n",
                "        correlated_z = z @ self.cholesky.T\n",
                "        uniform = stats.norm.cdf(correlated_z)\n",
                "        uniform = np.clip(uniform, 0.001, 0.999)\n",
                "        \n",
                "        synthetic_data = {}\n",
                "        for i, col in enumerate(self.columns):\n",
                "            if col == self.target_col:\n",
                "                continue\n",
                "            \n",
                "            sorted_vals = np.sort(self.marginals[col]['all_values'])\n",
                "            positions = np.linspace(0, 1, len(sorted_vals))\n",
                "            synthetic_data[col] = np.interp(uniform[:, i], positions, sorted_vals)\n",
                "        \n",
                "        # Generate target\n",
                "        if self.target_col in self.columns:\n",
                "            X_synth = pd.DataFrame({c: synthetic_data[c] for c in self.feature_cols})\n",
                "            for col in X_synth.columns:\n",
                "                if X_synth[col].dtype == 'object' or col in self.marginals:\n",
                "                    X_synth[col] = X_synth[col].round().astype(int)\n",
                "            \n",
                "            probs = self.causal_model.predict_proba(X_synth)[:, 1]\n",
                "            threshold = np.percentile(probs, (1 - self.target_rate) * 100)\n",
                "            synthetic_data[self.target_col] = (probs >= threshold).astype(int)\n",
                "        \n",
                "        return pd.DataFrame(synthetic_data)[self.columns]\n",
                "\n",
                "\n",
                "def fit_misata(df):\n",
                "    # Encode categoricals for MISATA\n",
                "    df_encoded = df.copy()\n",
                "    for col in df_encoded.columns:\n",
                "        if df_encoded[col].dtype == 'object':\n",
                "            df_encoded[col] = LabelEncoder().fit_transform(df_encoded[col].astype(str))\n",
                "    \n",
                "    synth = MISATAIPFSynthesizer(target_col='income', random_state=SEED)\n",
                "    synth.fit(df_encoded)\n",
                "    return synth\n",
                "\n",
                "def generate_misata(model, n):\n",
                "    return model.sample(n)\n",
                "\n",
                "print(\"Benchmarking MISATA-IPF...\")\n",
                "misata_results = benchmark_with_timing('MISATA-IPF', fit_misata, generate_misata, df_sample, n_runs=5)\n",
                "print(f\"MISATA: Fit={misata_results['fit_time_mean']:.2f}s, Gen={misata_results['gen_time_mean']:.3f}s\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Method 4: Faker (Baseline)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from faker import Faker\n",
                "\n",
                "def fit_faker(df):\n",
                "    # Faker doesn't really \"fit\" - just store schema\n",
                "    return {'columns': list(df.columns), 'n_rows': len(df)}\n",
                "\n",
                "def generate_faker(model, n):\n",
                "    fake = Faker()\n",
                "    Faker.seed(SEED)\n",
                "    \n",
                "    data = {\n",
                "        'age': [fake.random_int(17, 90) for _ in range(n)],\n",
                "        'workclass': [fake.random_element(['Private', 'Self-emp', 'Gov', 'Other']) for _ in range(n)],\n",
                "        'fnlwgt': [fake.random_int(10000, 1000000) for _ in range(n)],\n",
                "        'education': [fake.random_element(['HS-grad', 'Some-college', 'Bachelors', 'Masters']) for _ in range(n)],\n",
                "        'education_num': [fake.random_int(1, 16) for _ in range(n)],\n",
                "        'marital_status': [fake.random_element(['Married', 'Single', 'Divorced']) for _ in range(n)],\n",
                "        'occupation': [fake.job()[:20] for _ in range(n)],\n",
                "        'relationship': [fake.random_element(['Husband', 'Wife', 'Own-child', 'Other']) for _ in range(n)],\n",
                "        'race': [fake.random_element(['White', 'Black', 'Asian', 'Other']) for _ in range(n)],\n",
                "        'sex': [fake.random_element(['Male', 'Female']) for _ in range(n)],\n",
                "        'capital_gain': [fake.random_int(0, 100000) for _ in range(n)],\n",
                "        'capital_loss': [fake.random_int(0, 5000) for _ in range(n)],\n",
                "        'hours_per_week': [fake.random_int(1, 100) for _ in range(n)],\n",
                "        'native_country': [fake.country()[:20] for _ in range(n)],\n",
                "        'income': [fake.random_element(['<=50K', '>50K']) for _ in range(n)]\n",
                "    }\n",
                "    return pd.DataFrame(data)\n",
                "\n",
                "print(\"Benchmarking Faker...\")\n",
                "faker_results = benchmark_with_timing('Faker', fit_faker, generate_faker, df_sample, n_runs=3)\n",
                "print(f\"Faker: Fit={faker_results['fit_time_mean']:.4f}s, Gen={faker_results['gen_time_mean']:.2f}s\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Results Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compile results\n",
                "all_results = [ctgan_results, copula_results, misata_results, faker_results]\n",
                "results_df = pd.DataFrame(all_results)\n",
                "\n",
                "# Calculate speedups\n",
                "ctgan_total = ctgan_results['total_time']\n",
                "results_df['speedup_vs_ctgan'] = ctgan_total / results_df['total_time']\n",
                "\n",
                "print(\"=\" * 80)\n",
                "print(\"FAIR PERFORMANCE BENCHMARK - SEPARATED TIMINGS\")\n",
                "print(\"=\" * 80)\n",
                "print(f\"\\nDataset: Adult Census, {len(df_sample):,} rows\")\n",
                "print(f\"Generated: 10,000 samples\")\n",
                "print(f\"Runs: Multiple (with std)\")\n",
                "print()\n",
                "\n",
                "print(\"Detailed Results:\")\n",
                "print(\"-\" * 80)\n",
                "print(f\"{'Method':<20} {'Fit Time':<15} {'Gen Time':<15} {'Total':<12} {'Gen Throughput':<15} {'Speedup'}\")\n",
                "print(\"-\" * 80)\n",
                "\n",
                "for _, row in results_df.iterrows():\n",
                "    fit_str = f\"{row['fit_time_mean']:.2f}s ±{row['fit_time_std']:.2f}\"\n",
                "    gen_str = f\"{row['gen_time_mean']:.3f}s ±{row['gen_time_std']:.3f}\"\n",
                "    total_str = f\"{row['total_time']:.2f}s\"\n",
                "    throughput = f\"{row['gen_throughput']:,.0f}/s\"\n",
                "    speedup = f\"{row['speedup_vs_ctgan']:.1f}x\"\n",
                "    \n",
                "    print(f\"{row['name']:<20} {fit_str:<15} {gen_str:<15} {total_str:<12} {throughput:<15} {speedup}\")\n",
                "\n",
                "print(\"-\" * 80)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Visualization\n",
                "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
                "\n",
                "methods = results_df['name'].tolist()\n",
                "colors = ['#e74c3c', '#3498db', '#2ecc71', '#9b59b6']\n",
                "\n",
                "# Plot 1: Fit Time\n",
                "ax1 = axes[0]\n",
                "bars1 = ax1.bar(methods, results_df['fit_time_mean'], yerr=results_df['fit_time_std'], \n",
                "                color=colors, capsize=5, alpha=0.8)\n",
                "ax1.set_ylabel('Time (seconds)', fontsize=11)\n",
                "ax1.set_title('Fit/Training Time', fontsize=12, fontweight='bold')\n",
                "ax1.tick_params(axis='x', rotation=45)\n",
                "for bar, val in zip(bars1, results_df['fit_time_mean']):\n",
                "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
                "             f'{val:.1f}s', ha='center', fontsize=9)\n",
                "\n",
                "# Plot 2: Generation Time\n",
                "ax2 = axes[1]\n",
                "bars2 = ax2.bar(methods, results_df['gen_time_mean'], yerr=results_df['gen_time_std'],\n",
                "                color=colors, capsize=5, alpha=0.8)\n",
                "ax2.set_ylabel('Time (seconds)', fontsize=11)\n",
                "ax2.set_title('Generation Time (10K samples)', fontsize=12, fontweight='bold')\n",
                "ax2.tick_params(axis='x', rotation=45)\n",
                "for bar, val in zip(bars2, results_df['gen_time_mean']):\n",
                "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
                "             f'{val:.2f}s', ha='center', fontsize=9)\n",
                "\n",
                "# Plot 3: Total Time\n",
                "ax3 = axes[2]\n",
                "bars3 = ax3.bar(methods, results_df['total_time'], color=colors, alpha=0.8)\n",
                "ax3.set_ylabel('Time (seconds)', fontsize=11)\n",
                "ax3.set_title('Total Time (Fit + Generate)', fontsize=12, fontweight='bold')\n",
                "ax3.tick_params(axis='x', rotation=45)\n",
                "for bar, val, speedup in zip(bars3, results_df['total_time'], results_df['speedup_vs_ctgan']):\n",
                "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
                "             f'{val:.1f}s\\n({speedup:.0f}x)', ha='center', fontsize=9)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('fair_performance_benchmark.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(\"\\n✓ Saved fair_performance_benchmark.png\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save results\n",
                "results_df.to_csv('fair_performance_results.csv', index=False)\n",
                "\n",
                "print(\"\\n\" + \"=\" * 80)\n",
                "print(\"EXPERIMENT COMPLETE - FAIR COMPARISON\")\n",
                "print(\"=\" * 80)\n",
                "print(\"\\nKey Findings (HONEST):\")\n",
                "print(f\"  1. CTGAN takes {ctgan_results['fit_time_mean']:.0f}s to train, {ctgan_results['gen_time_mean']:.1f}s to generate\")\n",
                "print(f\"  2. MISATA takes {misata_results['fit_time_mean']:.1f}s to fit, {misata_results['gen_time_mean']:.3f}s to generate\")\n",
                "print(f\"  3. Total time speedup: {ctgan_total / misata_results['total_time']:.0f}x\")\n",
                "print(f\"  4. Generation-only speedup: {ctgan_results['gen_time_mean'] / misata_results['gen_time_mean']:.0f}x\")\n",
                "print(\"\\nThis is a FAIR comparison with all timings separated.\")\n",
                "print(\"\\nFiles saved:\")\n",
                "print(\"  - fair_performance_benchmark.png\")\n",
                "print(\"  - fair_performance_results.csv\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}