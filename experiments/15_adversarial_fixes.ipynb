{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Experiment 15: Adversarial Attack Resolution\n",
                "\n",
                "This experiment addresses multiple adversarial review attacks:\n",
                "\n",
                "| Attack | Issue | Fix |\n",
                "|--------|-------|-----|\n",
                "| 1 | Copula bias | Add Kendall's tau, Chi-square |\n",
                "| 3 | Only 5 seeds | Increase to 20 seeds |\n",
                "| 4 | Linear SCM trivial | Add nonlinear SCM |\n",
                "| 7 | Cherry-picked data | Add Cover Type dataset |\n",
                "| 8 | No privacy metrics | Add DCR, MIA |\n",
                "| 9 | Single TRTR model | Add XGBoost, LogReg |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q numpy pandas scikit-learn scipy matplotlib seaborn xgboost"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "from scipy import stats\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
                "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.neighbors import NearestNeighbors\n",
                "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n",
                "from sklearn.datasets import fetch_covtype\n",
                "import xgboost as xgb\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "SEED = 42\n",
                "np.random.seed(SEED)\n",
                "print(\"Setup complete.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## MISATA Synthesizer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MISATASynthesizer:\n",
                "    \"\"\"Universal MISATA synthesizer.\"\"\"\n",
                "    \n",
                "    def __init__(self, target_col=None, task='classification', random_state=42):\n",
                "        self.target_col = target_col\n",
                "        self.task = task\n",
                "        self.random_state = random_state\n",
                "        \n",
                "    def fit(self, df):\n",
                "        self.columns = list(df.columns)\n",
                "        self.marginals = {col: {'values': df[col].values.copy()} for col in self.columns}\n",
                "        \n",
                "        uniform_df = df.copy()\n",
                "        for col in self.columns:\n",
                "            uniform_df[col] = stats.rankdata(df[col]) / (len(df) + 1)\n",
                "        \n",
                "        normal_df = uniform_df.apply(lambda x: stats.norm.ppf(np.clip(x, 0.001, 0.999)))\n",
                "        corr_matrix = normal_df.corr().values\n",
                "        corr_matrix = np.nan_to_num(corr_matrix, nan=0.0)\n",
                "        np.fill_diagonal(corr_matrix, 1.0)\n",
                "        \n",
                "        eigvals, eigvecs = np.linalg.eigh(corr_matrix)\n",
                "        eigvals = np.maximum(eigvals, 1e-6)\n",
                "        corr_matrix = eigvecs @ np.diag(eigvals) @ eigvecs.T\n",
                "        \n",
                "        self.cholesky = np.linalg.cholesky(corr_matrix)\n",
                "        \n",
                "        if self.target_col and self.target_col in self.columns:\n",
                "            feature_cols = [c for c in self.columns if c != self.target_col]\n",
                "            if self.task == 'classification':\n",
                "                self.target_model = GradientBoostingClassifier(n_estimators=50, max_depth=4, random_state=self.random_state)\n",
                "            else:\n",
                "                from sklearn.ensemble import GradientBoostingRegressor\n",
                "                self.target_model = GradientBoostingRegressor(n_estimators=50, max_depth=4, random_state=self.random_state)\n",
                "            self.target_model.fit(df[feature_cols], df[self.target_col])\n",
                "            self.feature_cols = feature_cols\n",
                "            self.target_rate = df[self.target_col].mean() if self.task == 'classification' else None\n",
                "        return self\n",
                "    \n",
                "    def sample(self, n_samples, seed=None):\n",
                "        if seed is None:\n",
                "            seed = self.random_state\n",
                "        rng = np.random.default_rng(seed)\n",
                "        \n",
                "        z = rng.standard_normal((n_samples, len(self.columns)))\n",
                "        uniform = stats.norm.cdf(z @ self.cholesky.T)\n",
                "        uniform = np.clip(uniform, 0.001, 0.999)\n",
                "        \n",
                "        synthetic_data = {}\n",
                "        for i, col in enumerate(self.columns):\n",
                "            if col == self.target_col:\n",
                "                continue\n",
                "            sorted_vals = np.sort(self.marginals[col]['values'])\n",
                "            positions = np.linspace(0, 1, len(sorted_vals))\n",
                "            synthetic_data[col] = np.interp(uniform[:, i], positions, sorted_vals)\n",
                "        \n",
                "        if self.target_col and self.target_col in self.columns:\n",
                "            X_synth = pd.DataFrame({c: synthetic_data[c] for c in self.feature_cols})\n",
                "            if self.task == 'classification':\n",
                "                probs = self.target_model.predict_proba(X_synth)[:, 1]\n",
                "                threshold = np.percentile(probs, (1 - self.target_rate) * 100)\n",
                "                synthetic_data[self.target_col] = (probs >= threshold).astype(int)\n",
                "            else:\n",
                "                synthetic_data[self.target_col] = self.target_model.predict(X_synth)\n",
                "        \n",
                "        return pd.DataFrame(synthetic_data)[self.columns]\n",
                "\n",
                "print(\"Synthesizer defined.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Attack 1 Fix: Comprehensive Correlation Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def comprehensive_correlation_metrics(real_df, synth_df):\n",
                "    \"\"\"\n",
                "    Compute multiple correlation metrics beyond Pearson.\n",
                "    \"\"\"\n",
                "    metrics = {}\n",
                "    \n",
                "    # 1. Pearson (original)\n",
                "    real_corr = real_df.corr().values.flatten()\n",
                "    synth_corr = synth_df.corr().values.flatten()\n",
                "    mask = ~(np.isnan(real_corr) | np.isnan(synth_corr))\n",
                "    metrics['pearson_similarity'] = np.corrcoef(real_corr[mask], synth_corr[mask])[0, 1]\n",
                "    \n",
                "    # 2. Kendall's Tau (rank-based, better for ordinal)\n",
                "    kendall_similarities = []\n",
                "    for col in real_df.columns:\n",
                "        tau, _ = stats.kendalltau(real_df[col], synth_df[col].iloc[:len(real_df)])\n",
                "        if not np.isnan(tau):\n",
                "            kendall_similarities.append(tau)\n",
                "    metrics['kendall_tau_mean'] = np.mean(kendall_similarities)\n",
                "    \n",
                "    # 3. Spearman's Rho (rank correlation)\n",
                "    real_spearman = real_df.corr(method='spearman').values.flatten()\n",
                "    synth_spearman = synth_df.corr(method='spearman').values.flatten()\n",
                "    mask = ~(np.isnan(real_spearman) | np.isnan(synth_spearman))\n",
                "    metrics['spearman_similarity'] = np.corrcoef(real_spearman[mask], synth_spearman[mask])[0, 1]\n",
                "    \n",
                "    # 4. Tail Dependence (for financial data)\n",
                "    # Approximate via extreme value correlation\n",
                "    tail_corrs = []\n",
                "    for col in real_df.columns:\n",
                "        threshold = np.percentile(real_df[col], 95)\n",
                "        real_tail = real_df[col] > threshold\n",
                "        synth_tail = synth_df[col] > threshold\n",
                "        if real_tail.sum() > 5 and synth_tail.sum() > 5:\n",
                "            tail_corrs.append(np.abs(real_tail.mean() - synth_tail.mean()))\n",
                "    metrics['tail_preservation'] = 1 - np.mean(tail_corrs) if tail_corrs else 1.0\n",
                "    \n",
                "    return metrics\n",
                "\n",
                "\n",
                "# Test on Adult Census\n",
                "print(\"Loading Adult Census for Attack 1 fix...\")\n",
                "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
                "columns = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status',\n",
                "           'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss',\n",
                "           'hours_per_week', 'native_country', 'income']\n",
                "df_raw = pd.read_csv(url, names=columns, na_values=' ?', skipinitialspace=True)\n",
                "df_raw = df_raw.dropna().reset_index(drop=True).sample(5000, random_state=SEED)\n",
                "df_raw['income'] = (df_raw['income'] == '>50K').astype(int)\n",
                "for col in ['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country']:\n",
                "    df_raw[col] = LabelEncoder().fit_transform(df_raw[col].astype(str))\n",
                "\n",
                "synth = MISATASynthesizer(target_col='income', random_state=SEED)\n",
                "synth.fit(df_raw)\n",
                "df_synth = synth.sample(len(df_raw))\n",
                "\n",
                "corr_metrics = comprehensive_correlation_metrics(df_raw, df_synth)\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"ATTACK 1 FIX: Comprehensive Correlation Metrics\")\n",
                "print(\"=\"*60)\n",
                "for metric, value in corr_metrics.items():\n",
                "    print(f\"  {metric}: {value:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Attack 3 Fix: 20-Seed Validation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 20-seed validation\n",
                "n_seeds = 20\n",
                "seeds = list(range(42, 42 + n_seeds))\n",
                "\n",
                "fidelity_scores = []\n",
                "tstr_scores = []\n",
                "\n",
                "train_df, test_df = train_test_split(df_raw, test_size=0.2, random_state=42)\n",
                "\n",
                "print(f\"\\nRunning {n_seeds}-seed validation...\")\n",
                "for i, seed in enumerate(seeds):\n",
                "    synth = MISATASynthesizer(target_col='income', random_state=seed)\n",
                "    synth.fit(train_df)\n",
                "    df_synth = synth.sample(len(train_df), seed=seed)\n",
                "    \n",
                "    # Fidelity\n",
                "    ks_scores = [1 - stats.ks_2samp(train_df[col], df_synth[col])[0] for col in train_df.columns]\n",
                "    fidelity_scores.append(np.mean(ks_scores))\n",
                "    \n",
                "    # TSTR\n",
                "    X_synth = df_synth.drop('income', axis=1)\n",
                "    y_synth = df_synth['income']\n",
                "    X_test = test_df.drop('income', axis=1)\n",
                "    y_test = test_df['income']\n",
                "    \n",
                "    model = RandomForestClassifier(n_estimators=50, random_state=seed, n_jobs=-1)\n",
                "    model.fit(X_synth, y_synth)\n",
                "    tstr_scores.append(roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]))\n",
                "    \n",
                "    if (i+1) % 5 == 0:\n",
                "        print(f\"  Completed {i+1}/{n_seeds} seeds\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"ATTACK 3 FIX: 20-Seed Validation\")\n",
                "print(\"=\"*60)\n",
                "print(f\"  Fidelity: {np.mean(fidelity_scores):.4f} ± {np.std(fidelity_scores):.4f}\")\n",
                "print(f\"  TSTR AUC: {np.mean(tstr_scores):.4f} ± {np.std(tstr_scores):.4f}\")\n",
                "print(f\"  95% CI Fidelity: [{np.percentile(fidelity_scores, 2.5):.4f}, {np.percentile(fidelity_scores, 97.5):.4f}]\")\n",
                "print(f\"  95% CI TSTR: [{np.percentile(tstr_scores, 2.5):.4f}, {np.percentile(tstr_scores, 97.5):.4f}]\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Attack 4 Fix: Nonlinear SCM Validation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class NonlinearSCM:\n",
                "    \"\"\"\n",
                "    Nonlinear Structural Causal Model.\n",
                "    \n",
                "    DAG with NONLINEAR relationships:\n",
                "        X1 → X2 (exponential)\n",
                "        X1 → X3 (quadratic)\n",
                "        X2, X3 → Y (interaction)\n",
                "    \"\"\"\n",
                "    \n",
                "    @classmethod\n",
                "    def generate(cls, n_samples, seed=42):\n",
                "        rng = np.random.default_rng(seed)\n",
                "        \n",
                "        # Exogenous\n",
                "        X1 = rng.normal(0, 1, n_samples)\n",
                "        \n",
                "        # Nonlinear relationships\n",
                "        X2 = np.exp(0.3 * X1) + rng.normal(0, 0.5, n_samples)\n",
                "        X3 = 0.5 * X1**2 + rng.normal(0, 0.5, n_samples)\n",
                "        \n",
                "        # Interaction term\n",
                "        Y = np.tanh(X2 * X3) + 0.3 * X1 + rng.normal(0, 0.3, n_samples)\n",
                "        \n",
                "        return pd.DataFrame({'X1': X1, 'X2': X2, 'X3': X3, 'Y': Y})\n",
                "    \n",
                "    @classmethod\n",
                "    def intervene(cls, n_samples, intervention_var, value, seed=42):\n",
                "        rng = np.random.default_rng(seed)\n",
                "        \n",
                "        if intervention_var == 'X1':\n",
                "            X1 = np.full(n_samples, value)\n",
                "        else:\n",
                "            X1 = rng.normal(0, 1, n_samples)\n",
                "            \n",
                "        if intervention_var == 'X2':\n",
                "            X2 = np.full(n_samples, value)\n",
                "        else:\n",
                "            X2 = np.exp(0.3 * X1) + rng.normal(0, 0.5, n_samples)\n",
                "            \n",
                "        if intervention_var == 'X3':\n",
                "            X3 = np.full(n_samples, value)\n",
                "        else:\n",
                "            X3 = 0.5 * X1**2 + rng.normal(0, 0.5, n_samples)\n",
                "        \n",
                "        Y = np.tanh(X2 * X3) + 0.3 * X1 + rng.normal(0, 0.3, n_samples)\n",
                "        \n",
                "        return pd.DataFrame({'X1': X1, 'X2': X2, 'X3': X3, 'Y': Y})\n",
                "\n",
                "\n",
                "# Generate data from nonlinear SCM\n",
                "print(\"\\nGenerating nonlinear SCM data...\")\n",
                "df_nonlinear = NonlinearSCM.generate(5000, seed=SEED)\n",
                "\n",
                "# Fit MISATA\n",
                "synth_nl = MISATASynthesizer(target_col='Y', task='regression', random_state=SEED)\n",
                "synth_nl.fit(df_nonlinear)\n",
                "\n",
                "# Test interventions\n",
                "intervention_values = [-1, 0, 1, 2]\n",
                "nl_results = []\n",
                "\n",
                "df_baseline_true = NonlinearSCM.generate(3000, seed=100)\n",
                "df_baseline_misata = synth_nl.sample(3000, seed=100)\n",
                "\n",
                "for val in intervention_values:\n",
                "    # True effect\n",
                "    df_true = NonlinearSCM.intervene(3000, 'X1', val, seed=100)\n",
                "    true_effect = df_true['Y'].mean() - df_baseline_true['Y'].mean()\n",
                "    \n",
                "    # MISATA effect (approximate via conditioning)\n",
                "    # For intervention, regenerate with X1 fixed\n",
                "    df_misata_int = synth_nl.sample(3000, seed=100)\n",
                "    # Approximate do(X1=val) by filtering samples near val\n",
                "    mask = np.abs(df_misata_int['X1'] - val) < 0.5\n",
                "    if mask.sum() > 100:\n",
                "        misata_effect = df_misata_int.loc[mask, 'Y'].mean() - df_baseline_misata['Y'].mean()\n",
                "    else:\n",
                "        misata_effect = np.nan\n",
                "    \n",
                "    nl_results.append({\n",
                "        'intervention': f'do(X1={val})',\n",
                "        'true_effect': true_effect,\n",
                "        'misata_effect': misata_effect\n",
                "    })\n",
                "\n",
                "nl_df = pd.DataFrame(nl_results)\n",
                "nl_df = nl_df.dropna()\n",
                "\n",
                "if len(nl_df) > 1:\n",
                "    nl_correlation = np.corrcoef(nl_df['true_effect'], nl_df['misata_effect'])[0, 1]\n",
                "else:\n",
                "    nl_correlation = np.nan\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"ATTACK 4 FIX: Nonlinear SCM Validation\")\n",
                "print(\"=\"*60)\n",
                "print(nl_df.to_string(index=False))\n",
                "print(f\"\\n  Nonlinear Effect Recovery: r = {nl_correlation:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Attack 7 Fix: Cover Type Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Cover Type (54 features)\n",
                "print(\"\\nLoading Cover Type dataset (54 features)...\")\n",
                "covtype = fetch_covtype()\n",
                "df_cover = pd.DataFrame(covtype.data, columns=[f'f{i}' for i in range(54)])\n",
                "df_cover['target'] = covtype.target\n",
                "\n",
                "# Use subset\n",
                "df_cover = df_cover.sample(10000, random_state=SEED).reset_index(drop=True)\n",
                "\n",
                "# Binary classification: forest type 1 vs rest\n",
                "df_cover['target'] = (df_cover['target'] == 1).astype(int)\n",
                "\n",
                "print(f\"  Shape: {df_cover.shape}\")\n",
                "print(f\"  Target distribution: {df_cover['target'].mean():.2%} positive\")\n",
                "\n",
                "# Split\n",
                "train_cover, test_cover = train_test_split(df_cover, test_size=0.2, random_state=SEED)\n",
                "\n",
                "# Fit MISATA\n",
                "synth_cover = MISATASynthesizer(target_col='target', task='classification', random_state=SEED)\n",
                "synth_cover.fit(train_cover)\n",
                "df_synth_cover = synth_cover.sample(len(train_cover))\n",
                "\n",
                "# Evaluate\n",
                "# Fidelity\n",
                "ks_scores = [1 - stats.ks_2samp(train_cover[col], df_synth_cover[col])[0] for col in train_cover.columns]\n",
                "cover_fidelity = np.mean(ks_scores)\n",
                "\n",
                "# TSTR\n",
                "X_synth = df_synth_cover.drop('target', axis=1)\n",
                "y_synth = df_synth_cover['target']\n",
                "X_test = test_cover.drop('target', axis=1)\n",
                "y_test = test_cover['target']\n",
                "\n",
                "model = RandomForestClassifier(n_estimators=100, random_state=SEED, n_jobs=-1)\n",
                "model.fit(X_synth, y_synth)\n",
                "cover_tstr = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
                "\n",
                "# TRTR baseline\n",
                "X_train = train_cover.drop('target', axis=1)\n",
                "y_train = train_cover['target']\n",
                "model_real = RandomForestClassifier(n_estimators=100, random_state=SEED, n_jobs=-1)\n",
                "model_real.fit(X_train, y_train)\n",
                "cover_trtr = roc_auc_score(y_test, model_real.predict_proba(X_test)[:, 1])\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"ATTACK 7 FIX: Cover Type Dataset (54 features)\")\n",
                "print(\"=\"*60)\n",
                "print(f\"  Marginal Fidelity: {cover_fidelity:.4f}\")\n",
                "print(f\"  TRTR AUC: {cover_trtr:.4f}\")\n",
                "print(f\"  TSTR AUC: {cover_tstr:.4f}\")\n",
                "print(f\"  TSTR Ratio: {cover_tstr/cover_trtr:.2%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Attack 8 Fix: Privacy Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_privacy_metrics(real_df, synth_df, n_neighbors=5):\n",
                "    \"\"\"\n",
                "    Compute privacy metrics:\n",
                "    1. DCR (Distance to Closest Record)\n",
                "    2. NNDR (Nearest Neighbor Distance Ratio)\n",
                "    3. Simple Membership Inference Attack\n",
                "    \"\"\"\n",
                "    metrics = {}\n",
                "    \n",
                "    # Normalize for distance computation\n",
                "    scaler = StandardScaler()\n",
                "    real_scaled = scaler.fit_transform(real_df)\n",
                "    synth_scaled = scaler.transform(synth_df)\n",
                "    \n",
                "    # 1. Distance to Closest Record (DCR)\n",
                "    # For each synthetic record, find distance to nearest real record\n",
                "    nn = NearestNeighbors(n_neighbors=1, algorithm='ball_tree')\n",
                "    nn.fit(real_scaled)\n",
                "    distances, _ = nn.kneighbors(synth_scaled)\n",
                "    \n",
                "    metrics['dcr_mean'] = np.mean(distances)\n",
                "    metrics['dcr_std'] = np.std(distances)\n",
                "    metrics['dcr_min'] = np.min(distances)  # Risk indicator\n",
                "    metrics['dcr_5th_percentile'] = np.percentile(distances, 5)\n",
                "    \n",
                "    # 2. NNDR (ratio of closest to second-closest)\n",
                "    # Higher = more diverse synthetic data\n",
                "    nn2 = NearestNeighbors(n_neighbors=2, algorithm='ball_tree')\n",
                "    nn2.fit(real_scaled)\n",
                "    distances2, _ = nn2.kneighbors(synth_scaled)\n",
                "    nndr = distances2[:, 0] / (distances2[:, 1] + 1e-10)\n",
                "    metrics['nndr_mean'] = np.mean(nndr)\n",
                "    \n",
                "    # 3. Membership Inference Attack (MIA)\n",
                "    # Train classifier to distinguish real (in training) vs synthetic\n",
                "    n_test = min(1000, len(real_df), len(synth_df))\n",
                "    \n",
                "    # Create holdout set from real data\n",
                "    real_sample = real_df.sample(n_test, random_state=42)\n",
                "    synth_sample = synth_df.sample(n_test, random_state=42)\n",
                "    \n",
                "    # Label: 1 = real, 0 = synthetic\n",
                "    X_mia = pd.concat([real_sample, synth_sample], ignore_index=True)\n",
                "    y_mia = np.array([1] * n_test + [0] * n_test)\n",
                "    \n",
                "    X_train, X_test_mia, y_train, y_test_mia = train_test_split(\n",
                "        X_mia, y_mia, test_size=0.3, random_state=42\n",
                "    )\n",
                "    \n",
                "    mia_model = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
                "    mia_model.fit(X_train, y_train)\n",
                "    \n",
                "    mia_auc = roc_auc_score(y_test_mia, mia_model.predict_proba(X_test_mia)[:, 1])\n",
                "    metrics['mia_auc'] = mia_auc\n",
                "    metrics['mia_advantage'] = 2 * (mia_auc - 0.5)  # 0 = perfect privacy, 1 = no privacy\n",
                "    \n",
                "    return metrics\n",
                "\n",
                "\n",
                "# Compute privacy metrics on Adult dataset\n",
                "print(\"\\nComputing privacy metrics...\")\n",
                "privacy_metrics = compute_privacy_metrics(train_df, df_synth)\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"ATTACK 8 FIX: Privacy Metrics\")\n",
                "print(\"=\"*60)\n",
                "print(f\"  DCR Mean: {privacy_metrics['dcr_mean']:.4f}\")\n",
                "print(f\"  DCR 5th Percentile: {privacy_metrics['dcr_5th_percentile']:.4f}\")\n",
                "print(f\"  NNDR Mean: {privacy_metrics['nndr_mean']:.4f}\")\n",
                "print(f\"  MIA AUC: {privacy_metrics['mia_auc']:.4f}\")\n",
                "print(f\"  MIA Advantage: {privacy_metrics['mia_advantage']:.4f}\")\n",
                "print(f\"\\n  Interpretation:\")\n",
                "if privacy_metrics['dcr_5th_percentile'] < 0.5:\n",
                "    print(f\"    ⚠ LOW DCR: Some synthetic records are close to real records\")\n",
                "else:\n",
                "    print(f\"    ✓ Good DCR: Synthetic records are sufficiently distant\")\n",
                "if privacy_metrics['mia_auc'] > 0.7:\n",
                "    print(f\"    ⚠ HIGH MIA: Synthetic data is distinguishable from real\")\n",
                "else:\n",
                "    print(f\"    ✓ Good MIA: Synthetic data is hard to distinguish\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Attack 9 Fix: Multi-Model TSTR"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Multi-model TSTR evaluation\n",
                "print(\"\\nRunning multi-model TSTR evaluation...\")\n",
                "\n",
                "models = {\n",
                "    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=SEED, n_jobs=-1),\n",
                "    'XGBoost': xgb.XGBClassifier(n_estimators=100, random_state=SEED, use_label_encoder=False, eval_metric='logloss'),\n",
                "    'LogisticRegression': LogisticRegression(max_iter=500, random_state=SEED),\n",
                "    'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=SEED)\n",
                "}\n",
                "\n",
                "multi_model_results = []\n",
                "\n",
                "X_train_real = train_df.drop('income', axis=1)\n",
                "y_train_real = train_df['income']\n",
                "X_synth_mm = df_synth.drop('income', axis=1)\n",
                "y_synth_mm = df_synth['income']\n",
                "X_test_mm = test_df.drop('income', axis=1)\n",
                "y_test_mm = test_df['income']\n",
                "\n",
                "for name, model in models.items():\n",
                "    # TRTR\n",
                "    model_real = model.__class__(**model.get_params())\n",
                "    model_real.fit(X_train_real, y_train_real)\n",
                "    trtr = roc_auc_score(y_test_mm, model_real.predict_proba(X_test_mm)[:, 1])\n",
                "    \n",
                "    # TSTR\n",
                "    model_synth = model.__class__(**model.get_params())\n",
                "    model_synth.fit(X_synth_mm, y_synth_mm)\n",
                "    tstr = roc_auc_score(y_test_mm, model_synth.predict_proba(X_test_mm)[:, 1])\n",
                "    \n",
                "    multi_model_results.append({\n",
                "        'model': name,\n",
                "        'trtr': trtr,\n",
                "        'tstr': tstr,\n",
                "        'ratio': tstr / trtr\n",
                "    })\n",
                "    print(f\"  {name}: TRTR={trtr:.4f}, TSTR={tstr:.4f}, Ratio={tstr/trtr:.2%}\")\n",
                "\n",
                "mm_df = pd.DataFrame(multi_model_results)\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"ATTACK 9 FIX: Multi-Model TSTR\")\n",
                "print(\"=\"*60)\n",
                "print(mm_df.to_string(index=False))\n",
                "print(f\"\\n  Overall Mean TSTR Ratio: {mm_df['ratio'].mean():.2%} ± {mm_df['ratio'].std():.2%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Save All Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compile all results\n",
                "all_results = {\n",
                "    # Attack 1\n",
                "    'pearson_similarity': corr_metrics['pearson_similarity'],\n",
                "    'kendall_tau': corr_metrics['kendall_tau_mean'],\n",
                "    'spearman_similarity': corr_metrics['spearman_similarity'],\n",
                "    'tail_preservation': corr_metrics['tail_preservation'],\n",
                "    \n",
                "    # Attack 3\n",
                "    'fidelity_20seed_mean': np.mean(fidelity_scores),\n",
                "    'fidelity_20seed_std': np.std(fidelity_scores),\n",
                "    'tstr_20seed_mean': np.mean(tstr_scores),\n",
                "    'tstr_20seed_std': np.std(tstr_scores),\n",
                "    \n",
                "    # Attack 4\n",
                "    'nonlinear_scm_correlation': nl_correlation,\n",
                "    \n",
                "    # Attack 7\n",
                "    'covtype_fidelity': cover_fidelity,\n",
                "    'covtype_tstr_ratio': cover_tstr / cover_trtr,\n",
                "    \n",
                "    # Attack 8\n",
                "    'dcr_mean': privacy_metrics['dcr_mean'],\n",
                "    'dcr_5th': privacy_metrics['dcr_5th_percentile'],\n",
                "    'mia_auc': privacy_metrics['mia_auc'],\n",
                "    'mia_advantage': privacy_metrics['mia_advantage'],\n",
                "    \n",
                "    # Attack 9\n",
                "    'multi_model_tstr_mean': mm_df['ratio'].mean(),\n",
                "    'multi_model_tstr_std': mm_df['ratio'].std()\n",
                "}\n",
                "\n",
                "pd.DataFrame([all_results]).to_csv('adversarial_fixes_results.csv', index=False)\n",
                "\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"ADVERSARIAL ATTACK FIXES COMPLETE\")\n",
                "print(\"=\"*70)\n",
                "print(\"\\nSummary:\")\n",
                "print(f\"  Attack 1 (Correlation): Kendall τ = {corr_metrics['kendall_tau_mean']:.4f}\")\n",
                "print(f\"  Attack 3 (Seeds): 20-seed TSTR = {np.mean(tstr_scores):.4f} ± {np.std(tstr_scores):.4f}\")\n",
                "print(f\"  Attack 4 (Nonlinear): Effect recovery r = {nl_correlation:.4f}\")\n",
                "print(f\"  Attack 7 (Datasets): Cover Type TSTR = {cover_tstr/cover_trtr:.2%}\")\n",
                "print(f\"  Attack 8 (Privacy): MIA advantage = {privacy_metrics['mia_advantage']:.4f}\")\n",
                "print(f\"  Attack 9 (Models): Multi-model TSTR = {mm_df['ratio'].mean():.2%}\")\n",
                "print(\"\\nFile saved: adversarial_fixes_results.csv\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}