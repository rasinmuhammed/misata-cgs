{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Experiment 23: Additional Baselines (TVAE, Higher-Order Tests)\n",
                "\n",
                "Addresses reviewer concerns:\n",
                "1. Missing TVAE baseline\n",
                "2. Higher-order dependency tests\n",
                "3. Heavy-tail distribution analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q sdv"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import time\n",
                "from scipy import stats\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
                "from sklearn.metrics import roc_auc_score\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "SEED = 42\n",
                "np.random.seed(SEED)\n",
                "print(\"Setup complete.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Adult Census\n",
                "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
                "columns = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status',\n",
                "           'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss',\n",
                "           'hours_per_week', 'native_country', 'income']\n",
                "df_raw = pd.read_csv(url, names=columns, na_values=' ?', skipinitialspace=True)\n",
                "df_raw = df_raw.dropna().reset_index(drop=True).sample(5000, random_state=SEED)\n",
                "df_raw['income'] = (df_raw['income'] == '>50K').astype(int)\n",
                "\n",
                "for col in ['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country']:\n",
                "    df_raw[col] = LabelEncoder().fit_transform(df_raw[col].astype(str))\n",
                "\n",
                "train_df, test_df = train_test_split(df_raw, test_size=0.2, random_state=SEED)\n",
                "print(f\"Train: {len(train_df)}, Test: {len(test_df)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 1: TVAE Baseline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sdv.single_table import TVAESynthesizer, CTGANSynthesizer, GaussianCopulaSynthesizer\n",
                "from sdv.metadata import SingleTableMetadata\n",
                "\n",
                "metadata = SingleTableMetadata()\n",
                "metadata.detect_from_dataframe(train_df)\n",
                "\n",
                "print(\"SDV models ready.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# MISATA\n",
                "class MISATASynthesizer:\n",
                "    def __init__(self, target_col='income', random_state=42):\n",
                "        self.target_col = target_col\n",
                "        self.random_state = random_state\n",
                "        \n",
                "    def fit(self, df):\n",
                "        self.columns = list(df.columns)\n",
                "        self.marginals = {col: {'values': df[col].values.copy()} for col in self.columns}\n",
                "        uniform_df = df.copy()\n",
                "        for col in self.columns:\n",
                "            uniform_df[col] = stats.rankdata(df[col]) / (len(df) + 1)\n",
                "        normal_df = uniform_df.apply(lambda x: stats.norm.ppf(np.clip(x, 0.001, 0.999)))\n",
                "        corr_matrix = normal_df.corr().values\n",
                "        corr_matrix = np.nan_to_num(corr_matrix, nan=0.0)\n",
                "        np.fill_diagonal(corr_matrix, 1.0)\n",
                "        eigvals, eigvecs = np.linalg.eigh(corr_matrix)\n",
                "        eigvals = np.maximum(eigvals, 1e-6)\n",
                "        corr_matrix = eigvecs @ np.diag(eigvals) @ eigvecs.T\n",
                "        self.cholesky = np.linalg.cholesky(corr_matrix)\n",
                "        feature_cols = [c for c in self.columns if c != self.target_col]\n",
                "        self.target_model = GradientBoostingClassifier(n_estimators=50, max_depth=4, random_state=self.random_state)\n",
                "        self.target_model.fit(df[feature_cols], df[self.target_col])\n",
                "        self.feature_cols = feature_cols\n",
                "        self.target_rate = df[self.target_col].mean()\n",
                "        return self\n",
                "    \n",
                "    def sample(self, n_samples):\n",
                "        rng = np.random.default_rng(self.random_state)\n",
                "        z = rng.standard_normal((n_samples, len(self.columns)))\n",
                "        uniform = stats.norm.cdf(z @ self.cholesky.T)\n",
                "        uniform = np.clip(uniform, 0.001, 0.999)\n",
                "        synthetic_data = {}\n",
                "        for i, col in enumerate(self.columns):\n",
                "            if col == self.target_col:\n",
                "                continue\n",
                "            sorted_vals = np.sort(self.marginals[col]['values'])\n",
                "            positions = np.linspace(0, 1, len(sorted_vals))\n",
                "            synthetic_data[col] = np.interp(uniform[:, i], positions, sorted_vals)\n",
                "        X_synth = pd.DataFrame({c: synthetic_data[c] for c in self.feature_cols})\n",
                "        probs = self.target_model.predict_proba(X_synth)[:, 1]\n",
                "        threshold = np.percentile(probs, (1 - self.target_rate) * 100)\n",
                "        synthetic_data[self.target_col] = (probs >= threshold).astype(int)\n",
                "        return pd.DataFrame(synthetic_data)[self.columns]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Benchmark all methods\n",
                "results = []\n",
                "\n",
                "# MISATA\n",
                "print(\"MISATA...\")\n",
                "start = time.time()\n",
                "misata = MISATASynthesizer()\n",
                "misata.fit(train_df)\n",
                "df_misata = misata.sample(len(train_df))\n",
                "misata_time = time.time() - start\n",
                "\n",
                "# TVAE\n",
                "print(\"TVAE...\")\n",
                "start = time.time()\n",
                "tvae = TVAESynthesizer(metadata, epochs=100)\n",
                "tvae.fit(train_df)\n",
                "df_tvae = tvae.sample(len(train_df))\n",
                "tvae_time = time.time() - start\n",
                "\n",
                "# GaussianCopula\n",
                "print(\"GaussianCopula...\")\n",
                "start = time.time()\n",
                "gc = GaussianCopulaSynthesizer(metadata)\n",
                "gc.fit(train_df)\n",
                "df_gc = gc.sample(len(train_df))\n",
                "gc_time = time.time() - start\n",
                "\n",
                "print(\"All methods complete.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate\n",
                "def evaluate(syn_df, name, total_time):\n",
                "    model = RandomForestClassifier(n_estimators=100, random_state=SEED, n_jobs=-1)\n",
                "    model.fit(syn_df.drop('income', axis=1), syn_df['income'])\n",
                "    tstr = roc_auc_score(test_df['income'], model.predict_proba(test_df.drop('income', axis=1))[:, 1])\n",
                "    \n",
                "    # Marginal fidelity\n",
                "    ks_scores = [1 - stats.ks_2samp(train_df[col], syn_df[col])[0] for col in train_df.columns]\n",
                "    fidelity = np.mean(ks_scores)\n",
                "    \n",
                "    return {'method': name, 'time': total_time, 'tstr': tstr, 'fidelity': fidelity}\n",
                "\n",
                "# TRTR\n",
                "model_real = RandomForestClassifier(n_estimators=100, random_state=SEED, n_jobs=-1)\n",
                "model_real.fit(train_df.drop('income', axis=1), train_df['income'])\n",
                "trtr = roc_auc_score(test_df['income'], model_real.predict_proba(test_df.drop('income', axis=1))[:, 1])\n",
                "\n",
                "results = [\n",
                "    evaluate(df_misata, 'MISATA', misata_time),\n",
                "    evaluate(df_tvae, 'TVAE', tvae_time),\n",
                "    evaluate(df_gc, 'GaussianCopula', gc_time)\n",
                "]\n",
                "\n",
                "for r in results:\n",
                "    r['tstr_ratio'] = r['tstr'] / trtr"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"BASELINE COMPARISON (incl. TVAE)\")\n",
                "print(\"=\"*60)\n",
                "print(f\"\\nTRTR: {trtr:.4f}\\n\")\n",
                "\n",
                "results_df = pd.DataFrame(results)\n",
                "print(results_df.to_string(index=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 2: Higher-Order Dependency Tests"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def higher_order_test(real_df, syn_df, cols):\n",
                "    \"\"\"\n",
                "    Test 3-way interaction preservation.\n",
                "    Compare P(c | a, b) in real vs synthetic.\n",
                "    \"\"\"\n",
                "    a, b, c = cols\n",
                "    \n",
                "    # Bin continuous variables\n",
                "    real_df = real_df.copy()\n",
                "    syn_df = syn_df.copy()\n",
                "    \n",
                "    for df in [real_df, syn_df]:\n",
                "        for col in [a, b, c]:\n",
                "            if df[col].nunique() > 10:\n",
                "                df[col + '_bin'] = pd.qcut(df[col], 5, labels=False, duplicates='drop')\n",
                "            else:\n",
                "                df[col + '_bin'] = df[col]\n",
                "    \n",
                "    # Compute joint distributions\n",
                "    real_joint = real_df.groupby([a + '_bin', b + '_bin', c + '_bin']).size() / len(real_df)\n",
                "    syn_joint = syn_df.groupby([a + '_bin', b + '_bin', c + '_bin']).size() / len(syn_df)\n",
                "    \n",
                "    # Align indices\n",
                "    all_idx = real_joint.index.union(syn_joint.index)\n",
                "    real_joint = real_joint.reindex(all_idx, fill_value=0)\n",
                "    syn_joint = syn_joint.reindex(all_idx, fill_value=0)\n",
                "    \n",
                "    # Total variation distance\n",
                "    tvd = np.abs(real_joint - syn_joint).sum() / 2\n",
                "    \n",
                "    return 1 - tvd  # Higher = better\n",
                "\n",
                "# Test 3-way interactions\n",
                "triplets = [\n",
                "    ['age', 'education_num', 'income'],\n",
                "    ['hours_per_week', 'capital_gain', 'income'],\n",
                "    ['age', 'sex', 'income']\n",
                "]\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"HIGHER-ORDER DEPENDENCY PRESERVATION\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "for triplet in triplets:\n",
                "    misata_score = higher_order_test(train_df, df_misata, triplet)\n",
                "    tvae_score = higher_order_test(train_df, df_tvae, triplet)\n",
                "    gc_score = higher_order_test(train_df, df_gc, triplet)\n",
                "    \n",
                "    print(f\"\\n{triplet}:\")\n",
                "    print(f\"  MISATA: {misata_score:.3f}\")\n",
                "    print(f\"  TVAE: {tvae_score:.3f}\")\n",
                "    print(f\"  GaussianCopula: {gc_score:.3f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 3: Tail Distribution Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def tail_preservation(real_df, syn_df, col, percentile=95):\n",
                "    \"\"\"Test if extreme values are preserved.\"\"\"\n",
                "    real_upper = np.percentile(real_df[col], percentile)\n",
                "    syn_upper = np.percentile(syn_df[col], percentile)\n",
                "    \n",
                "    real_lower = np.percentile(real_df[col], 100 - percentile)\n",
                "    syn_lower = np.percentile(syn_df[col], 100 - percentile)\n",
                "    \n",
                "    upper_ratio = min(syn_upper, real_upper) / max(syn_upper, real_upper)\n",
                "    lower_ratio = min(syn_lower, real_lower) / max(syn_lower, real_lower) if min(syn_lower, real_lower) > 0 else 0\n",
                "    \n",
                "    return (upper_ratio + lower_ratio) / 2 if lower_ratio > 0 else upper_ratio\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"TAIL DISTRIBUTION PRESERVATION (95th percentile)\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "tail_cols = ['age', 'capital_gain', 'hours_per_week']\n",
                "\n",
                "for col in tail_cols:\n",
                "    misata_tail = tail_preservation(train_df, df_misata, col)\n",
                "    tvae_tail = tail_preservation(train_df, df_tvae, col)\n",
                "    gc_tail = tail_preservation(train_df, df_gc, col)\n",
                "    \n",
                "    print(f\"\\n{col}:\")\n",
                "    print(f\"  MISATA: {misata_tail:.3f}\")\n",
                "    print(f\"  TVAE: {tvae_tail:.3f}\")\n",
                "    print(f\"  GaussianCopula: {gc_tail:.3f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save results\n",
                "results_df.to_csv('additional_baselines_results.csv', index=False)\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"EXPERIMENT 23 COMPLETE\")\n",
                "print(\"=\"*60)\n",
                "print(\"\\nKey Findings:\")\n",
                "print(\"  - MISATA vs TVAE: Direct comparison available\")\n",
                "print(\"  - Higher-order dependencies tested\")\n",
                "print(\"  - Tail preservation analyzed\")\n",
                "print(\"\\nFile saved: additional_baselines_results.csv\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}