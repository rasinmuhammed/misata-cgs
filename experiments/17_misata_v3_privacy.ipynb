{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Experiment 17: MISATA v3 - Privacy Fixed\n",
                "\n",
                "## Issue Analysis\n",
                "MIA AUC = 1.0 with noise meant the noise created EASILY DETECTABLE artifacts:\n",
                "- Out-of-range values\n",
                "- Unusual noise patterns\n",
                "- Breaks natural data correlations\n",
                "\n",
                "## v3 Solution: Smarter Privacy\n",
                "1. **Resample instead of noise**: Add diversity via bootstrap, not Gaussian noise\n",
                "2. **Manifold-preserving augmentation**: Stay within data support\n",
                "3. **Mixup-style interpolation**: Combine samples for diversity\n",
                "4. **K-anonymity inspired**: Ensure each synthetic record is near K real records"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "from scipy import stats\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
                "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
                "from sklearn.neighbors import NearestNeighbors\n",
                "from sklearn.metrics import roc_auc_score\n",
                "import matplotlib.pyplot as plt\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "SEED = 42\n",
                "np.random.seed(SEED)\n",
                "print(\"Setup complete.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## MISATA v3: Privacy-Preserving Synthesizer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MISATAv3Synthesizer:\n",
                "    \"\"\"\n",
                "    MISATA v3 with PROPER privacy:\n",
                "    \n",
                "    Key insight: Don't add noise, add DIVERSITY through:\n",
                "    1. Interpolation between samples (mixup)\n",
                "    2. Staying within natural data bounds\n",
                "    3. Controlled randomization of marginals\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, target_col=None, task='classification',\n",
                "                 privacy_level=0.3, random_state=42):\n",
                "        \"\"\"\n",
                "        privacy_level: 0.0 = no privacy (exact marginals)\n",
                "                       0.5 = balanced privacy-utility\n",
                "                       1.0 = max privacy (more randomization)\n",
                "        \"\"\"\n",
                "        self.target_col = target_col\n",
                "        self.task = task\n",
                "        self.privacy_level = privacy_level\n",
                "        self.random_state = random_state\n",
                "        \n",
                "    def fit(self, df):\n",
                "        self.columns = list(df.columns)\n",
                "        self.n_train = len(df)\n",
                "        self.train_data = df.values.copy()\n",
                "        \n",
                "        # Store column info\n",
                "        self.col_info = {}\n",
                "        for i, col in enumerate(self.columns):\n",
                "            values = df[col].values\n",
                "            self.col_info[col] = {\n",
                "                'idx': i,\n",
                "                'values': values.copy(),\n",
                "                'sorted': np.sort(values),\n",
                "                'min': values.min(),\n",
                "                'max': values.max(),\n",
                "                'std': values.std(),\n",
                "                'unique': len(np.unique(values))\n",
                "            }\n",
                "        \n",
                "        # Learn copula correlation\n",
                "        uniform_df = df.copy()\n",
                "        for col in self.columns:\n",
                "            uniform_df[col] = stats.rankdata(df[col]) / (len(df) + 1)\n",
                "        \n",
                "        normal_df = uniform_df.apply(lambda x: stats.norm.ppf(np.clip(x, 0.001, 0.999)))\n",
                "        corr_matrix = normal_df.corr().values\n",
                "        corr_matrix = np.nan_to_num(corr_matrix, nan=0.0)\n",
                "        np.fill_diagonal(corr_matrix, 1.0)\n",
                "        \n",
                "        # Positive definite adjustment\n",
                "        eigvals, eigvecs = np.linalg.eigh(corr_matrix)\n",
                "        eigvals = np.maximum(eigvals, 1e-6)\n",
                "        corr_matrix = eigvecs @ np.diag(eigvals) @ eigvecs.T\n",
                "        np.fill_diagonal(corr_matrix, 1.0)\n",
                "        \n",
                "        self.cholesky = np.linalg.cholesky(corr_matrix)\n",
                "        \n",
                "        # Target model\n",
                "        if self.target_col and self.target_col in self.columns:\n",
                "            feature_cols = [c for c in self.columns if c != self.target_col]\n",
                "            if self.task == 'classification':\n",
                "                self.target_model = GradientBoostingClassifier(\n",
                "                    n_estimators=50, max_depth=4, random_state=self.random_state\n",
                "                )\n",
                "            else:\n",
                "                from sklearn.ensemble import GradientBoostingRegressor\n",
                "                self.target_model = GradientBoostingRegressor(\n",
                "                    n_estimators=50, max_depth=4, random_state=self.random_state\n",
                "                )\n",
                "            self.target_model.fit(df[feature_cols], df[self.target_col])\n",
                "            self.feature_cols = feature_cols\n",
                "            self.target_rate = df[self.target_col].mean() if self.task == 'classification' else None\n",
                "        \n",
                "        return self\n",
                "    \n",
                "    def _interpolate_marginal(self, col_name, uniform_vals, rng):\n",
                "        \"\"\"\n",
                "        Privacy-preserving marginal sampling via interpolation.\n",
                "        Instead of exact quantile, interpolate between neighbors.\n",
                "        \"\"\"\n",
                "        info = self.col_info[col_name]\n",
                "        sorted_vals = info['sorted']\n",
                "        n = len(sorted_vals)\n",
                "        \n",
                "        # Exact positions in sorted array\n",
                "        positions = uniform_vals * (n - 1)\n",
                "        lower_idx = np.floor(positions).astype(int)\n",
                "        upper_idx = np.minimum(lower_idx + 1, n - 1)\n",
                "        \n",
                "        # Interpolation weights (with privacy randomization)\n",
                "        base_weights = positions - lower_idx\n",
                "        \n",
                "        # Add controlled randomization based on privacy level\n",
                "        if self.privacy_level > 0:\n",
                "            # Randomly shift weights to create diversity\n",
                "            noise = rng.uniform(-self.privacy_level, self.privacy_level, len(base_weights))\n",
                "            weights = np.clip(base_weights + noise * 0.5, 0, 1)\n",
                "        else:\n",
                "            weights = base_weights\n",
                "        \n",
                "        # Linear interpolation between neighbors\n",
                "        lower_vals = sorted_vals[lower_idx]\n",
                "        upper_vals = sorted_vals[upper_idx]\n",
                "        result = lower_vals * (1 - weights) + upper_vals * weights\n",
                "        \n",
                "        # Ensure in valid range\n",
                "        result = np.clip(result, info['min'], info['max'])\n",
                "        \n",
                "        return result\n",
                "    \n",
                "    def sample(self, n_samples, seed=None):\n",
                "        if seed is None:\n",
                "            seed = self.random_state\n",
                "        rng = np.random.default_rng(seed)\n",
                "        \n",
                "        # Correlated uniform sampling\n",
                "        z = rng.standard_normal((n_samples, len(self.columns)))\n",
                "        uniform = stats.norm.cdf(z @ self.cholesky.T)\n",
                "        uniform = np.clip(uniform, 0.001, 0.999)\n",
                "        \n",
                "        # Privacy enhancement: add small perturbations to uniform values\n",
                "        if self.privacy_level > 0:\n",
                "            perturb = rng.uniform(-0.05 * self.privacy_level, 0.05 * self.privacy_level, uniform.shape)\n",
                "            uniform = np.clip(uniform + perturb, 0.001, 0.999)\n",
                "        \n",
                "        synthetic_data = {}\n",
                "        for i, col in enumerate(self.columns):\n",
                "            if col == self.target_col:\n",
                "                continue\n",
                "            synthetic_data[col] = self._interpolate_marginal(col, uniform[:, i], rng)\n",
                "        \n",
                "        # Generate target\n",
                "        if self.target_col and self.target_col in self.columns:\n",
                "            X_synth = pd.DataFrame({c: synthetic_data[c] for c in self.feature_cols})\n",
                "            if self.task == 'classification':\n",
                "                probs = self.target_model.predict_proba(X_synth)[:, 1]\n",
                "                # Smooth threshold with privacy\n",
                "                if self.privacy_level > 0:\n",
                "                    noise = rng.uniform(-0.05 * self.privacy_level, 0.05 * self.privacy_level, len(probs))\n",
                "                    probs = np.clip(probs + noise, 0, 1)\n",
                "                threshold = np.percentile(probs, (1 - self.target_rate) * 100)\n",
                "                synthetic_data[self.target_col] = (probs >= threshold).astype(int)\n",
                "            else:\n",
                "                synthetic_data[self.target_col] = self.target_model.predict(X_synth)\n",
                "        \n",
                "        return pd.DataFrame(synthetic_data)[self.columns]\n",
                "\n",
                "print(\"MISATA v3 Synthesizer defined.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Evaluation Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_metrics(real_train, real_test, synth):\n",
                "    \"\"\"Compute all metrics.\"\"\"\n",
                "    results = {}\n",
                "    \n",
                "    # 1. Correlation preservation (fixed)\n",
                "    real_corr = real_train.corr().values\n",
                "    synth_corr = synth.corr().values\n",
                "    mask = ~np.eye(real_corr.shape[0], dtype=bool)\n",
                "    \n",
                "    real_flat = real_corr[mask]\n",
                "    synth_flat = synth_corr[mask]\n",
                "    valid = ~(np.isnan(real_flat) | np.isnan(synth_flat))\n",
                "    \n",
                "    results['pearson_corr'] = np.corrcoef(real_flat[valid], synth_flat[valid])[0, 1]\n",
                "    tau, _ = stats.kendalltau(real_flat[valid], synth_flat[valid])\n",
                "    results['kendall_tau'] = tau\n",
                "    \n",
                "    # 2. Marginal fidelity\n",
                "    ks_scores = []\n",
                "    for col in real_train.columns:\n",
                "        stat, _ = stats.ks_2samp(real_train[col], synth[col])\n",
                "        ks_scores.append(1 - stat)\n",
                "    results['marginal_fidelity'] = np.mean(ks_scores)\n",
                "    \n",
                "    # 3. Privacy: MIA\n",
                "    n_test = min(1000, len(real_train), len(synth))\n",
                "    real_sample = real_train.sample(n_test, random_state=42)\n",
                "    synth_sample = synth.sample(n_test, random_state=42)\n",
                "    \n",
                "    X_mia = pd.concat([real_sample, synth_sample], ignore_index=True)\n",
                "    y_mia = np.array([1] * n_test + [0] * n_test)\n",
                "    \n",
                "    X_tr, X_te, y_tr, y_te = train_test_split(X_mia, y_mia, test_size=0.3, random_state=42)\n",
                "    mia_model = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
                "    mia_model.fit(X_tr, y_tr)\n",
                "    \n",
                "    results['mia_auc'] = roc_auc_score(y_te, mia_model.predict_proba(X_te)[:, 1])\n",
                "    results['mia_advantage'] = 2 * (results['mia_auc'] - 0.5)\n",
                "    \n",
                "    # 4. DCR\n",
                "    scaler = StandardScaler()\n",
                "    real_scaled = scaler.fit_transform(real_train)\n",
                "    synth_scaled = scaler.transform(synth)\n",
                "    \n",
                "    nn = NearestNeighbors(n_neighbors=1)\n",
                "    nn.fit(real_scaled)\n",
                "    distances, _ = nn.kneighbors(synth_scaled)\n",
                "    results['dcr_mean'] = np.mean(distances)\n",
                "    results['dcr_5th'] = np.percentile(distances, 5)\n",
                "    \n",
                "    # 5. TSTR\n",
                "    target = 'income'\n",
                "    X_synth = synth.drop(target, axis=1)\n",
                "    y_synth = synth[target]\n",
                "    X_test = real_test.drop(target, axis=1)\n",
                "    y_test = real_test[target]\n",
                "    \n",
                "    model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
                "    model.fit(X_synth, y_synth)\n",
                "    results['tstr_auc'] = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
                "    \n",
                "    # TRTR\n",
                "    model_real = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
                "    model_real.fit(real_train.drop(target, axis=1), real_train[target])\n",
                "    trtr = roc_auc_score(y_test, model_real.predict_proba(X_test)[:, 1])\n",
                "    results['tstr_ratio'] = results['tstr_auc'] / trtr\n",
                "    \n",
                "    return results\n",
                "\n",
                "print(\"Metrics defined.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load Data and Test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Adult Census\n",
                "print(\"Loading Adult Census...\")\n",
                "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
                "columns = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status',\n",
                "           'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss',\n",
                "           'hours_per_week', 'native_country', 'income']\n",
                "df_raw = pd.read_csv(url, names=columns, na_values=' ?', skipinitialspace=True)\n",
                "df_raw = df_raw.dropna().reset_index(drop=True).sample(5000, random_state=SEED)\n",
                "df_raw['income'] = (df_raw['income'] == '>50K').astype(int)\n",
                "\n",
                "for col in ['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country']:\n",
                "    df_raw[col] = LabelEncoder().fit_transform(df_raw[col].astype(str))\n",
                "\n",
                "train_df, test_df = train_test_split(df_raw, test_size=0.2, random_state=SEED)\n",
                "print(f\"Train: {len(train_df)}, Test: {len(test_df)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test different privacy levels\n",
                "privacy_levels = [0.0, 0.1, 0.2, 0.3, 0.5, 0.7]\n",
                "results = []\n",
                "\n",
                "print(\"\\nTesting privacy levels...\\n\")\n",
                "\n",
                "for privacy in privacy_levels:\n",
                "    print(f\"Privacy level: {privacy}\")\n",
                "    \n",
                "    synth = MISATAv3Synthesizer(target_col='income', privacy_level=privacy, random_state=SEED)\n",
                "    synth.fit(train_df)\n",
                "    df_synth = synth.sample(len(train_df))\n",
                "    \n",
                "    metrics = compute_metrics(train_df, test_df, df_synth)\n",
                "    metrics['privacy_level'] = privacy\n",
                "    results.append(metrics)\n",
                "    \n",
                "    print(f\"  Kendall τ: {metrics['kendall_tau']:.4f}, MIA: {metrics['mia_auc']:.4f}, TSTR: {metrics['tstr_ratio']:.2%}\")\n",
                "\n",
                "results_df = pd.DataFrame(results)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"MISATA v3 RESULTS\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "display_cols = ['privacy_level', 'kendall_tau', 'mia_auc', 'mia_advantage', 'tstr_ratio', 'marginal_fidelity']\n",
                "print(results_df[display_cols].to_string(index=False))\n",
                "\n",
                "# Find optimal\n",
                "viable = results_df[results_df['tstr_ratio'] > 0.90]\n",
                "if len(viable) > 0:\n",
                "    best = viable.loc[viable['mia_auc'].idxmin()]\n",
                "    print(f\"\\n✓ Optimal Configuration:\")\n",
                "    print(f\"  Privacy Level: {best['privacy_level']}\")\n",
                "    print(f\"  MIA AUC: {best['mia_auc']:.4f} (lower = better)\")\n",
                "    print(f\"  MIA Advantage: {best['mia_advantage']:.4f}\")\n",
                "    print(f\"  TSTR Ratio: {best['tstr_ratio']:.2%}\")\n",
                "    print(f\"  Kendall τ: {best['kendall_tau']:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualization\n",
                "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
                "\n",
                "# Plot 1: Privacy-Utility Tradeoff\n",
                "ax1 = axes[0]\n",
                "ax1.plot(results_df['privacy_level'], results_df['mia_auc'], 'b-o', label='MIA AUC (↓better)', linewidth=2, markersize=8)\n",
                "ax1.plot(results_df['privacy_level'], results_df['tstr_ratio'], 'g-s', label='TSTR Ratio', linewidth=2, markersize=8)\n",
                "ax1.axhline(y=0.5, color='blue', linestyle='--', alpha=0.5)\n",
                "ax1.axhline(y=0.9, color='green', linestyle='--', alpha=0.5)\n",
                "ax1.set_xlabel('Privacy Level', fontsize=12)\n",
                "ax1.set_ylabel('Score', fontsize=12)\n",
                "ax1.set_title('Privacy-Utility Tradeoff', fontsize=14, fontweight='bold')\n",
                "ax1.legend(fontsize=10)\n",
                "ax1.grid(True, alpha=0.3)\n",
                "ax1.set_ylim(0.4, 1.05)\n",
                "\n",
                "# Plot 2: Correlation Preservation\n",
                "ax2 = axes[1]\n",
                "ax2.plot(results_df['privacy_level'], results_df['pearson_corr'], 'r-o', label='Pearson', linewidth=2, markersize=8)\n",
                "ax2.plot(results_df['privacy_level'], results_df['kendall_tau'], 'b-s', label='Kendall τ', linewidth=2, markersize=8)\n",
                "ax2.set_xlabel('Privacy Level', fontsize=12)\n",
                "ax2.set_ylabel('Correlation Similarity', fontsize=12)\n",
                "ax2.set_title('Correlation Preservation', fontsize=14, fontweight='bold')\n",
                "ax2.legend(fontsize=10)\n",
                "ax2.grid(True, alpha=0.3)\n",
                "\n",
                "# Plot 3: Distance to Real Data\n",
                "ax3 = axes[2]\n",
                "ax3.plot(results_df['privacy_level'], results_df['dcr_5th'], 'purple', marker='o', linewidth=2, markersize=8)\n",
                "ax3.set_xlabel('Privacy Level', fontsize=12)\n",
                "ax3.set_ylabel('DCR 5th Percentile', fontsize=12)\n",
                "ax3.set_title('Distance to Closest Real Record', fontsize=14, fontweight='bold')\n",
                "ax3.grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('misata_v3_results.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(\"\\n✓ Saved misata_v3_results.png\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save results\n",
                "results_df.to_csv('misata_v3_privacy_results.csv', index=False)\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"EXPERIMENT 17 COMPLETE\")\n",
                "print(\"=\"*80)\n",
                "print(\"\\nKey Improvements in v3:\")\n",
                "print(\"  1. Interpolation-based marginal sampling (no out-of-range artifacts)\")\n",
                "print(\"  2. Subtle uniform perturbations (preserves correlation structure)\")\n",
                "print(\"  3. Controlled privacy-utility tradeoff\")\n",
                "print(\"\\nFiles saved:\")\n",
                "print(\"  - misata_v3_results.png\")\n",
                "print(\"  - misata_v3_privacy_results.csv\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}