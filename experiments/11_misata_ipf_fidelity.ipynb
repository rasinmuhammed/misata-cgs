{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# MISATA-IPF v2: Production-Grade Statistical Fidelity\n",
                "\n",
                "This is the **definitive** implementation. No compromises.\n",
                "\n",
                "## What We're Fixing\n",
                "The previous IPF implementation had weak marginal matching (60%). This version:\n",
                "1. Uses **exact empirical sampling** for marginals\n",
                "2. Uses **Gaussian copula** for correlation preservation\n",
                "3. Applies **causal layer** for target variable\n",
                "\n",
                "## Target Metrics\n",
                "- Marginal Similarity: **95%+**\n",
                "- Correlation Similarity: **95%+**\n",
                "- TSTR Ratio: **90%+**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q numpy pandas scikit-learn matplotlib seaborn scipy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "from scipy import stats\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
                "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import time\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "print(\"Setup complete.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 1: Load and Prepare Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Adult Census\n",
                "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
                "columns = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status',\n",
                "           'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss',\n",
                "           'hours_per_week', 'native_country', 'income']\n",
                "\n",
                "df_raw = pd.read_csv(url, names=columns, na_values=' ?', skipinitialspace=True)\n",
                "df_raw = df_raw.dropna().reset_index(drop=True)\n",
                "df_raw['income'] = (df_raw['income'] == '>50K').astype(int)\n",
                "\n",
                "# Column types\n",
                "categorical_cols = ['workclass', 'education', 'marital_status', 'occupation', \n",
                "                    'relationship', 'race', 'sex', 'native_country']\n",
                "numerical_cols = ['age', 'fnlwgt', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week']\n",
                "\n",
                "# Encode\n",
                "df = df_raw.copy()\n",
                "encoders = {}\n",
                "for col in categorical_cols:\n",
                "    le = LabelEncoder()\n",
                "    df[col] = le.fit_transform(df[col].astype(str))\n",
                "    encoders[col] = le\n",
                "\n",
                "# Split\n",
                "X = df.drop('income', axis=1)\n",
                "y = df['income']\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
                "train_df = pd.concat([X_train, y_train], axis=1).reset_index(drop=True)\n",
                "\n",
                "print(f\"Dataset: {len(df_raw):,} rows\")\n",
                "print(f\"Train: {len(train_df):,}, Test: {len(X_test):,}\")\n",
                "print(f\"Income distribution: {train_df['income'].mean():.1%} high income\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 2: Production IPF Synthesizer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ProductionIPFSynthesizer:\n",
                "    \"\"\"\n",
                "    Production-grade IPF-based synthesizer.\n",
                "    \n",
                "    Key innovations:\n",
                "    1. Exact marginal matching via empirical resampling\n",
                "    2. Correlation preservation via Gaussian copula\n",
                "    3. Causal target modeling\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, random_state=42):\n",
                "        self.random_state = random_state\n",
                "        self.fitted = False\n",
                "        \n",
                "    def fit(self, df: pd.DataFrame, target_col: str = 'income'):\n",
                "        \"\"\"Learn distributions from training data.\"\"\"\n",
                "        self.columns = list(df.columns)\n",
                "        self.target_col = target_col\n",
                "        self.n_train = len(df)\n",
                "        \n",
                "        # Store raw training data for resampling\n",
                "        self.train_data = df.copy()\n",
                "        \n",
                "        # Learn marginal CDFs for each column\n",
                "        self.marginals = {}\n",
                "        for col in self.columns:\n",
                "            values = df[col].values\n",
                "            self.marginals[col] = {\n",
                "                'values': np.sort(np.unique(values)),\n",
                "                'all_values': values,  # Keep all for resampling\n",
                "                'mean': np.mean(values),\n",
                "                'std': np.std(values)\n",
                "            }\n",
                "        \n",
                "        # Learn correlation structure via copula\n",
                "        # Convert to uniform marginals via rank transform\n",
                "        uniform_df = df.copy()\n",
                "        for col in self.columns:\n",
                "            uniform_df[col] = stats.rankdata(df[col]) / (len(df) + 1)\n",
                "        \n",
                "        # Convert to normal space\n",
                "        normal_df = uniform_df.apply(lambda x: stats.norm.ppf(np.clip(x, 0.001, 0.999)))\n",
                "        \n",
                "        # Compute and fix correlation matrix\n",
                "        corr_matrix = normal_df.corr().values\n",
                "        corr_matrix = np.nan_to_num(corr_matrix, nan=0.0)\n",
                "        np.fill_diagonal(corr_matrix, 1.0)\n",
                "        \n",
                "        # Ensure positive definite\n",
                "        eigvals, eigvecs = np.linalg.eigh(corr_matrix)\n",
                "        eigvals = np.maximum(eigvals, 1e-6)\n",
                "        corr_matrix = eigvecs @ np.diag(eigvals) @ eigvecs.T\n",
                "        corr_matrix = (corr_matrix + corr_matrix.T) / 2\n",
                "        np.fill_diagonal(corr_matrix, 1.0)\n",
                "        \n",
                "        self.corr_matrix = corr_matrix\n",
                "        self.cholesky = np.linalg.cholesky(corr_matrix)\n",
                "        \n",
                "        # Learn causal model for target\n",
                "        feature_cols = [c for c in self.columns if c != target_col]\n",
                "        X_causal = df[feature_cols]\n",
                "        y_causal = df[target_col]\n",
                "        \n",
                "        self.causal_model = GradientBoostingClassifier(\n",
                "            n_estimators=50, max_depth=4, random_state=self.random_state\n",
                "        )\n",
                "        self.causal_model.fit(X_causal, y_causal)\n",
                "        self.feature_cols = feature_cols\n",
                "        self.target_rate = y_causal.mean()\n",
                "        \n",
                "        self.fitted = True\n",
                "        return self\n",
                "    \n",
                "    def sample(self, n_samples: int) -> pd.DataFrame:\n",
                "        \"\"\"Generate synthetic samples.\"\"\"\n",
                "        if not self.fitted:\n",
                "            raise ValueError(\"Call fit() first\")\n",
                "        \n",
                "        rng = np.random.default_rng(self.random_state)\n",
                "        \n",
                "        # Method: Correlated uniform sampling + quantile transform\n",
                "        \n",
                "        # 1. Generate correlated standard normals\n",
                "        z = rng.standard_normal((n_samples, len(self.columns)))\n",
                "        correlated_z = z @ self.cholesky.T\n",
                "        \n",
                "        # 2. Convert to uniform [0, 1]\n",
                "        uniform = stats.norm.cdf(correlated_z)\n",
                "        uniform = np.clip(uniform, 0.001, 0.999)\n",
                "        \n",
                "        # 3. Transform each column using empirical quantiles\n",
                "        synthetic_data = {}\n",
                "        for i, col in enumerate(self.columns):\n",
                "            if col == self.target_col:\n",
                "                continue  # Handle target separately\n",
                "            \n",
                "            # Get sorted training values\n",
                "            sorted_vals = np.sort(self.marginals[col]['all_values'])\n",
                "            n_vals = len(sorted_vals)\n",
                "            \n",
                "            # Compute quantile positions\n",
                "            positions = np.linspace(0, 1, n_vals)\n",
                "            \n",
                "            # Interpolate\n",
                "            synthetic_vals = np.interp(uniform[:, i], positions, sorted_vals)\n",
                "            \n",
                "            # Round categorical/integer columns\n",
                "            if col in ['age', 'fnlwgt', 'education_num', 'capital_gain', \n",
                "                       'capital_loss', 'hours_per_week'] or \\\n",
                "               col in ['workclass', 'education', 'marital_status', 'occupation',\n",
                "                       'relationship', 'race', 'sex', 'native_country']:\n",
                "                synthetic_vals = np.round(synthetic_vals).astype(int)\n",
                "                # Clip to valid range\n",
                "                min_val = self.marginals[col]['values'].min()\n",
                "                max_val = self.marginals[col]['values'].max()\n",
                "                synthetic_vals = np.clip(synthetic_vals, min_val, max_val)\n",
                "            \n",
                "            synthetic_data[col] = synthetic_vals\n",
                "        \n",
                "        # 4. Generate target using causal model\n",
                "        synth_features = pd.DataFrame({c: synthetic_data[c] for c in self.feature_cols})\n",
                "        target_probs = self.causal_model.predict_proba(synth_features)[:, 1]\n",
                "        \n",
                "        # Calibrate to match training rate\n",
                "        threshold = np.percentile(target_probs, (1 - self.target_rate) * 100)\n",
                "        synthetic_data[self.target_col] = (target_probs >= threshold).astype(int)\n",
                "        \n",
                "        # Create DataFrame with correct column order\n",
                "        return pd.DataFrame(synthetic_data)[self.columns]\n",
                "\n",
                "\n",
                "# Fit synthesizer\n",
                "print(\"Fitting Production IPF Synthesizer...\")\n",
                "start = time.time()\n",
                "synth = ProductionIPFSynthesizer(random_state=42)\n",
                "synth.fit(train_df, target_col='income')\n",
                "fit_time = time.time() - start\n",
                "print(f\"Fitted in {fit_time:.2f}s\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate synthetic data\n",
                "print(\"\\nGenerating synthetic data...\")\n",
                "n_synthetic = len(train_df)\n",
                "\n",
                "start = time.time()\n",
                "df_synth = synth.sample(n_synthetic)\n",
                "gen_time = time.time() - start\n",
                "\n",
                "print(f\"Generated {len(df_synth):,} rows in {gen_time:.3f}s\")\n",
                "print(f\"Throughput: {len(df_synth)/gen_time:,.0f} rows/sec\")\n",
                "print(f\"\\nIncome distribution:\")\n",
                "print(f\"  Real:      {train_df['income'].mean():.2%}\")\n",
                "print(f\"  Synthetic: {df_synth['income'].mean():.2%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 3: Comprehensive Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate_comprehensive(real_df: pd.DataFrame, synth_df: pd.DataFrame):\n",
                "    \"\"\"Comprehensive statistical fidelity evaluation.\"\"\"\n",
                "    results = {}\n",
                "    \n",
                "    # 1. Marginal similarity (KS statistic)\n",
                "    ks_scores = []\n",
                "    for col in real_df.columns:\n",
                "        stat, _ = stats.ks_2samp(real_df[col], synth_df[col])\n",
                "        ks_scores.append(1 - stat)  # Convert to similarity\n",
                "    results['marginal_similarity'] = np.mean(ks_scores)\n",
                "    \n",
                "    # 2. Per-column marginal analysis\n",
                "    print(\"\\nPer-column marginal similarity (1 - KS statistic):\")\n",
                "    for col in real_df.columns:\n",
                "        stat, _ = stats.ks_2samp(real_df[col], synth_df[col])\n",
                "        print(f\"  {col}: {1-stat:.2%}\")\n",
                "    \n",
                "    # 3. Correlation preservation\n",
                "    num_cols = real_df.select_dtypes(include=[np.number]).columns\n",
                "    real_corr = real_df[num_cols].corr().values.flatten()\n",
                "    synth_corr = synth_df[num_cols].corr().values.flatten()\n",
                "    mask = ~(np.isnan(real_corr) | np.isnan(synth_corr))\n",
                "    results['correlation_similarity'] = np.corrcoef(real_corr[mask], synth_corr[mask])[0, 1]\n",
                "    \n",
                "    # 4. Mean/Std preservation\n",
                "    mean_scores = []\n",
                "    std_scores = []\n",
                "    for col in num_cols:\n",
                "        real_mean, synth_mean = real_df[col].mean(), synth_df[col].mean()\n",
                "        real_std, synth_std = real_df[col].std(), synth_df[col].std()\n",
                "        \n",
                "        if abs(real_mean) > 0:\n",
                "            mean_scores.append(1 - min(abs(real_mean - synth_mean) / abs(real_mean), 1))\n",
                "        if abs(real_std) > 0:\n",
                "            std_scores.append(1 - min(abs(real_std - synth_std) / abs(real_std), 1))\n",
                "    \n",
                "    results['mean_preservation'] = np.mean(mean_scores) if mean_scores else 0\n",
                "    results['std_preservation'] = np.mean(std_scores) if std_scores else 0\n",
                "    \n",
                "    # 5. Overall fidelity\n",
                "    results['overall_fidelity'] = np.mean([\n",
                "        results['marginal_similarity'],\n",
                "        results['correlation_similarity'],\n",
                "        results['mean_preservation'],\n",
                "        results['std_preservation']\n",
                "    ])\n",
                "    \n",
                "    return results\n",
                "\n",
                "\n",
                "# Evaluate\n",
                "print(\"=\" * 70)\n",
                "print(\"STATISTICAL FIDELITY EVALUATION\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "fidelity = evaluate_comprehensive(train_df, df_synth)\n",
                "\n",
                "print(f\"\\n{'='*70}\")\n",
                "print(\"SUMMARY\")\n",
                "print(f\"{'='*70}\")\n",
                "print(f\"Marginal Similarity:    {fidelity['marginal_similarity']:.2%}\")\n",
                "print(f\"Correlation Similarity: {fidelity['correlation_similarity']:.2%}\")\n",
                "print(f\"Mean Preservation:      {fidelity['mean_preservation']:.2%}\")\n",
                "print(f\"Std Preservation:       {fidelity['std_preservation']:.2%}\")\n",
                "print(f\"\\nOVERALL FIDELITY:       {fidelity['overall_fidelity']:.2%}\")\n",
                "print(f\"\\nComparison:\")\n",
                "print(f\"  Previous MISATA: 54%\")\n",
                "print(f\"  Previous IPF v1: 72.6%\")\n",
                "print(f\"  Current IPF v2:  {fidelity['overall_fidelity']:.1%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 4: TSTR Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate_tstr(df_train_real, df_train_synth, X_test, y_test):\n",
                "    \"\"\"Comprehensive TSTR evaluation.\"\"\"\n",
                "    results = {}\n",
                "    \n",
                "    # TRTR (baseline)\n",
                "    X_real = df_train_real.drop('income', axis=1)\n",
                "    y_real = df_train_real['income']\n",
                "    \n",
                "    model_real = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
                "    model_real.fit(X_real, y_real)\n",
                "    y_pred_real = model_real.predict(X_test)\n",
                "    y_prob_real = model_real.predict_proba(X_test)[:, 1]\n",
                "    \n",
                "    results['real'] = {\n",
                "        'accuracy': accuracy_score(y_test, y_pred_real),\n",
                "        'roc_auc': roc_auc_score(y_test, y_prob_real),\n",
                "        'f1': f1_score(y_test, y_pred_real)\n",
                "    }\n",
                "    \n",
                "    # TSTR (synthetic)\n",
                "    X_synth = df_train_synth.drop('income', axis=1)\n",
                "    y_synth = df_train_synth['income']\n",
                "    \n",
                "    model_synth = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
                "    model_synth.fit(X_synth, y_synth)\n",
                "    y_pred_synth = model_synth.predict(X_test)\n",
                "    y_prob_synth = model_synth.predict_proba(X_test)[:, 1]\n",
                "    \n",
                "    results['synth'] = {\n",
                "        'accuracy': accuracy_score(y_test, y_pred_synth),\n",
                "        'roc_auc': roc_auc_score(y_test, y_prob_synth),\n",
                "        'f1': f1_score(y_test, y_pred_synth)\n",
                "    }\n",
                "    \n",
                "    results['tstr_ratio'] = results['synth']['roc_auc'] / results['real']['roc_auc']\n",
                "    \n",
                "    return results\n",
                "\n",
                "\n",
                "print(\"\\n\" + \"=\" * 70)\n",
                "print(\"TSTR EVALUATION\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "tstr = evaluate_tstr(train_df, df_synth, X_test, y_test)\n",
                "\n",
                "print(f\"\\nReal (TRTR):\")\n",
                "print(f\"  Accuracy: {tstr['real']['accuracy']:.4f}\")\n",
                "print(f\"  ROC-AUC:  {tstr['real']['roc_auc']:.4f}\")\n",
                "print(f\"  F1:       {tstr['real']['f1']:.4f}\")\n",
                "\n",
                "print(f\"\\nSynthetic (TSTR):\")\n",
                "print(f\"  Accuracy: {tstr['synth']['accuracy']:.4f}\")\n",
                "print(f\"  ROC-AUC:  {tstr['synth']['roc_auc']:.4f}\")\n",
                "print(f\"  F1:       {tstr['synth']['f1']:.4f}\")\n",
                "\n",
                "print(f\"\\nTSTR RATIO: {tstr['tstr_ratio']:.2%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 5: Visualizations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Distribution comparison\n",
                "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
                "\n",
                "compare_cols = ['age', 'education_num', 'hours_per_week', 'capital_gain', 'income', 'fnlwgt']\n",
                "\n",
                "for ax, col in zip(axes.flat, compare_cols):\n",
                "    if col == 'income':\n",
                "        x = np.arange(2)\n",
                "        real_counts = train_df[col].value_counts(normalize=True).sort_index()\n",
                "        synth_counts = df_synth[col].value_counts(normalize=True).sort_index()\n",
                "        width = 0.35\n",
                "        ax.bar(x - width/2, real_counts.values, width, label='Real', alpha=0.8, color='steelblue')\n",
                "        ax.bar(x + width/2, synth_counts.values, width, label='MISATA-IPF', alpha=0.8, color='coral')\n",
                "        ax.set_xticks(x)\n",
                "        ax.set_xticklabels(['<=50K', '>50K'])\n",
                "    else:\n",
                "        ax.hist(train_df[col], bins=30, alpha=0.6, label='Real', density=True, color='steelblue')\n",
                "        ax.hist(df_synth[col], bins=30, alpha=0.6, label='MISATA-IPF', density=True, color='coral')\n",
                "    \n",
                "    ax.set_xlabel(col, fontsize=11)\n",
                "    ax.set_ylabel('Density', fontsize=11)\n",
                "    ax.set_title(f'{col} Distribution', fontsize=12, fontweight='bold')\n",
                "    ax.legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('misata_ipf_v2_distributions.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(\"✓ Saved misata_ipf_v2_distributions.png\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Correlation heatmaps\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
                "\n",
                "key_cols = ['age', 'education_num', 'hours_per_week', 'capital_gain', 'income']\n",
                "\n",
                "real_corr = train_df[key_cols].corr()\n",
                "synth_corr = df_synth[key_cols].corr()\n",
                "\n",
                "sns.heatmap(real_corr, annot=True, fmt='.2f', cmap='RdBu_r', center=0, \n",
                "            ax=axes[0], vmin=-1, vmax=1, square=True)\n",
                "axes[0].set_title('Real Data Correlations', fontsize=12, fontweight='bold')\n",
                "\n",
                "sns.heatmap(synth_corr, annot=True, fmt='.2f', cmap='RdBu_r', center=0,\n",
                "            ax=axes[1], vmin=-1, vmax=1, square=True)\n",
                "axes[1].set_title('MISATA-IPF Correlations', fontsize=12, fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('misata_ipf_v2_correlations.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(\"✓ Saved misata_ipf_v2_correlations.png\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save all results\n",
                "results_summary = {\n",
                "    'method': 'MISATA-IPF-v2',\n",
                "    'statistical_fidelity': fidelity['overall_fidelity'],\n",
                "    'marginal_similarity': fidelity['marginal_similarity'],\n",
                "    'correlation_similarity': fidelity['correlation_similarity'],\n",
                "    'mean_preservation': fidelity['mean_preservation'],\n",
                "    'std_preservation': fidelity['std_preservation'],\n",
                "    'tstr_roc_auc': tstr['synth']['roc_auc'],\n",
                "    'tstr_f1': tstr['synth']['f1'],\n",
                "    'tstr_ratio': tstr['tstr_ratio'],\n",
                "    'generation_time': gen_time,\n",
                "    'fit_time': fit_time,\n",
                "    'rows_per_second': n_synthetic / gen_time\n",
                "}\n",
                "\n",
                "pd.DataFrame([results_summary]).to_csv('misata_ipf_v2_results.csv', index=False)\n",
                "\n",
                "print(\"\\n\" + \"=\" * 70)\n",
                "print(\"EXPERIMENT COMPLETE\")\n",
                "print(\"=\" * 70)\n",
                "print(f\"\\nFinal Metrics:\")\n",
                "print(f\"  Statistical Fidelity: {fidelity['overall_fidelity']:.1%}\")\n",
                "print(f\"  TSTR Ratio:           {tstr['tstr_ratio']:.1%}\")\n",
                "print(f\"  Speed:                {n_synthetic/gen_time:,.0f} rows/sec\")\n",
                "print(f\"\\nFiles saved:\")\n",
                "print(f\"  - misata_ipf_v2_distributions.png\")\n",
                "print(f\"  - misata_ipf_v2_correlations.png\")\n",
                "print(f\"  - misata_ipf_v2_results.csv\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}