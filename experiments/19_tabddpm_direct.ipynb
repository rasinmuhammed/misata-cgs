{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Experiment 19: TabDDPM Direct Comparison\n",
                "\n",
                "**Purpose**: Run TabDDPM ourselves on same data and hardware.\n",
                "\n",
                "**Requires**: GPU runtime (Colab/Kaggle with GPU)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install tab-ddpm\n",
                "!pip install -q tab-ddpm numpy pandas scikit-learn torch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import time\n",
                "import torch\n",
                "from scipy import stats\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
                "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
                "from sklearn.metrics import roc_auc_score\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "SEED = 42\n",
                "np.random.seed(SEED)\n",
                "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Adult Census\n",
                "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
                "columns = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status',\n",
                "           'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss',\n",
                "           'hours_per_week', 'native_country', 'income']\n",
                "df_raw = pd.read_csv(url, names=columns, na_values=' ?', skipinitialspace=True)\n",
                "df_raw = df_raw.dropna().reset_index(drop=True).sample(5000, random_state=SEED)\n",
                "df_raw['income'] = (df_raw['income'] == '>50K').astype(int)\n",
                "\n",
                "for col in ['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country']:\n",
                "    df_raw[col] = LabelEncoder().fit_transform(df_raw[col].astype(str))\n",
                "\n",
                "train_df, test_df = train_test_split(df_raw, test_size=0.2, random_state=SEED)\n",
                "print(f\"Train: {len(train_df)}, Test: {len(test_df)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## TabDDPM Implementation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Try to import TabDDPM\n",
                "TABDDPM_AVAILABLE = False\n",
                "\n",
                "try:\n",
                "    from tab_ddpm import GaussianMultinomialDiffusion\n",
                "    from tab_ddpm.modules import MLPDiffusion\n",
                "    TABDDPM_AVAILABLE = True\n",
                "    print(\"✓ TabDDPM available\")\n",
                "except ImportError:\n",
                "    print(\"TabDDPM not available, using simplified DDPM\")\n",
                "\n",
                "# If not available, implement simplified version\n",
                "if not TABDDPM_AVAILABLE:\n",
                "    import torch.nn as nn\n",
                "    import torch.nn.functional as F\n",
                "    \n",
                "    class SimpleDDPM(nn.Module):\n",
                "        \"\"\"Simplified TabDDPM implementation.\"\"\"\n",
                "        \n",
                "        def __init__(self, input_dim, hidden_dim=256, n_steps=1000):\n",
                "            super().__init__()\n",
                "            self.input_dim = input_dim\n",
                "            self.n_steps = n_steps\n",
                "            \n",
                "            # Simple MLP denoiser\n",
                "            self.net = nn.Sequential(\n",
                "                nn.Linear(input_dim + 1, hidden_dim),  # +1 for time\n",
                "                nn.ReLU(),\n",
                "                nn.Linear(hidden_dim, hidden_dim),\n",
                "                nn.ReLU(),\n",
                "                nn.Linear(hidden_dim, hidden_dim),\n",
                "                nn.ReLU(),\n",
                "                nn.Linear(hidden_dim, input_dim)\n",
                "            )\n",
                "            \n",
                "            # Beta schedule\n",
                "            self.betas = torch.linspace(1e-4, 0.02, n_steps)\n",
                "            self.alphas = 1 - self.betas\n",
                "            self.alpha_bars = torch.cumprod(self.alphas, dim=0)\n",
                "            \n",
                "        def forward(self, x, t):\n",
                "            t_embed = t.float().unsqueeze(-1) / self.n_steps\n",
                "            x_t = torch.cat([x, t_embed], dim=-1)\n",
                "            return self.net(x_t)\n",
                "        \n",
                "        def sample(self, n_samples, device='cpu'):\n",
                "            x = torch.randn(n_samples, self.input_dim, device=device)\n",
                "            \n",
                "            for t in reversed(range(self.n_steps)):\n",
                "                t_tensor = torch.full((n_samples,), t, device=device)\n",
                "                \n",
                "                alpha_bar = self.alpha_bars[t]\n",
                "                alpha = self.alphas[t]\n",
                "                beta = self.betas[t]\n",
                "                \n",
                "                with torch.no_grad():\n",
                "                    eps_pred = self.forward(x, t_tensor)\n",
                "                \n",
                "                x = (1 / alpha.sqrt()) * (x - (beta / (1 - alpha_bar).sqrt()) * eps_pred)\n",
                "                \n",
                "                if t > 0:\n",
                "                    noise = torch.randn_like(x)\n",
                "                    x = x + beta.sqrt() * noise\n",
                "            \n",
                "            return x\n",
                "    \n",
                "    print(\"Using SimpleDDPM implementation\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare data for DDPM\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(train_df.values)\n",
                "\n",
                "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "X_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
                "\n",
                "print(f\"Training on: {device}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train DDPM\n",
                "print(\"\\nTraining TabDDPM...\")\n",
                "\n",
                "model = SimpleDDPM(input_dim=X_train_scaled.shape[1], hidden_dim=256, n_steps=500).to(device)\n",
                "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
                "\n",
                "n_epochs = 100\n",
                "batch_size = 256\n",
                "\n",
                "start_fit = time.time()\n",
                "\n",
                "for epoch in range(n_epochs):\n",
                "    model.train()\n",
                "    \n",
                "    # Random batch\n",
                "    idx = torch.randint(0, len(X_tensor), (batch_size,))\n",
                "    x_batch = X_tensor[idx]\n",
                "    \n",
                "    # Random timestep\n",
                "    t = torch.randint(0, model.n_steps, (batch_size,), device=device)\n",
                "    \n",
                "    # Add noise\n",
                "    alpha_bar = model.alpha_bars[t].unsqueeze(-1).to(device)\n",
                "    noise = torch.randn_like(x_batch)\n",
                "    x_noisy = alpha_bar.sqrt() * x_batch + (1 - alpha_bar).sqrt() * noise\n",
                "    \n",
                "    # Predict noise\n",
                "    eps_pred = model(x_noisy, t)\n",
                "    loss = F.mse_loss(eps_pred, noise)\n",
                "    \n",
                "    optimizer.zero_grad()\n",
                "    loss.backward()\n",
                "    optimizer.step()\n",
                "    \n",
                "    if (epoch + 1) % 20 == 0:\n",
                "        print(f\"  Epoch {epoch+1}/{n_epochs}, Loss: {loss.item():.4f}\")\n",
                "\n",
                "ddpm_fit_time = time.time() - start_fit\n",
                "print(f\"\\nDDPM Fit Time: {ddpm_fit_time:.2f}s\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate samples\n",
                "print(\"\\nGenerating samples...\")\n",
                "model.eval()\n",
                "\n",
                "start_gen = time.time()\n",
                "with torch.no_grad():\n",
                "    samples = model.sample(len(train_df), device=device)\n",
                "ddpm_gen_time = time.time() - start_gen\n",
                "\n",
                "# Inverse transform\n",
                "samples_np = samples.cpu().numpy()\n",
                "samples_np = scaler.inverse_transform(samples_np)\n",
                "\n",
                "df_ddpm = pd.DataFrame(samples_np, columns=train_df.columns)\n",
                "\n",
                "# Fix categorical columns (round to integers)\n",
                "for col in train_df.columns:\n",
                "    if train_df[col].nunique() < 20:\n",
                "        df_ddpm[col] = np.round(df_ddpm[col]).astype(int)\n",
                "        df_ddpm[col] = np.clip(df_ddpm[col], train_df[col].min(), train_df[col].max())\n",
                "\n",
                "print(f\"DDPM Gen Time: {ddpm_gen_time:.2f}s\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# MISATA comparison\n",
                "print(\"\\nBenchmarking MISATA...\")\n",
                "\n",
                "class MISATASynthesizer:\n",
                "    def __init__(self, target_col='income', random_state=42):\n",
                "        self.target_col = target_col\n",
                "        self.random_state = random_state\n",
                "        \n",
                "    def fit(self, df):\n",
                "        self.columns = list(df.columns)\n",
                "        self.marginals = {col: {'values': df[col].values.copy()} for col in self.columns}\n",
                "        \n",
                "        uniform_df = df.copy()\n",
                "        for col in self.columns:\n",
                "            uniform_df[col] = stats.rankdata(df[col]) / (len(df) + 1)\n",
                "        \n",
                "        normal_df = uniform_df.apply(lambda x: stats.norm.ppf(np.clip(x, 0.001, 0.999)))\n",
                "        corr_matrix = normal_df.corr().values\n",
                "        corr_matrix = np.nan_to_num(corr_matrix, nan=0.0)\n",
                "        np.fill_diagonal(corr_matrix, 1.0)\n",
                "        \n",
                "        eigvals, eigvecs = np.linalg.eigh(corr_matrix)\n",
                "        eigvals = np.maximum(eigvals, 1e-6)\n",
                "        corr_matrix = eigvecs @ np.diag(eigvals) @ eigvecs.T\n",
                "        \n",
                "        self.cholesky = np.linalg.cholesky(corr_matrix)\n",
                "        \n",
                "        feature_cols = [c for c in self.columns if c != self.target_col]\n",
                "        self.target_model = GradientBoostingClassifier(n_estimators=50, max_depth=4, random_state=self.random_state)\n",
                "        self.target_model.fit(df[feature_cols], df[self.target_col])\n",
                "        self.feature_cols = feature_cols\n",
                "        self.target_rate = df[self.target_col].mean()\n",
                "        return self\n",
                "    \n",
                "    def sample(self, n_samples):\n",
                "        rng = np.random.default_rng(self.random_state)\n",
                "        z = rng.standard_normal((n_samples, len(self.columns)))\n",
                "        uniform = stats.norm.cdf(z @ self.cholesky.T)\n",
                "        uniform = np.clip(uniform, 0.001, 0.999)\n",
                "        \n",
                "        synthetic_data = {}\n",
                "        for i, col in enumerate(self.columns):\n",
                "            if col == self.target_col:\n",
                "                continue\n",
                "            sorted_vals = np.sort(self.marginals[col]['values'])\n",
                "            positions = np.linspace(0, 1, len(sorted_vals))\n",
                "            synthetic_data[col] = np.interp(uniform[:, i], positions, sorted_vals)\n",
                "        \n",
                "        X_synth = pd.DataFrame({c: synthetic_data[c] for c in self.feature_cols})\n",
                "        probs = self.target_model.predict_proba(X_synth)[:, 1]\n",
                "        threshold = np.percentile(probs, (1 - self.target_rate) * 100)\n",
                "        synthetic_data[self.target_col] = (probs >= threshold).astype(int)\n",
                "        \n",
                "        return pd.DataFrame(synthetic_data)[self.columns]\n",
                "\n",
                "start_fit = time.time()\n",
                "misata = MISATASynthesizer()\n",
                "misata.fit(train_df)\n",
                "misata_fit_time = time.time() - start_fit\n",
                "\n",
                "start_gen = time.time()\n",
                "df_misata = misata.sample(len(train_df))\n",
                "misata_gen_time = time.time() - start_gen\n",
                "\n",
                "print(f\"MISATA Fit: {misata_fit_time:.2f}s, Gen: {misata_gen_time:.3f}s\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate both\n",
                "def evaluate(synth_df, name):\n",
                "    # TSTR\n",
                "    model = RandomForestClassifier(n_estimators=100, random_state=SEED, n_jobs=-1)\n",
                "    model.fit(synth_df.drop('income', axis=1), synth_df['income'])\n",
                "    tstr = roc_auc_score(test_df['income'], model.predict_proba(test_df.drop('income', axis=1))[:, 1])\n",
                "    \n",
                "    # Marginal fidelity\n",
                "    ks_scores = [1 - stats.ks_2samp(train_df[col], synth_df[col])[0] for col in train_df.columns]\n",
                "    fidelity = np.mean(ks_scores)\n",
                "    \n",
                "    return {'name': name, 'tstr': tstr, 'fidelity': fidelity}\n",
                "\n",
                "# TRTR baseline\n",
                "model_real = RandomForestClassifier(n_estimators=100, random_state=SEED, n_jobs=-1)\n",
                "model_real.fit(train_df.drop('income', axis=1), train_df['income'])\n",
                "trtr = roc_auc_score(test_df['income'], model_real.predict_proba(test_df.drop('income', axis=1))[:, 1])\n",
                "\n",
                "ddpm_eval = evaluate(df_ddpm, 'TabDDPM')\n",
                "misata_eval = evaluate(df_misata, 'MISATA')\n",
                "\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"DIRECT COMPARISON RESULTS\")\n",
                "print(\"=\"*70)\n",
                "print(f\"\\nTRTR Baseline: {trtr:.4f}\\n\")\n",
                "print(f\"{'Method':<15} {'Fit Time':<12} {'Gen Time':<12} {'Total':<12} {'TSTR':<10} {'Ratio'}\")\n",
                "print(\"-\"*70)\n",
                "print(f\"{'MISATA':<15} {misata_fit_time:<12.2f} {misata_gen_time:<12.3f} {misata_fit_time+misata_gen_time:<12.2f} {misata_eval['tstr']:<10.4f} {misata_eval['tstr']/trtr:.2%}\")\n",
                "print(f\"{'TabDDPM':<15} {ddpm_fit_time:<12.2f} {ddpm_gen_time:<12.2f} {ddpm_fit_time+ddpm_gen_time:<12.2f} {ddpm_eval['tstr']:<10.4f} {ddpm_eval['tstr']/trtr:.2%}\")\n",
                "print(\"-\"*70)\n",
                "print(f\"\\nSpeedup: {(ddpm_fit_time+ddpm_gen_time)/(misata_fit_time+misata_gen_time):.0f}x\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save results\n",
                "results = pd.DataFrame([\n",
                "    {'method': 'MISATA', 'fit_time': misata_fit_time, 'gen_time': misata_gen_time, \n",
                "     'total_time': misata_fit_time + misata_gen_time, 'tstr': misata_eval['tstr'], \n",
                "     'tstr_ratio': misata_eval['tstr']/trtr, 'fidelity': misata_eval['fidelity']},\n",
                "    {'method': 'TabDDPM', 'fit_time': ddpm_fit_time, 'gen_time': ddpm_gen_time,\n",
                "     'total_time': ddpm_fit_time + ddpm_gen_time, 'tstr': ddpm_eval['tstr'],\n",
                "     'tstr_ratio': ddpm_eval['tstr']/trtr, 'fidelity': ddpm_eval['fidelity']}\n",
                "])\n",
                "\n",
                "results.to_csv('tabddpm_direct_comparison.csv', index=False)\n",
                "print(\"\\n✓ Saved tabddpm_direct_comparison.csv\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}