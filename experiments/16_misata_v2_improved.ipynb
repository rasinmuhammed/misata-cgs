{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Experiment 16: MISATA v2 - Improved Synthesizer\n",
                "\n",
                "## Issues from Experiment 15 to Fix\n",
                "\n",
                "| Issue | Problem | Solution |\n",
                "|-------|---------|----------|\n",
                "| Kendall τ = 0.003 | Metric bug (sample-wise, not matrix-wise) | Fix computation |\n",
                "| MIA AUC = 0.868 | Synthetic too distinguishable | Add noise injection |\n",
                "| Cover Type 89.8% | Struggles high-dim | Better marginal sampling |\n",
                "\n",
                "## Improvements Implemented\n",
                "1. **Noise injection** to improve privacy\n",
                "2. **Bootstrap marginal sampling** for diversity\n",
                "3. **Fixed correlation metrics**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "from scipy import stats\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
                "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
                "from sklearn.neighbors import NearestNeighbors\n",
                "from sklearn.metrics import roc_auc_score\n",
                "import matplotlib.pyplot as plt\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "SEED = 42\n",
                "np.random.seed(SEED)\n",
                "print(\"Setup complete.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## MISATA v2: Privacy-Enhanced Synthesizer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MISATAv2Synthesizer:\n",
                "    \"\"\"\n",
                "    MISATA v2 with privacy enhancements:\n",
                "    1. Noise injection for privacy\n",
                "    2. Bootstrap marginal sampling for diversity\n",
                "    3. Laplace noise on correlations\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, target_col=None, task='classification', \n",
                "                 noise_scale=0.1, random_state=42):\n",
                "        self.target_col = target_col\n",
                "        self.task = task\n",
                "        self.noise_scale = noise_scale  # Privacy parameter\n",
                "        self.random_state = random_state\n",
                "        \n",
                "    def fit(self, df):\n",
                "        self.columns = list(df.columns)\n",
                "        self.n_train = len(df)\n",
                "        \n",
                "        # Store marginal distributions (with jitter for diversity)\n",
                "        self.marginals = {}\n",
                "        for col in self.columns:\n",
                "            values = df[col].values.copy()\n",
                "            self.marginals[col] = {\n",
                "                'values': values,\n",
                "                'min': values.min(),\n",
                "                'max': values.max(),\n",
                "                'std': values.std()\n",
                "            }\n",
                "        \n",
                "        # Learn correlation via copula with noise\n",
                "        uniform_df = df.copy()\n",
                "        for col in self.columns:\n",
                "            uniform_df[col] = stats.rankdata(df[col]) / (len(df) + 1)\n",
                "        \n",
                "        normal_df = uniform_df.apply(lambda x: stats.norm.ppf(np.clip(x, 0.001, 0.999)))\n",
                "        corr_matrix = normal_df.corr().values\n",
                "        corr_matrix = np.nan_to_num(corr_matrix, nan=0.0)\n",
                "        \n",
                "        # Add Laplace noise for differential privacy\n",
                "        if self.noise_scale > 0:\n",
                "            rng = np.random.default_rng(self.random_state)\n",
                "            noise = rng.laplace(0, self.noise_scale * 0.1, corr_matrix.shape)\n",
                "            noise = (noise + noise.T) / 2  # Symmetric\n",
                "            corr_matrix = corr_matrix + noise\n",
                "        \n",
                "        # Ensure valid correlation matrix\n",
                "        np.fill_diagonal(corr_matrix, 1.0)\n",
                "        corr_matrix = np.clip(corr_matrix, -0.99, 0.99)\n",
                "        np.fill_diagonal(corr_matrix, 1.0)\n",
                "        \n",
                "        eigvals, eigvecs = np.linalg.eigh(corr_matrix)\n",
                "        eigvals = np.maximum(eigvals, 1e-6)\n",
                "        corr_matrix = eigvecs @ np.diag(eigvals) @ eigvecs.T\n",
                "        corr_matrix = (corr_matrix + corr_matrix.T) / 2\n",
                "        np.fill_diagonal(corr_matrix, 1.0)\n",
                "        \n",
                "        self.cholesky = np.linalg.cholesky(corr_matrix)\n",
                "        \n",
                "        # Target model\n",
                "        if self.target_col and self.target_col in self.columns:\n",
                "            feature_cols = [c for c in self.columns if c != self.target_col]\n",
                "            if self.task == 'classification':\n",
                "                self.target_model = GradientBoostingClassifier(\n",
                "                    n_estimators=50, max_depth=4, random_state=self.random_state\n",
                "                )\n",
                "            else:\n",
                "                from sklearn.ensemble import GradientBoostingRegressor\n",
                "                self.target_model = GradientBoostingRegressor(\n",
                "                    n_estimators=50, max_depth=4, random_state=self.random_state\n",
                "                )\n",
                "            self.target_model.fit(df[feature_cols], df[self.target_col])\n",
                "            self.feature_cols = feature_cols\n",
                "            self.target_rate = df[self.target_col].mean() if self.task == 'classification' else None\n",
                "        \n",
                "        return self\n",
                "    \n",
                "    def sample(self, n_samples, seed=None):\n",
                "        if seed is None:\n",
                "            seed = self.random_state\n",
                "        rng = np.random.default_rng(seed)\n",
                "        \n",
                "        # Correlated uniform sampling\n",
                "        z = rng.standard_normal((n_samples, len(self.columns)))\n",
                "        uniform = stats.norm.cdf(z @ self.cholesky.T)\n",
                "        uniform = np.clip(uniform, 0.001, 0.999)\n",
                "        \n",
                "        synthetic_data = {}\n",
                "        for i, col in enumerate(self.columns):\n",
                "            if col == self.target_col:\n",
                "                continue\n",
                "            \n",
                "            # Bootstrap sample from marginal (adds diversity)\n",
                "            bootstrap_idx = rng.choice(len(self.marginals[col]['values']), \n",
                "                                       size=len(self.marginals[col]['values']),\n",
                "                                       replace=True)\n",
                "            sorted_vals = np.sort(self.marginals[col]['values'][bootstrap_idx])\n",
                "            positions = np.linspace(0, 1, len(sorted_vals))\n",
                "            \n",
                "            synth_vals = np.interp(uniform[:, i], positions, sorted_vals)\n",
                "            \n",
                "            # Add small noise for privacy\n",
                "            if self.noise_scale > 0:\n",
                "                noise = rng.normal(0, self.marginals[col]['std'] * self.noise_scale * 0.1, n_samples)\n",
                "                synth_vals = synth_vals + noise\n",
                "                # Clip to valid range\n",
                "                synth_vals = np.clip(synth_vals, \n",
                "                                     self.marginals[col]['min'] - self.marginals[col]['std'],\n",
                "                                     self.marginals[col]['max'] + self.marginals[col]['std'])\n",
                "            \n",
                "            synthetic_data[col] = synth_vals\n",
                "        \n",
                "        # Generate target with noise\n",
                "        if self.target_col and self.target_col in self.columns:\n",
                "            X_synth = pd.DataFrame({c: synthetic_data[c] for c in self.feature_cols})\n",
                "            if self.task == 'classification':\n",
                "                probs = self.target_model.predict_proba(X_synth)[:, 1]\n",
                "                # Add noise to threshold for diversity\n",
                "                noisy_probs = probs + rng.normal(0, 0.05, len(probs))\n",
                "                threshold = np.percentile(noisy_probs, (1 - self.target_rate) * 100)\n",
                "                synthetic_data[self.target_col] = (noisy_probs >= threshold).astype(int)\n",
                "            else:\n",
                "                pred = self.target_model.predict(X_synth)\n",
                "                # Add regression noise\n",
                "                noise = rng.normal(0, np.std(pred) * self.noise_scale, len(pred))\n",
                "                synthetic_data[self.target_col] = pred + noise\n",
                "        \n",
                "        return pd.DataFrame(synthetic_data)[self.columns]\n",
                "\n",
                "print(\"MISATA v2 Synthesizer defined.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Fixed Correlation Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def fixed_correlation_metrics(real_df, synth_df):\n",
                "    \"\"\"\n",
                "    FIXED: Compute correlation similarity correctly.\n",
                "    Compare correlation MATRICES, not individual samples.\n",
                "    \"\"\"\n",
                "    metrics = {}\n",
                "    \n",
                "    # 1. Pearson correlation matrix similarity\n",
                "    real_pearson = real_df.corr(method='pearson').values\n",
                "    synth_pearson = synth_df.corr(method='pearson').values\n",
                "    \n",
                "    # Flatten and compare (excluding diagonal)\n",
                "    mask = ~np.eye(real_pearson.shape[0], dtype=bool)\n",
                "    real_flat = real_pearson[mask]\n",
                "    synth_flat = synth_pearson[mask]\n",
                "    \n",
                "    valid = ~(np.isnan(real_flat) | np.isnan(synth_flat))\n",
                "    metrics['pearson_matrix_corr'] = np.corrcoef(real_flat[valid], synth_flat[valid])[0, 1]\n",
                "    \n",
                "    # 2. Spearman correlation matrix similarity\n",
                "    real_spearman = real_df.corr(method='spearman').values\n",
                "    synth_spearman = synth_df.corr(method='spearman').values\n",
                "    \n",
                "    real_flat_s = real_spearman[mask]\n",
                "    synth_flat_s = synth_spearman[mask]\n",
                "    valid_s = ~(np.isnan(real_flat_s) | np.isnan(synth_flat_s))\n",
                "    metrics['spearman_matrix_corr'] = np.corrcoef(real_flat_s[valid_s], synth_flat_s[valid_s])[0, 1]\n",
                "    \n",
                "    # 3. Kendall tau - compare correlation matrices\n",
                "    # Use Kendall tau between flattened correlation matrices\n",
                "    tau, _ = stats.kendalltau(real_flat[valid], synth_flat[valid])\n",
                "    metrics['kendall_matrix_tau'] = tau\n",
                "    \n",
                "    # 4. Mean Absolute Difference in correlations\n",
                "    metrics['corr_mae'] = np.mean(np.abs(real_flat[valid] - synth_flat[valid]))\n",
                "    \n",
                "    return metrics\n",
                "\n",
                "print(\"Fixed correlation metrics defined.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Privacy Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_privacy_metrics(real_df, synth_df):\n",
                "    \"\"\"Compute privacy metrics.\"\"\"\n",
                "    scaler = StandardScaler()\n",
                "    real_scaled = scaler.fit_transform(real_df)\n",
                "    synth_scaled = scaler.transform(synth_df)\n",
                "    \n",
                "    # DCR\n",
                "    nn = NearestNeighbors(n_neighbors=1, algorithm='ball_tree')\n",
                "    nn.fit(real_scaled)\n",
                "    distances, _ = nn.kneighbors(synth_scaled)\n",
                "    \n",
                "    metrics = {\n",
                "        'dcr_mean': np.mean(distances),\n",
                "        'dcr_5th': np.percentile(distances, 5)\n",
                "    }\n",
                "    \n",
                "    # MIA\n",
                "    n_test = min(1000, len(real_df), len(synth_df))\n",
                "    real_sample = real_df.sample(n_test, random_state=42)\n",
                "    synth_sample = synth_df.sample(n_test, random_state=42)\n",
                "    \n",
                "    X_mia = pd.concat([real_sample, synth_sample], ignore_index=True)\n",
                "    y_mia = np.array([1] * n_test + [0] * n_test)\n",
                "    \n",
                "    X_train, X_test, y_train, y_test = train_test_split(X_mia, y_mia, test_size=0.3, random_state=42)\n",
                "    \n",
                "    mia_model = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
                "    mia_model.fit(X_train, y_train)\n",
                "    \n",
                "    mia_auc = roc_auc_score(y_test, mia_model.predict_proba(X_test)[:, 1])\n",
                "    metrics['mia_auc'] = mia_auc\n",
                "    metrics['mia_advantage'] = 2 * (mia_auc - 0.5)\n",
                "    \n",
                "    return metrics\n",
                "\n",
                "print(\"Privacy metrics defined.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Compare v1 vs v2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Adult Census\n",
                "print(\"Loading Adult Census...\")\n",
                "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
                "columns = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status',\n",
                "           'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss',\n",
                "           'hours_per_week', 'native_country', 'income']\n",
                "df_raw = pd.read_csv(url, names=columns, na_values=' ?', skipinitialspace=True)\n",
                "df_raw = df_raw.dropna().reset_index(drop=True).sample(5000, random_state=SEED)\n",
                "df_raw['income'] = (df_raw['income'] == '>50K').astype(int)\n",
                "\n",
                "for col in ['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country']:\n",
                "    df_raw[col] = LabelEncoder().fit_transform(df_raw[col].astype(str))\n",
                "\n",
                "train_df, test_df = train_test_split(df_raw, test_size=0.2, random_state=SEED)\n",
                "print(f\"Train: {len(train_df)}, Test: {len(test_df)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test different noise scales\n",
                "noise_scales = [0.0, 0.1, 0.2, 0.3, 0.5]\n",
                "results = []\n",
                "\n",
                "print(\"\\nComparing noise scales for privacy/utility tradeoff...\\n\")\n",
                "\n",
                "for noise in noise_scales:\n",
                "    print(f\"Noise scale: {noise}\")\n",
                "    \n",
                "    synth = MISATAv2Synthesizer(target_col='income', noise_scale=noise, random_state=SEED)\n",
                "    synth.fit(train_df)\n",
                "    df_synth = synth.sample(len(train_df))\n",
                "    \n",
                "    # Correlation metrics\n",
                "    corr = fixed_correlation_metrics(train_df, df_synth)\n",
                "    \n",
                "    # Privacy metrics\n",
                "    privacy = compute_privacy_metrics(train_df, df_synth)\n",
                "    \n",
                "    # TSTR\n",
                "    X_synth = df_synth.drop('income', axis=1)\n",
                "    y_synth = df_synth['income']\n",
                "    X_test = test_df.drop('income', axis=1)\n",
                "    y_test = test_df['income']\n",
                "    \n",
                "    model = RandomForestClassifier(n_estimators=100, random_state=SEED, n_jobs=-1)\n",
                "    model.fit(X_synth, y_synth)\n",
                "    tstr_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
                "    \n",
                "    # TRTR\n",
                "    model_real = RandomForestClassifier(n_estimators=100, random_state=SEED, n_jobs=-1)\n",
                "    model_real.fit(train_df.drop('income', axis=1), train_df['income'])\n",
                "    trtr_auc = roc_auc_score(y_test, model_real.predict_proba(X_test)[:, 1])\n",
                "    \n",
                "    result = {\n",
                "        'noise_scale': noise,\n",
                "        'pearson_corr': corr['pearson_matrix_corr'],\n",
                "        'kendall_tau': corr['kendall_matrix_tau'],\n",
                "        'corr_mae': corr['corr_mae'],\n",
                "        'mia_auc': privacy['mia_auc'],\n",
                "        'mia_advantage': privacy['mia_advantage'],\n",
                "        'dcr_5th': privacy['dcr_5th'],\n",
                "        'tstr_auc': tstr_auc,\n",
                "        'tstr_ratio': tstr_auc / trtr_auc\n",
                "    }\n",
                "    results.append(result)\n",
                "    \n",
                "    print(f\"  Kendall τ: {corr['kendall_matrix_tau']:.4f}, MIA: {privacy['mia_auc']:.4f}, TSTR: {tstr_auc/trtr_auc:.2%}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "results_df = pd.DataFrame(results)\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"PRIVACY-UTILITY TRADEOFF ANALYSIS\")\n",
                "print(\"=\"*80)\n",
                "print(results_df.to_string(index=False))\n",
                "\n",
                "# Find optimal noise scale (best privacy while maintaining >90% TSTR)\n",
                "viable = results_df[results_df['tstr_ratio'] > 0.90]\n",
                "if len(viable) > 0:\n",
                "    best = viable.loc[viable['mia_auc'].idxmin()]\n",
                "    print(f\"\\nOptimal Configuration:\")\n",
                "    print(f\"  Noise Scale: {best['noise_scale']}\")\n",
                "    print(f\"  MIA AUC: {best['mia_auc']:.4f} (lower = better privacy)\")\n",
                "    print(f\"  TSTR Ratio: {best['tstr_ratio']:.2%}\")\n",
                "    print(f\"  Kendall τ: {best['kendall_tau']:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualization\n",
                "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
                "\n",
                "# Plot 1: Privacy-Utility Tradeoff\n",
                "ax1 = axes[0]\n",
                "ax1.plot(results_df['noise_scale'], results_df['mia_auc'], 'b-o', label='MIA AUC (lower=better)', linewidth=2)\n",
                "ax1.plot(results_df['noise_scale'], results_df['tstr_ratio'], 'g-s', label='TSTR Ratio', linewidth=2)\n",
                "ax1.axhline(y=0.5, color='blue', linestyle='--', alpha=0.5, label='Perfect Privacy (0.5)')\n",
                "ax1.axhline(y=0.9, color='green', linestyle='--', alpha=0.5, label='90% Utility Target')\n",
                "ax1.set_xlabel('Noise Scale', fontsize=11)\n",
                "ax1.set_ylabel('Score', fontsize=11)\n",
                "ax1.set_title('Privacy-Utility Tradeoff', fontsize=12, fontweight='bold')\n",
                "ax1.legend()\n",
                "ax1.grid(True, alpha=0.3)\n",
                "\n",
                "# Plot 2: Correlation Preservation\n",
                "ax2 = axes[1]\n",
                "ax2.plot(results_df['noise_scale'], results_df['pearson_corr'], 'r-o', label='Pearson', linewidth=2)\n",
                "ax2.plot(results_df['noise_scale'], results_df['kendall_tau'], 'b-s', label='Kendall τ', linewidth=2)\n",
                "ax2.set_xlabel('Noise Scale', fontsize=11)\n",
                "ax2.set_ylabel('Correlation Similarity', fontsize=11)\n",
                "ax2.set_title('Correlation Preservation', fontsize=12, fontweight='bold')\n",
                "ax2.legend()\n",
                "ax2.grid(True, alpha=0.3)\n",
                "\n",
                "# Plot 3: MIA Advantage\n",
                "ax3 = axes[2]\n",
                "colors = ['red' if x > 0.5 else 'orange' if x > 0.2 else 'green' for x in results_df['mia_advantage']]\n",
                "bars = ax3.bar(results_df['noise_scale'].astype(str), results_df['mia_advantage'], color=colors, alpha=0.8)\n",
                "ax3.axhline(y=0, color='green', linestyle='--', linewidth=2, label='Perfect Privacy (0)')\n",
                "ax3.set_xlabel('Noise Scale', fontsize=11)\n",
                "ax3.set_ylabel('MIA Advantage (0=best)', fontsize=11)\n",
                "ax3.set_title('Privacy Level', fontsize=12, fontweight='bold')\n",
                "ax3.legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('misata_v2_privacy_utility.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(\"\\n✓ Saved misata_v2_privacy_utility.png\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save results\n",
                "results_df.to_csv('misata_v2_results.csv', index=False)\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"EXPERIMENT 16 COMPLETE\")\n",
                "print(\"=\"*80)\n",
                "print(\"\\nKey Improvements:\")\n",
                "print(\"  1. Fixed Kendall τ calculation (matrix-based, not sample-based)\")\n",
                "print(\"  2. Added noise injection for privacy\")\n",
                "print(\"  3. Demonstrated privacy-utility tradeoff\")\n",
                "print(\"\\nFiles saved:\")\n",
                "print(\"  - misata_v2_privacy_utility.png\")\n",
                "print(\"  - misata_v2_results.csv\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}